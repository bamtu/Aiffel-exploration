{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1788937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n",
      "text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
      "headlines 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n",
      "전체 남은 샘플수 : 98360\n",
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "# 이 NLTK에는 I, my, me, over, 조사, 접미사와 같이 문장에는 자주 등장하지만, 의미를 분석하고 요약하는 데는 거의 의미가 없는 100여개의 불용어가 미리 정리\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from importlib.metadata import version\n",
    "import summa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/news_summarization/news_summary_more.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))\n",
    "data = data[['text','headlines']]\n",
    "print('text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
    "print('headlines 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())\n",
    "\n",
    "# 중복 샘플 제거\n",
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['text'], inplace=True)\n",
    "\n",
    "# null값 있는 것 제거\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 남은 샘플수 :', (len(data)))\n",
    "\n",
    "# 샘플 들의 Text normalization\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c1b033",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d431ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # # NLTK에서 제공하는 stopwords 제거 (text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)( Text 전처리 시에서만 호출하고 이미 상대적으로 문장 길이가 짧은 Summary 전처리할 때는 호출하지 않을 예정)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0bed89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 전처리 후 결과:  ['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers', 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult fit', 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series india lost international match rohit sharma captaincy consecutive victories dating back march match witnessed india getting seventh lowest total odi cricket history', 'aegon life iterm insurance plan customers enjoy tax benefits premiums paid save taxes plan provides life cover age years also customers options insure critical illnesses disability accidental death benefit rider life cover age years', 'speaking sexual harassment allegations rajkumar hirani sonam kapoor said known hirani many years true metoo movement get derailed metoo movement always believe woman case need reserve judgment added hirani accused assistant worked sanju']\n",
      "headlines 전처리 후 결과:  ['upgrad learner switches to career in ml al with salary hike', 'delhi techie wins free food from swiggy for one year on cred', 'new zealand end rohit sharma led india match winning streak', 'aegon life iterm insurance plan helps customers save tax', 'have known hirani for yrs what if metoo claims are not true sonam']\n"
     ]
    }
   ],
   "source": [
    "# 전처리 시작\n",
    "\n",
    "clean_text = []\n",
    "for s in data['text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "# 전처리 후 출력\n",
    "print(\"text 전처리 후 결과: \", clean_text[:5])\n",
    "\n",
    "\n",
    "clean_summary = []\n",
    "for s in data['headlines']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(\"headlines 전처리 후 결과: \", clean_summary[:5])\n",
    "\n",
    "# null값 처리\n",
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dee018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text와 summary의 텍스트 길이 구하기\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "summary_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "text_max_len = 50\n",
    "summary_max_len = 8\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s.split()) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "\n",
    "\n",
    "# 정해진 길이보다 길면 제외\n",
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= summary_max_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "329af8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가.\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "\n",
    "# 인코더의 입력, 디코더의 입력과 출력 레이블을 각각 다시 Numpy 타입으로 저장\n",
    "encoder_input = np.array(data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d40ff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 21684\n",
      "훈련 레이블의 개수 : 21684\n",
      "테스트 데이터의 개수 : 5421\n",
      "테스트 레이블의 개수 : 5421\n"
     ]
    }
   ],
   "source": [
    "# 샘플을 섞은 후, 재구성\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 8:2 비율로 할당\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22611d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 42529\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 30818\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 11711\n",
      "단어 집합에서 희귀 단어의 비율: 72.46349549719015\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 8.30014118166689\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징(각 단어에 고유한 정수를 맵핑하는 작업)\n",
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "\n",
    "# src_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장돼 있는데, 이를 통해서 통계적인 정보를 얻을 수 있다.\n",
    "# 등장 빈도수가 7회 미만인 단어들이 얼마나 비중을 차지하는지 확인.\n",
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f30ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10747, 3949, 23, 60, 2122, 5748, 10748, 46, 229, 325, 255, 165, 1722, 924, 529, 646, 10748, 559, 391, 173, 3850, 3949, 820, 152, 93, 1110, 71, 3949, 208, 3441, 10748, 480, 25, 4629], [1111, 6466, 707, 27, 1224, 3950, 157, 4782, 273, 7410, 163, 36, 460, 1723, 418, 4400, 1111, 1493, 778, 3951, 213, 2733, 251, 1, 90, 19, 3182, 419, 630, 309, 126, 172, 11, 25, 251, 6], [90, 7, 158, 70, 965, 1085, 185, 5965, 5, 17, 31, 28, 1269, 185, 1802, 5236, 624, 551, 185, 4783, 2519, 3015, 9, 1, 385, 95, 185]]\n"
     ]
    }
   ],
   "source": [
    "# 등장빈도가 7회 미만인 단어들은 실제 훈련 데이터에서 등장빈도가 매우 낮다. 그래서 뺀다.\n",
    "src_vocab = 11711\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 11,711으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcefbb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 18878\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 14290\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 4588\n",
      "단어 집합에서 희귀 단어의 비율: 75.6965780273334\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 14.32411406497304\n",
      "input\n",
      "input  [[1, 816, 53, 1392, 44, 568], [1, 817, 649, 114, 5, 21, 1151], [1, 88, 3, 975, 586, 14], [1, 60, 1104, 368, 1544, 11], [1, 1933, 4, 1934]]\n",
      "target\n",
      "decoder  [[816, 53, 1392, 44, 568, 2], [817, 649, 114, 5, 21, 1151, 2], [88, 3, 975, 586, 14, 2], [60, 1104, 368, 1544, 11, 2], [1933, 4, 1934, 2]]\n"
     ]
    }
   ],
   "source": [
    "# summary 데이터에 대해 토크나이징\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "\n",
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fb53bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 11\n",
      "삭제할 테스트 데이터의 개수 : 1\n",
      "훈련 데이터의 개수 : 21673\n",
      "훈련 레이블의 개수 : 21673\n",
      "테스트 데이터의 개수 : 5420\n",
      "테스트 레이블의 개수 : 5420\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6403f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c3d87",
   "metadata": {},
   "source": [
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f014024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "# encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "# encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "# encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "887e237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1499008     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 2000)   514000      lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,108,112\n",
      "Trainable params: 4,108,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7507dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 128)      1499008     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    256000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_3[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,620,368\n",
      "Trainable params: 4,620,368\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01baa1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "85/85 [==============================] - 13s 80ms/step - loss: 4.9265 - val_loss: 4.5503\n",
      "Epoch 2/50\n",
      "85/85 [==============================] - 6s 67ms/step - loss: 4.5283 - val_loss: 4.3534\n",
      "Epoch 3/50\n",
      "85/85 [==============================] - 6s 67ms/step - loss: 4.3380 - val_loss: 4.1939\n",
      "Epoch 4/50\n",
      "85/85 [==============================] - 6s 67ms/step - loss: 4.1652 - val_loss: 4.0839\n",
      "Epoch 5/50\n",
      "85/85 [==============================] - 6s 68ms/step - loss: 4.0158 - val_loss: 3.9490\n",
      "Epoch 6/50\n",
      "85/85 [==============================] - 6s 68ms/step - loss: 3.8833 - val_loss: 3.8542\n",
      "Epoch 7/50\n",
      "85/85 [==============================] - 6s 68ms/step - loss: 3.7530 - val_loss: 3.7703\n",
      "Epoch 8/50\n",
      "85/85 [==============================] - 6s 69ms/step - loss: 3.6308 - val_loss: 3.6970\n",
      "Epoch 9/50\n",
      "85/85 [==============================] - 6s 69ms/step - loss: 3.5145 - val_loss: 3.6284\n",
      "Epoch 10/50\n",
      "85/85 [==============================] - 6s 70ms/step - loss: 3.4085 - val_loss: 3.5757\n",
      "Epoch 11/50\n",
      "85/85 [==============================] - 6s 70ms/step - loss: 3.3108 - val_loss: 3.5181\n",
      "Epoch 12/50\n",
      "85/85 [==============================] - 6s 71ms/step - loss: 3.2176 - val_loss: 3.4822\n",
      "Epoch 13/50\n",
      "85/85 [==============================] - 6s 70ms/step - loss: 3.1317 - val_loss: 3.4509\n",
      "Epoch 14/50\n",
      "85/85 [==============================] - 6s 70ms/step - loss: 3.0523 - val_loss: 3.4306\n",
      "Epoch 15/50\n",
      "85/85 [==============================] - 6s 70ms/step - loss: 2.9771 - val_loss: 3.4083\n",
      "Epoch 16/50\n",
      "85/85 [==============================] - 6s 70ms/step - loss: 2.9051 - val_loss: 3.3849\n",
      "Epoch 17/50\n",
      "85/85 [==============================] - 6s 69ms/step - loss: 2.8381 - val_loss: 3.3803\n",
      "Epoch 18/50\n",
      "85/85 [==============================] - 6s 69ms/step - loss: 2.7700 - val_loss: 3.3699\n",
      "Epoch 19/50\n",
      "85/85 [==============================] - 6s 69ms/step - loss: 2.7078 - val_loss: 3.3614\n",
      "Epoch 20/50\n",
      "85/85 [==============================] - 6s 69ms/step - loss: 2.6456 - val_loss: 3.3690\n",
      "Epoch 21/50\n",
      "85/85 [==============================] - 6s 69ms/step - loss: 2.5865 - val_loss: 3.3546\n",
      "Epoch 22/50\n",
      "85/85 [==============================] - 6s 69ms/step - loss: 2.5301 - val_loss: 3.3531\n",
      "Epoch 23/50\n",
      "85/85 [==============================] - 6s 70ms/step - loss: 2.4721 - val_loss: 3.3564\n",
      "Epoch 24/50\n",
      "85/85 [==============================] - 6s 70ms/step - loss: 2.4152 - val_loss: 3.3561\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f920c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adfbd3",
   "metadata": {},
   "source": [
    "## 모델 테스트 - Inference 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ea954d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 단계에서는 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 필요한 3개의 사전을 아래와 같이 미리 준비\n",
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2a37070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ac51c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인퍼런스 단계에서 단어 시퀀스를 완성하는 함수 만듦\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec781b48",
   "metadata": {},
   "source": [
    "## 모델 테스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8359df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : mcdonald franchise owner canada apologised pregnant woman served cleaning solution instead milk liquid smell chemical store owner said coffee machine cleaned milk supply line connected cleaning solution \n",
      "실제 요약 : mcdonald pregnant woman in \n",
      "예측 요약 :  woman fined for\n",
      "\n",
      "\n",
      "원문 : maldives commonwealth two years following cabinet approval backing move maldives quit commonwealth accusing interfering domestic affairs ex president abdulla yameen administration taken step following pressure commonwealth address issues corruption human rights abuses country \n",
      "실제 요약 : maldives to the after years \n",
      "예측 요약 :  pak foreign ministry website hacked\n",
      "\n",
      "\n",
      "원문 : india second odi world cup years april defeating sri lanka mumbai stadium win india became third country win world cup twice first win home soil captain ms dhoni man match last ball six \n",
      "실제 요약 : in pictures india world cup victory \n",
      "예측 요약 :  smith since india cricket\n",
      "\n",
      "\n",
      "원문 : cbi conducted raids residences former union minister chidambaram son karti chennai tuesday per reports cbi raided least properties cities including mumbai delhi raids reportedly linked given inx media owned media baron peter mukerjea chidambaram finance minister \n",
      "실제 요약 : cbi raids of former union minister chidambaram \n",
      "예측 요약 :  cbi arrests cbi for money\n",
      "\n",
      "\n",
      "원문 : philippine president rodrigo duterte told group business leaders delhi would offer interested investing country mocking isis said die go could make come also like come country \n",
      "실제 요약 : philippine prez offers to indian investors \n",
      "예측 요약 :  philippine prez duterte duterte\n",
      "\n",
      "\n",
      "원문 : used star wars films auctioned crore california items auctioned included sold crore meanwhile helmet original film auctioned lakh \n",
      "실제 요약 : star auctioned for crore \n",
      "예측 요약 :  video of crore in\n",
      "\n",
      "\n",
      "원문 : least people including deputy killed us state three separate homes according reports state authorities taken year old suspect named custody initiated proceedings however motive behind killings established yet \n",
      "실제 요약 : cop among killed in us \n",
      "예측 요약 :  killed in us\n",
      "\n",
      "\n",
      "원문 : audi launched new price range lakh car uses litre engine generates hp torque nm kmph seconds top speed kmph features include virtual cockpit smartphone interface wireless charging zone ac \n",
      "실제 요약 : second launched at lakh \n",
      "예측 요약 :  isro launches in\n",
      "\n",
      "\n",
      "원문 : least two people killed sunday france saint charles train station man attacked people knife allegedly shouted akbar according reports shot dead security forces adding incident likely terrorist act police ordered people avoid area \n",
      "실제 요약 : kills in france \n",
      "예측 요약 :  killed in explosion in\n",
      "\n",
      "\n",
      "원문 : india second lunar exploration mission scheduled next year named vikram honour isro founder vikram sarabhai marking th birth anniversary sunday isro also unveiled vikram sarabhai bust headquarters bengaluru tribute sarabhai isro plans launch missions next five months \n",
      "실제 요약 : named to isro founder \n",
      "예측 요약 :  india named world cup beats rd\n",
      "\n",
      "\n",
      "원문 : former pakistan captain suggested reducing women odis overs side instead regular overs compared game women tennis saying like sport features three sets ladies instead five women cricket also reduced overs said thought overs many \n",
      "실제 요약 : suggests women to \n",
      "예측 요약 :  indian team team to win\n",
      "\n",
      "\n",
      "원문 : industry body nasscom wednesday appointed raman roy chairman roy assume position april takes ceo tech mahindra nasscom also named premji wipro chief strategy officer son azim premji vice chairman \n",
      "실제 요약 : appoints as chairman \n",
      "예측 요약 :  replaces as tweets performance\n",
      "\n",
      "\n",
      "원문 : capital malta officially named european capital culture ceremony held saturday share title city netherlands notably european culture initiative aims highlight diversity european culture increase citizens sense belonging common cultural area \n",
      "실제 요약 : capital named european capital of \n",
      "예측 요약 :  canada to\n",
      "\n",
      "\n",
      "원문 : austria based researchers developed brain computer interface allows music composed power thought setup employs special cap tracks brain waves subject reading musical notes converted software music study involved participants asked think onto musical score \n",
      "실제 요약 : researchers music using brain \n",
      "예측 요약 :  scientists discover new of\n",
      "\n",
      "\n",
      "원문 : shillong set host third edition india international cherry festival november festival feature several events including japanese tea ceremony music beauty addition film archery competitions dog show golf tournament \n",
      "실제 요약 : to host festival this month \n",
      "예측 요약 :  india first ever festival\n",
      "\n",
      "\n",
      "원문 : harvard scientists achieved world controlled chemical reaction atoms create molecule elements belong group periodic table normally bond form molecule discovery could help develop efficient quantum computers new type said researchers \n",
      "실제 요약 : scientists world most chemical \n",
      "예측 요약 :  scientists discover new of\n",
      "\n",
      "\n",
      "원문 : late actress sridevi body wrapped tricolour funeral held wednesday afternoon state honours gun salute seva samaj crematorium hindu cemetery sridevi passed away saturday night accidentally drowned hotel room dubai \n",
      "실제 요약 : sridevi body in during funeral \n",
      "예측 요약 :  malayalam actor arrested for mother\n",
      "\n",
      "\n",
      "원문 : new trailer rajkummar rao starrer released rao seen playing role ahmed omar saeed sheikh british terrorist pakistani descent film marks fourth collaboration rao hansal mehta actor director scheduled release may \n",
      "실제 요약 : new trailer of rajkummar rao starrer released \n",
      "예측 요약 :  trailer of rajkummar rao starrer released\n",
      "\n",
      "\n",
      "원문 : snapchat parent company snap friday said laid employees recruiting division email sent employees last month snap ceo evan spiegel said company hiring employees rate last month snap reportedly laid dozen employees hardware marketing division \n",
      "실제 요약 : snapchat parent company off employees \n",
      "예측 요약 :  uber employees off data\n",
      "\n",
      "\n",
      "원문 : shraddha kapoor said girlfriend past wants give space allow partner wants earlier like man good sense sense adventure realise love happen anyone added shraddha \n",
      "실제 요약 : have been girlfriend in the shraddha \n",
      "예측 요약 :  have when was\n",
      "\n",
      "\n",
      "원문 : world second richest person bill gates said cryptocurrency rare technology caused deaths direct way used buy drugs adding speculative wave around cryptocurrencies super risky gates highlighted main feature reduces governments ability detect money laundering tax evasion terrorist funding \n",
      "실제 요약 : cryptocurrencies super deaths gates \n",
      "예측 요약 :  microsoft to build cryptocurrency\n",
      "\n",
      "\n",
      "원문 : video footage truck loaded steel crashing toll barrier along mexico la highway surfaced online crash reportedly happened due failure least seven people working repair area already damaged crash april injured crash reports added \n",
      "실제 요약 : video truck into toll in mexico \n",
      "예측 요약 :  dead as car collapses in\n",
      "\n",
      "\n",
      "원문 : four students drowned boat ferrying kl high school students capsized two miles away shore maharashtra saturday morning search operations underway students rescued far boat capsized according reports \n",
      "실제 요약 : killed as boat students near maha \n",
      "예측 요약 :  rescued from dera\n",
      "\n",
      "\n",
      "원문 : enforcement directorate arrested businessman dhawan reportedly close aide congress leader ahmed patel alleged involvement crore bank fraud dhawan booked sections prevention money laundering act produced court wednesday sent seven days custody \n",
      "실제 요약 : top congress leader arrested for bank fraud \n",
      "예측 요약 :  ed files defamation defamation suit against\n",
      "\n",
      "\n",
      "원문 : appointments committee cabinet appointed sanjay kothari batch ias officer haryana cadre secretary president ram nath kovind committee also declared senior journalist ashok malik press secretary kovind appointments cleared initial period two years committee headed prime minister narendra modi \n",
      "실제 요약 : sanjay appointed secretary to president kovind \n",
      "예측 요약 :  zimbabwe prez terms president\n",
      "\n",
      "\n",
      "원문 : technology giant apple granted patent self apple watch automatically fit user wrist patent describes users could adjust either manually automatic response biometric data watch however proof whether apple build self actual product \n",
      "실제 요약 : apple patents self apple watch \n",
      "예측 요약 :  apple admits to iphone iphone\n",
      "\n",
      "\n",
      "원문 : calling us gangster like north korea thursday slammed nation bombers conducted military drill near korean peninsula us nuclear threat north korea state run news agency said however us air force said drill response anything particular \n",
      "실제 요약 : korea calls us like over military \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  korea to korea against korea\n",
      "\n",
      "\n",
      "원문 : cbse affiliated schools follow uniform assessment scheme introducing two system class onwards new system schools conduct two periodic tests marks half yearly written examination account system make students confident facing class board examinations said cbse chairman rk \n",
      "실제 요약 : cbse system from class \n",
      "예측 요약 :  aadhaar for aadhaar in schools\n",
      "\n",
      "\n",
      "원문 : bengaluru based online bigbasket posted year year jump revenue crore fy losses increased crore crore larger markets bangalore hyderabad already broken even operational level co founder hari menon said alibaba also investing reported amount million acquire stake bigbasket \n",
      "실제 요약 : posts in revenue for fy \n",
      "예측 요약 :  bengaluru to invest crore in\n",
      "\n",
      "\n",
      "원문 : teachers shocking act delhi high court said court remarks came told delhi university law student threatened female professor allegedly caught cheating exams court also slammed university taking action student incident \n",
      "실제 요약 : teachers hc to delhi university \n",
      "예측 요약 :  jnu students for sexual harassment\n",
      "\n",
      "\n",
      "원문 : american baseball player bowled perfect game world series bowling nevada sunday two time star gold winner claimed th time bowled consecutive strikes first time professional bowlers association event year old plays red major league baseball \n",
      "실제 요약 : player straight strikes at world \n",
      "예측 요약 :  indian origin was was years ago\n",
      "\n",
      "\n",
      "원문 : suspected terrorists looted four ak rifles sunday house congress jammu kashmir nagar area police filed case connection incident alert also sounded area four policemen attached summoned questioning police said \n",
      "실제 요약 : terrorists from congress house in \n",
      "예측 요약 :  over militants in kashmir\n",
      "\n",
      "\n",
      "원문 : poster praising congress vice president rahul gandhi suspending mani shankar referring prime minister narendra modi surfaced online poster released congress leader ahmad shows batsman gandhi face hitting ball face \n",
      "실제 요약 : poster rahul for online \n",
      "예측 요약 :  rahul gandhi calls in\n",
      "\n",
      "\n",
      "원문 : according reports actors kangana ranaut rajkummar rao star together yet titled psychological thriller shooting expected start march jointly produced ekta kapoor shailesh singh film also mark bollywood directorial debut veteran filmmaker rao son prakash \n",
      "실제 요약 : kangana rao to star in reports \n",
      "예측 요약 :  amitabh bachchan tweets about\n",
      "\n",
      "\n",
      "원문 : john nicolas played longest ever tennis match lasted hours five minutes june wimbledon first round match played three days saw record games played hours minute long final set longer previous longest ever tennis match \n",
      "실제 요약 : longest ever tennis match for over hours \n",
      "예측 요약 :  becomes batsman to reach runs in\n",
      "\n",
      "\n",
      "원문 : actor rishi kapoor discussing baahubali success tweeted ki match film triumph business film title baahubali rishi called film indian celebration said glad part business films \n",
      "실제 요약 : on baahubali \n",
      "예측 요약 :  nawazuddin to have film film\n",
      "\n",
      "\n",
      "원문 : indian cricketing legend sachin tendulkar thursday took instagram share picture beach trophy school team nine years old tendulkar captioned picture trophy holding dream \n",
      "실제 요약 : tendulkar shares his childhood picture \n",
      "예측 요약 :  yuvraj singh shares picture with\n",
      "\n",
      "\n",
      "원문 : photo gallery pictures de annual fair held capital spain celebration began cattle fair features banks river covered decorated tents people often dressed traditional attire \n",
      "실제 요약 : in pictures spain celebrates de \n",
      "예측 요약 :  video shows volcano in\n",
      "\n",
      "\n",
      "원문 : video reportedly showing xiaomi note exploding bengaluru store emerged online damage due faulty third party used customer company claimed xiaomi india said replaced customer damaged unit brand new note \n",
      "실제 요약 : note explodes at store in bengaluru \n",
      "예측 요약 :  man creates to\n",
      "\n",
      "\n",
      "원문 : billionaire elon musk tunnel making startup boring company said build hyperloop system transport people kmph vacuum cities last month musk claimed verbal government approval dig hyperloop tunnel new york washington dc musk introduced idea behind hyperloop \n",
      "실제 요약 : elon musk startup to build system \n",
      "예측 요약 :  tesla recalls over cars\n",
      "\n",
      "\n",
      "원문 : two juveniles arrested allegedly raping seven year old girl gone community toilet delhi accused live area took girl secluded place pretext offering ride bicycle police said girl currently admitted hospital stable \n",
      "실제 요약 : two arrested for raping year old girl \n",
      "예측 요약 :  woman arrested for raping woman in\n",
      "\n",
      "\n",
      "원문 : brexit secretary david rejected bill much billion european union threatened walk away bloc without deal uk would pay legally due said however eu chief michel said desire punish uk accounts must settled \n",
      "실제 요약 : will not pay billion eu bill brexit secretary \n",
      "예측 요약 :  eu president resigns over\n",
      "\n",
      "\n",
      "원문 : german cold war era nuclear transformed country largest legal cannabis factory according reports building originally designed protect nato allies nuclear attack chosen growing marijuana safe officials said germany legalised medical marijuana year expanding limited access drug \n",
      "실제 요약 : nuclear to become germany largest factory \n",
      "예측 요약 :  china to build military in years\n",
      "\n",
      "\n",
      "원문 : sussanne khan talking gauri khan runs design store said friends feel happy person competition sussanne also interior designer added really happy well earlier started venture factory collaboration reportedly work \n",
      "실제 요약 : there is no competition with says \n",
      "예측 요약 :  irrfan khan shares picture of\n",
      "\n",
      "\n",
      "원문 : casino billionaire steve tuesday resigned chairman ceo company resorts amid sexual misconduct allegations wall street journal reported harassed female employees decades sex also reported received million settlement accused year old forcing sex \n",
      "실제 요약 : billionaire quits firm after sexual misconduct claims \n",
      "예측 요약 :  ceo resigns after sexual misconduct\n",
      "\n",
      "\n",
      "원문 : opposition mlas congress national conference monday disrupted proceedings assembly discussion gst bill mlas protesting implementation new tax regime present state would affect state special status fiscal autonomy speaker gupta forced session twice \n",
      "실제 요약 : in assembly over of gst bill \n",
      "예측 요약 :  karnataka cm wins seats in tripura\n",
      "\n",
      "\n",
      "원문 : nasa billion mobile rocket tower started addition connecting arms us space agency acknowledged said sound need design experts claim prepared first launch space launch system could used \n",
      "실제 요약 : nasa confirms its billion is \n",
      "예측 요약 :  scientists develop world first ever\n",
      "\n",
      "\n",
      "원문 : google moved billion dutch shell company according filings dutch commerce dutch subsidiary used shift revenue royalties earned outside america google ireland holdings affiliate based companies pay income tax tax strategy known double irish dutch \n",
      "실제 요약 : google billion to tax in \n",
      "예측 요약 :  google fined million for indian airlines\n",
      "\n",
      "\n",
      "원문 : result bihar class arts topper ganesh kumar cancelled immediate effect bihar school examination board official said friday police officials arrested kumar board made announcement kumar failed answer simple questions related music subject scored practical exam \n",
      "실제 요약 : bihar cancels for not questions \n",
      "예측 요약 :  cbse class board board exam\n",
      "\n",
      "\n",
      "원문 : us president donald trump thursday said wants south korea pay billion terminal high altitude area defence system missiles right sky trump added however south korean defence ministry said country would provide site infrastructure us would pay deploy operate \n",
      "실제 요약 : korea should pay for bn system trump \n",
      "예측 요약 :  us to korea\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp\n",
    "\n",
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ae84b",
   "metadata": {},
   "source": [
    "## 추출적 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee97b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ebc87e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.\n",
      "실제 요약 : upGrad learner switches to career in ML & Al with 90% salary hike\n",
      "예측 요약 : upGrad's Online Power Learning has powered 3 lakh+ careers.\n",
      "\n",
      "\n",
      "원문 : Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.\n",
      "실제 요약 : Delhi techie wins free food from Swiggy for one year on CRED\n",
      "예측 요약 : Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.\n",
      "\n",
      "\n",
      "원문 : New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's captaincy after 12 consecutive victories dating back to March 2018. The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history.\n",
      "실제 요약 : New Zealand end Rohit Sharma-led India's 12-match winning streak\n",
      "예측 요약 : The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history.\n",
      "\n",
      "\n",
      "원문 : With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years.\n",
      "실제 요약 : Aegon life iTerm insurance plan helps customers save tax\n",
      "예측 요약 : Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years.\n",
      "\n",
      "\n",
      "원문 : Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman. But in this case, we need to reserve our judgment,\" she added. Hirani has been accused by an assistant who worked in 'Sanju'.\n",
      "실제 요약 : Have known Hirani for yrs, what if MeToo claims are not true: Sonam\n",
      "예측 요약 : Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman.\n",
      "\n",
      "\n",
      "원문 : Pakistani singer Rahat Fateh Ali Khan has denied receiving any notice from the Enforcement Directorate over allegedly smuggling foreign currency out of India. \"It would have been better if the authorities would have served the notice first if any and then publicised this,\" reads a press release issued on behalf of Rahat. The statement further called the allegation \"bizarre\".\n",
      "실제 요약 : Rahat Fateh Ali Khan denies getting notice for smuggling currency\n",
      "예측 요약 : Pakistani singer Rahat Fateh Ali Khan has denied receiving any notice from the Enforcement Directorate over allegedly smuggling foreign currency out of India.\n",
      "\n",
      "\n",
      "원문 : India recorded their lowest ODI total in New Zealand after getting all out for 92 runs in 30.5 overs in the fourth ODI at Hamilton on Thursday. Seven of India's batsmen were dismissed for single-digit scores, while their number ten batsman Yuzvendra Chahal top-scored with 18*(37). India's previous lowest ODI total in New Zealand was 108.\n",
      "실제 요약 : India get all out for 92, their lowest ODI total in New Zealand\n",
      "예측 요약 : India's previous lowest ODI total in New Zealand was 108.\n",
      "\n",
      "\n",
      "원문 : Weeks after ex-CBI Director Alok Verma told the Department of Personnel and Training to consider him retired, the Home Ministry asked him to join work on the last day of his fixed tenure as Director on Thursday. The ministry directed him to immediately join as DG, Fire Services, the post he was transferred to after his removal as CBI chief.\n",
      "실제 요약 : Govt directs Alok Verma to join work 1 day before his retirement\n",
      "예측 요약 : \n",
      "\n",
      "\n",
      "원문 : Andhra Pradesh CM N Chandrababu Naidu has said, \"When I met then US President Bill Clinton, I addressed him as Mr Clinton, not as 'sir'. (PM Narendra) Modi is my junior in politics...I addressed him as sir 10 times.\" \"I did this...to satisfy his ego in the hope that he will do justice to the state,\" he added.\n",
      "실제 요약 : Called PM Modi 'sir' 10 times to satisfy his ego: Andhra CM\n",
      "예측 요약 : \n",
      "\n",
      "\n",
      "원문 : Congress candidate Shafia Zubair won the Ramgarh Assembly seat in Rajasthan, by defeating BJP's Sukhwant Singh with a margin of 12,228 votes in the bypoll. With this victory, Congress has taken its total to 100 seats in the 200-member assembly. The election to the Ramgarh seat was delayed due to the death of sitting MLA and BSP candidate Laxman Singh.\n",
      "실제 요약 : Cong wins Ramgarh bypoll in Rajasthan, takes total to 100 seats\n",
      "예측 요약 : Congress candidate Shafia Zubair won the Ramgarh Assembly seat in Rajasthan, by defeating BJP's Sukhwant Singh with a margin of 12,228 votes in the bypoll.\n",
      "\n",
      "\n",
      "원문 : Two minor cousins in Uttar Pradesh's Gorakhpur were allegedly repeatedly burnt with tongs and forced to eat human excreta by their family for being friends with two boys from the same school. The cousins revealed their ordeal to the police and Child Welfare Committee after being brought back to Gorakhpur from Nepal, where they had fled to escape the torture.\n",
      "실제 요약 : UP cousins fed human excreta for friendship with boys\n",
      "예측 요약 : \n",
      "\n",
      "\n",
      "원문 : Isha Ghosh, an 81-year-old member of Bharat Scouts and Guides (BSG), has been imparting physical and mental training to schoolchildren in Jharkhand for several decades. Chaibasa-based Ghosh reportedly walks seven kilometres daily and spends eight hours conducting physical training, apart from climbing and yoga sessions. She says, \"One should do something for society till one's last breath.\"\n",
      "실제 요약 : 81-yr-old woman conducts physical training in J'khand schools\n",
      "예측 요약 : Isha Ghosh, an 81-year-old member of Bharat Scouts and Guides (BSG), has been imparting physical and mental training to schoolchildren in Jharkhand for several decades.\n",
      "\n",
      "\n",
      "원문 : Urging saints and seers at the Kumbh Mela to quit smoking, Yoga guru Ramdev said, \"We follow Ram and Krishna who never smoked in their life then why should we?\" Making them take a pledge to quit tobacco, he collected chillum (clay pipe) from several sadhus. He said he will deposit the chillums for display at a museum he'll build.\n",
      "실제 요약 : Ram, Krishna didn't smoke, why should we: Ramdev to sadhus at Kumbh\n",
      "예측 요약 : \n",
      "\n",
      "\n",
      "원문 : Former stripper and regional sales director of a pharmaceutical company, Sunrise Lee, gave a doctor a lap dance in a nightclub to persuade him to prescribe an addictive fentanyl spray in 2012, the company's sales representative told a US court. She said she saw Lee \"sitting on [doctor's] lap, kind of bouncing around.\" Lee has been accused of bribing doctors.\n",
      "실제 요약 : Pharma exec gave doctor a lap dance to sell medicine in US: Witness\n",
      "예측 요약 : \n",
      "\n",
      "\n",
      "원문 : Reliance Industries' Chairman Mukesh Ambani's daughter Isha Ambani, who got married last month, said she only cried at her 'bidaai' because she felt peer pressure as everyone was crying, especially her parents. \"I was emotional too but everyone around me would cry all the time,\" she added. \"It was a very emotional affair for everyone in my family,\" said Isha.\n",
      "실제 요약 :  I only cried at my 'bidaai' as I felt peer pressure: Isha Ambani\n",
      "예측 요약 : \"It was a very emotional affair for everyone in my family,\" said Isha.\n",
      "\n",
      "\n",
      "원문 : Louis Vuitton owner LVMH, which makes high-end beverages like MoÃÂ«t & Chandon champagne and Hennessy cognac, said it's stockpiling four months' worth of wine and spirits in UK in preparation for Brexit. \"We're ready for worst case scenario if there are difficulties with deliveries,\" the French luxury giant said. The UK is scheduled to leave the EU on March 29.\n",
      "실제 요약 : Louis Vuitton owner to stockpile 4 months of wine, spirits in UK\n",
      "예측 요약 : Louis Vuitton owner LVMH, which makes high-end beverages like MoÃÂ«t & Chandon champagne and Hennessy cognac, said it's stockpiling four months' worth of wine and spirits in UK in preparation for Brexit.\n",
      "\n",
      "\n",
      "원문 : Filmmaker Karan Johar and actress Tabu turned showstoppers for Gaurav Gupta on the opening night of LakmÃÂ© Fashion Week Summer/ Resort 2019. While Johar wore a red sequinned jacket with black pants, Tabu walked the ramp in a grey embellished gown. The fashion show, which began on January 29, will continue till February 3.\n",
      "실제 요약 : Karan Johar, Tabu turn showstoppers on opening night of LFW\n",
      "예측 요약 : Filmmaker Karan Johar and actress Tabu turned showstoppers for Gaurav Gupta on the opening night of LakmÃÂ© Fashion Week Summer/ Resort 2019.\n",
      "\n",
      "\n",
      "원문 : In a jibe at Congress President Rahul Gandhi, PM Narendra Modi on Wednesday said those on \"bail will have to go to jail.\" PM Modi added, \"He is out on bail and his associates too are facing charges...I know they will be convicted one day.\" The PM claimed he'd waged a war on corruption because he's from a common household. \n",
      "실제 요약 : Those on bail will go to jail: PM Modi takes jibe at Rahul\n",
      "예측 요약 : \n",
      "\n",
      "\n",
      "원문 : Days after he threatened to step down from his post if Congress MLAs continue \"crossing the line,\" Karnataka Chief Minister HD Kumaraswamy accused them of taking potshots and asked, \"How many more days can I tolerate such stuff?\" Kumaraswamy, who made the statements after a Congress MLA demanded that Siddaramaiah be made CM again, said, \"Power is ephemeral.\"\n",
      "실제 요약 : How long can I tolerate Congress leaders' potshots: K'taka CM\n",
      "예측 요약 : \n",
      "\n",
      "\n",
      "원문 : Union Minister Dharmendra Pradhan on Wednesday claimed the illegal mining mafia in Odisha operates under the control of CM Naveen Patnaik and state Congress chief Niranjan Patnaik. He added, \"The time has come for the people of Odisha to put a full stop to their activities...The time has come for us to ask for an explanation from this corrupt government.\"\n",
      "실제 요약 : Odisha CM Patnaik controls mining mafia: Union Minister\n",
      "예측 요약 : \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (head, text) in enumerate(zip(data['headlines'], data['text'])):\n",
    "    if idx == 20:\n",
    "        break\n",
    "    print(\"원문 :\", text)\n",
    "    print(\"실제 요약 :\", head)\n",
    "    print(\"예측 요약 :\", summarize(text, ratio=0.35))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a84450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
