{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae4dcad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       0.91      0.83      0.87        12\n",
      "           2       0.83      0.91      0.87        11\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.91      0.91      0.91        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (1) 필요한 모듈 import\n",
    "from sklearn.datasets import load_iris #싸이킷런 데이터셋에 있는 iris를 불러오는 코드\n",
    "from sklearn.model_selection import train_test_split #싸이킷런에 있는 데이터를 train과 test dataset으로 나누어주는 함수 import\n",
    "from sklearn.tree import DecisionTreeClassifier # 싸이킷런에 있는 결정트리분류기를 사용하기 위해 불러오는 코드\n",
    "from sklearn.metrics import classification_report #싸이킷런에 있는 분류 결과에 대한 시각화를 위해 쓰는 코드\n",
    "\n",
    "# (2) 데이터 준비\n",
    "iris = load_iris() #iris 데이터 전체를 불러온다.\n",
    "iris_data = iris.data #iris데이터의 data컬럼을 분류해 iris_data 변수에 담는다.\n",
    "iris_label = iris.target #iris데이터의 target컬럼을 분류해 iris_label 변수에 담는다.\n",
    "\n",
    "# (3) train, test 데이터 분리\n",
    "#train_test_split()를 사용하여 X값, y값을 각각 train data와 test data로 나눈다. 함수에 들어 갈 파라미터로는 x,y가 들어가고\n",
    "#test_size는 몇대몇으로 나눌지 정하는 옵션, random_state는 랜덤 패턴의 값을 지정한다. (어떤 값을 넣어도 무방하다.)\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, \n",
    "                                                    iris_label, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=7) \n",
    "\n",
    "\n",
    "# (4) 모델 학습 및 예측\n",
    "decision_tree = DecisionTreeClassifier(random_state=32) #결정트리분류기의 객체를 만든다.\n",
    "decision_tree.fit(X_train, y_train) # 분류기에 x와 y의 훈련 데이터를 넣어 훈련 시킨다.\n",
    "y_pred = decision_tree.predict(X_test) # 훈련된 분류기에 X_test라는 테스트 데이터셋을 넣어 얼마나 예측했는지 확인한다.\n",
    "\n",
    "print(classification_report(y_test, y_pred)) # 결과를 지표로 확인하기 위해 classification_report를 활용해 y_test, y_pred 값을 넣어 확인한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa0bf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 1797\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        43\n",
      "           1       0.93      0.77      0.84        35\n",
      "           2       0.94      0.86      0.90        36\n",
      "           3       0.80      0.78      0.79        41\n",
      "           4       0.77      0.89      0.83        38\n",
      "           5       0.83      0.97      0.89        30\n",
      "           6       0.92      0.97      0.95        37\n",
      "           7       0.89      0.86      0.88        37\n",
      "           8       0.78      0.86      0.82        29\n",
      "           9       0.75      0.71      0.73        34\n",
      "\n",
      "    accuracy                           0.86       360\n",
      "   macro avg       0.86      0.86      0.86       360\n",
      "weighted avg       0.87      0.86      0.86       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "digits = load_digits()\n",
    "digits_data = digits.data\n",
    "digits_label = digits.target\n",
    "print(digits.target)\n",
    "print(digits.DESCR)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits_data, digits_label, \n",
    "                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=32)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883cb1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        43\n",
      "           1       0.93      0.80      0.86        35\n",
      "           2       0.91      0.86      0.89        36\n",
      "           3       0.81      0.73      0.77        41\n",
      "           4       0.84      0.84      0.84        38\n",
      "           5       0.76      0.97      0.85        30\n",
      "           6       0.92      0.97      0.95        37\n",
      "           7       0.86      0.86      0.86        37\n",
      "           8       0.81      0.86      0.83        29\n",
      "           9       0.72      0.76      0.74        34\n",
      "\n",
      "    accuracy                           0.86       360\n",
      "   macro avg       0.86      0.86      0.86       360\n",
      "weighted avg       0.86      0.86      0.86       360\n",
      " <Decision Tree>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       0.97      0.94      0.96        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.98      0.99        41\n",
      "           4       1.00      1.00      1.00        38\n",
      "           5       0.85      0.97      0.91        30\n",
      "           6       0.93      1.00      0.96        37\n",
      "           7       1.00      0.95      0.97        37\n",
      "           8       0.96      0.93      0.95        29\n",
      "           9       0.97      0.91      0.94        34\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      " <SGD Classifier>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      1.00      1.00        41\n",
      "           4       1.00      1.00      1.00        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       1.00      0.97      0.98        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      " <SVM>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       0.97      0.97      0.97        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       0.95      0.98      0.96        41\n",
      "           4       0.93      1.00      0.96        38\n",
      "           5       0.90      0.93      0.92        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.89      0.94        37\n",
      "           8       0.97      0.97      0.97        29\n",
      "           9       0.97      0.97      0.97        34\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      " <Logistic Regression>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JD-Laptop\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      " <Random Forest Classifier>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Decision Tree>')\n",
    "\n",
    "\n",
    "classifier = SGDClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<SGD Classifier>')\n",
    "\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<SVM>')\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Logistic Regression>')\n",
    "\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Random Forest Classifier>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a03688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lazypredict\n",
      "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from lazypredict) (1.3.4)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.0-py3-none-win_amd64.whl (89.1 MB)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from lazypredict) (8.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jd-laptop\\appdata\\roaming\\python\\python39\\site-packages (from lazypredict) (1.1.3)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.3-py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from lazypredict) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from lazypredict) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->lazypredict) (0.4.4)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm->lazypredict) (1.7.1)\n",
      "Requirement already satisfied: wheel in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm->lazypredict) (0.37.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm->lazypredict) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->lazypredict) (2.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->lazypredict) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->lazypredict) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->lazypredict) (1.16.0)\n",
      "Installing collected packages: xgboost, lightgbm, lazypredict\n",
      "Successfully installed lazypredict-0.2.12 lightgbm-3.3.3 xgboost-1.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "100%|███████████████████████████████████████████████████████| 29/29 [00:07<00:00,  4.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>None</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>None</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>None</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>None</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>None</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>None</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>None</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>None</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>None</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>None</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.24</td>\n",
       "      <td>None</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>None</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
       "Model                                                                          \n",
       "ExtraTreesClassifier               0.99               0.99    None      0.99   \n",
       "RandomForestClassifier             0.98               0.98    None      0.98   \n",
       "SVC                                0.98               0.98    None      0.98   \n",
       "LogisticRegression                 0.98               0.98    None      0.98   \n",
       "LGBMClassifier                     0.98               0.98    None      0.98   \n",
       "KNeighborsClassifier               0.98               0.98    None      0.98   \n",
       "XGBClassifier                      0.97               0.97    None      0.97   \n",
       "CalibratedClassifierCV             0.97               0.97    None      0.97   \n",
       "LinearSVC                          0.97               0.97    None      0.97   \n",
       "PassiveAggressiveClassifier        0.96               0.96    None      0.96   \n",
       "LinearDiscriminantAnalysis         0.96               0.96    None      0.96   \n",
       "LabelSpreading                     0.96               0.96    None      0.96   \n",
       "LabelPropagation                   0.96               0.96    None      0.96   \n",
       "BaggingClassifier                  0.96               0.96    None      0.96   \n",
       "NuSVC                              0.96               0.95    None      0.96   \n",
       "RidgeClassifierCV                  0.95               0.95    None      0.95   \n",
       "RidgeClassifier                    0.95               0.95    None      0.95   \n",
       "Perceptron                         0.94               0.94    None      0.94   \n",
       "SGDClassifier                      0.94               0.94    None      0.94   \n",
       "NearestCentroid                    0.88               0.88    None      0.88   \n",
       "BernoulliNB                        0.87               0.86    None      0.87   \n",
       "DecisionTreeClassifier             0.85               0.85    None      0.85   \n",
       "QuadraticDiscriminantAnalysis      0.84               0.84    None      0.84   \n",
       "ExtraTreeClassifier                0.81               0.81    None      0.81   \n",
       "GaussianNB                         0.79               0.79    None      0.79   \n",
       "AdaBoostClassifier                 0.26               0.24    None      0.19   \n",
       "DummyClassifier                    0.08               0.10    None      0.01   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "ExtraTreesClassifier                 0.43  \n",
       "RandomForestClassifier               0.47  \n",
       "SVC                                  0.19  \n",
       "LogisticRegression                   0.11  \n",
       "LGBMClassifier                       1.30  \n",
       "KNeighborsClassifier                 0.15  \n",
       "XGBClassifier                        0.81  \n",
       "CalibratedClassifierCV               1.36  \n",
       "LinearSVC                            0.28  \n",
       "PassiveAggressiveClassifier          0.08  \n",
       "LinearDiscriminantAnalysis           0.06  \n",
       "LabelSpreading                       0.26  \n",
       "LabelPropagation                     0.22  \n",
       "BaggingClassifier                    0.19  \n",
       "NuSVC                                0.34  \n",
       "RidgeClassifierCV                    0.03  \n",
       "RidgeClassifier                      0.03  \n",
       "Perceptron                           0.06  \n",
       "SGDClassifier                        0.10  \n",
       "NearestCentroid                      0.02  \n",
       "BernoulliNB                          0.03  \n",
       "DecisionTreeClassifier               0.05  \n",
       "QuadraticDiscriminantAnalysis        0.04  \n",
       "ExtraTreeClassifier                  0.03  \n",
       "GaussianNB                           0.02  \n",
       "AdaBoostClassifier                   0.32  \n",
       "DummyClassifier                      0.02  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8205852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      1.00      1.00        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       1.00      1.00      1.00        29\n",
      "           9       0.97      0.97      0.97        34\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "classifier = ExtraTreesClassifier(n_estimators = 100)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6625a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97        14\n",
      "           1       0.80      0.92      0.86        13\n",
      "           2       1.00      0.67      0.80         9\n",
      "\n",
      "    accuracy                           0.89        36\n",
      "   macro avg       0.91      0.86      0.87        36\n",
      "weighted avg       0.90      0.89      0.89        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "wine = load_wine()\n",
    "wine_data = wine.data\n",
    "wine_label = wine.target\n",
    "print(wine.target)\n",
    "print(wine.DESCR)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_data, wine_label, \n",
    "                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=32)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bda7861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        14\n",
      "           1       0.75      0.92      0.83        13\n",
      "           2       1.00      0.67      0.80         9\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.89      0.84      0.85        36\n",
      "weighted avg       0.88      0.86      0.86        36\n",
      " <Decision Tree>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92        14\n",
      "           1       0.54      1.00      0.70        13\n",
      "           2       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.69        36\n",
      "   macro avg       0.51      0.62      0.54        36\n",
      "weighted avg       0.58      0.69      0.61        36\n",
      " <SGD Classifier>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85        14\n",
      "           1       0.58      0.85      0.69        13\n",
      "           2       0.20      0.11      0.14         9\n",
      "\n",
      "    accuracy                           0.64        36\n",
      "   macro avg       0.57      0.58      0.56        36\n",
      "weighted avg       0.62      0.64      0.61        36\n",
      " <SVM>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        14\n",
      "           1       0.87      1.00      0.93        13\n",
      "           2       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.96      0.94      0.94        36\n",
      "weighted avg       0.95      0.94      0.95        36\n",
      " <Logistic Regression>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97        14\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.97      0.98        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      " <Random Forest Classifier>\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Decision Tree>')\n",
    "\n",
    "\n",
    "classifier = SGDClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<SGD Classifier>')\n",
    "\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<SVM>')\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Logistic Regression>')\n",
    "\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Random Forest Classifier>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b7dba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [00:00<00:00, 30.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>None</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>None</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>None</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>None</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.84</td>\n",
       "      <td>None</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.80</td>\n",
       "      <td>None</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>None</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
       "Model                                                                          \n",
       "LinearSVC                          1.00               1.00    None      1.00   \n",
       "LinearDiscriminantAnalysis         1.00               1.00    None      1.00   \n",
       "RidgeClassifierCV                  1.00               1.00    None      1.00   \n",
       "RidgeClassifier                    1.00               1.00    None      1.00   \n",
       "QuadraticDiscriminantAnalysis      1.00               1.00    None      1.00   \n",
       "PassiveAggressiveClassifier        1.00               1.00    None      1.00   \n",
       "NuSVC                              1.00               1.00    None      1.00   \n",
       "LogisticRegression                 1.00               1.00    None      1.00   \n",
       "LGBMClassifier                     1.00               1.00    None      1.00   \n",
       "GaussianNB                         1.00               1.00    None      1.00   \n",
       "KNeighborsClassifier               0.97               0.97    None      0.97   \n",
       "ExtraTreeClassifier                0.97               0.97    None      0.97   \n",
       "XGBClassifier                      0.97               0.97    None      0.97   \n",
       "SGDClassifier                      0.97               0.97    None      0.97   \n",
       "RandomForestClassifier             0.97               0.97    None      0.97   \n",
       "LabelPropagation                   0.97               0.97    None      0.97   \n",
       "Perceptron                         0.97               0.97    None      0.97   \n",
       "NearestCentroid                    0.97               0.97    None      0.97   \n",
       "ExtraTreesClassifier               0.97               0.97    None      0.97   \n",
       "LabelSpreading                     0.97               0.97    None      0.97   \n",
       "CalibratedClassifierCV             0.97               0.96    None      0.97   \n",
       "SVC                                0.97               0.96    None      0.97   \n",
       "BaggingClassifier                  0.94               0.95    None      0.94   \n",
       "BernoulliNB                        0.92               0.91    None      0.92   \n",
       "DecisionTreeClassifier             0.86               0.84    None      0.86   \n",
       "AdaBoostClassifier                 0.83               0.80    None      0.83   \n",
       "DummyClassifier                    0.36               0.33    None      0.19   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "LinearSVC                            0.01  \n",
       "LinearDiscriminantAnalysis           0.01  \n",
       "RidgeClassifierCV                    0.02  \n",
       "RidgeClassifier                      0.01  \n",
       "QuadraticDiscriminantAnalysis        0.01  \n",
       "PassiveAggressiveClassifier          0.02  \n",
       "NuSVC                                0.01  \n",
       "LogisticRegression                   0.02  \n",
       "LGBMClassifier                       0.11  \n",
       "GaussianNB                           0.01  \n",
       "KNeighborsClassifier                 0.01  \n",
       "ExtraTreeClassifier                  0.01  \n",
       "XGBClassifier                        0.08  \n",
       "SGDClassifier                        0.01  \n",
       "RandomForestClassifier               0.18  \n",
       "LabelPropagation                     0.01  \n",
       "Perceptron                           0.02  \n",
       "NearestCentroid                      0.01  \n",
       "ExtraTreesClassifier                 0.14  \n",
       "LabelSpreading                       0.02  \n",
       "CalibratedClassifierCV               0.05  \n",
       "SVC                                  0.01  \n",
       "BaggingClassifier                    0.03  \n",
       "BernoulliNB                          0.01  \n",
       "DecisionTreeClassifier               0.01  \n",
       "AdaBoostClassifier                   0.10  \n",
       "DummyClassifier                      0.01  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7bc5f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397f35f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n",
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92        42\n",
      "           1       0.95      0.96      0.95        72\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.94      0.93      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "breast_cancer_data = breast_cancer.data\n",
    "breast_cancer_label = breast_cancer.target\n",
    "print(breast_cancer.target)\n",
    "print(breast_cancer.DESCR)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(breast_cancer_data, breast_cancer_label, \n",
    "                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=32)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c610d5d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4976/608116327.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'<Decision Tree>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Decision Tree>')\n",
    "\n",
    "\n",
    "classifier = SGDClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<SGD Classifier>')\n",
    "\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<SVM>')\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Logistic Regression>')\n",
    "\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred), '<Random Forest Classifier>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "234d4aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 29/29 [00:01<00:00, 17.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
       "Model                                                                           \n",
       "LinearSVC                          0.97               0.97     0.97      0.97   \n",
       "SVC                                0.97               0.97     0.97      0.97   \n",
       "PassiveAggressiveClassifier        0.97               0.97     0.97      0.97   \n",
       "LogisticRegression                 0.97               0.97     0.97      0.97   \n",
       "Perceptron                         0.97               0.96     0.96      0.97   \n",
       "CalibratedClassifierCV             0.96               0.96     0.96      0.96   \n",
       "SGDClassifier                      0.96               0.96     0.96      0.96   \n",
       "BaggingClassifier                  0.96               0.95     0.95      0.96   \n",
       "QuadraticDiscriminantAnalysis      0.96               0.95     0.95      0.96   \n",
       "LGBMClassifier                     0.96               0.95     0.95      0.96   \n",
       "DecisionTreeClassifier             0.96               0.95     0.95      0.96   \n",
       "KNeighborsClassifier               0.96               0.94     0.94      0.96   \n",
       "XGBClassifier                      0.96               0.94     0.94      0.96   \n",
       "ExtraTreesClassifier               0.96               0.94     0.94      0.96   \n",
       "RandomForestClassifier             0.96               0.94     0.94      0.96   \n",
       "RidgeClassifier                    0.96               0.94     0.94      0.96   \n",
       "GaussianNB                         0.95               0.94     0.94      0.95   \n",
       "NuSVC                              0.95               0.93     0.93      0.95   \n",
       "LinearDiscriminantAnalysis         0.95               0.93     0.93      0.95   \n",
       "RidgeClassifierCV                  0.95               0.93     0.93      0.95   \n",
       "BernoulliNB                        0.94               0.92     0.92      0.94   \n",
       "LabelPropagation                   0.93               0.92     0.92      0.93   \n",
       "LabelSpreading                     0.93               0.92     0.92      0.93   \n",
       "AdaBoostClassifier                 0.93               0.91     0.91      0.93   \n",
       "ExtraTreeClassifier                0.93               0.90     0.90      0.93   \n",
       "NearestCentroid                    0.92               0.90     0.90      0.92   \n",
       "DummyClassifier                    0.63               0.50     0.50      0.49   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "LinearSVC                            0.02  \n",
       "SVC                                  0.03  \n",
       "PassiveAggressiveClassifier          0.02  \n",
       "LogisticRegression                   0.02  \n",
       "Perceptron                           0.02  \n",
       "CalibratedClassifierCV               0.05  \n",
       "SGDClassifier                        0.02  \n",
       "BaggingClassifier                    0.07  \n",
       "QuadraticDiscriminantAnalysis        0.02  \n",
       "LGBMClassifier                       0.19  \n",
       "DecisionTreeClassifier               0.02  \n",
       "KNeighborsClassifier                 0.10  \n",
       "XGBClassifier                        0.10  \n",
       "ExtraTreesClassifier                 0.16  \n",
       "RandomForestClassifier               0.35  \n",
       "RidgeClassifier                      0.02  \n",
       "GaussianNB                           0.01  \n",
       "NuSVC                                0.04  \n",
       "LinearDiscriminantAnalysis           0.02  \n",
       "RidgeClassifierCV                    0.02  \n",
       "BernoulliNB                          0.01  \n",
       "LabelPropagation                     0.04  \n",
       "LabelSpreading                       0.03  \n",
       "AdaBoostClassifier                   0.20  \n",
       "ExtraTreeClassifier                  0.01  \n",
       "NearestCentroid                      0.02  \n",
       "DummyClassifier                      0.01  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5be72768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.94        42\n",
      "           1       0.94      1.00      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.94      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed983d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 2 folds for each of 4320 candidates, totalling 8640 fits\n"
     ]
    }
   ],
   "source": [
    "# 예상치않게 종료됨.--------------------------------------------------------\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import numpy as np\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "# print(random_grid)\n",
    "# rf = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(estimator = rf, param_grid = random_grid, cv = 2, n_jobs = -1, verbose = 2)\n",
    "\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d9f74b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 100\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 100\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 100\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 100\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 100\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 100\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 100\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 200\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 200\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 200\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 200\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 200\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 200\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 200\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 300\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 300\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 300\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 300\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 300\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 300\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 300\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 400\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 400\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 400\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 400\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 400\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 400\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 400\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 500\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 500\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 500\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 500\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 500\n",
      "min_samples_leaf = 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 500\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 500\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 600\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 600\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 600\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 600\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 600\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 600\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 600\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 700\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 700\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 700\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 700\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 700\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 700\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 700\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 800\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 800\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 800\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 800\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 800\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 800\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 800\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 900\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 900\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 900\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 900\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 900\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 900\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 900\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1000\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1000\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1000\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1000\n",
      "min_samples_leaf = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1000\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1000\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1000\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1100\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1100\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1100\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1100\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1100\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1100\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1100\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1200\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1200\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1200\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1200\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1200\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1200\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1200\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1300\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1300\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1300\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1300\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1300\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1300\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1300\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1400\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1400\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1400\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1400\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1400\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1400\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1400\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1500\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1500\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1500\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1500\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1500\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1500\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1500\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1600\n",
      "min_samples_leaf = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1600\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1600\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1600\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1600\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1600\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1600\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1700\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1700\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1700\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1700\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1700\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1700\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1700\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1800\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1800\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1800\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1800\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1800\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1800\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1800\n",
      "min_samples_leaf = 7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1900\n",
      "min_samples_leaf = 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1900\n",
      "min_samples_leaf = 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1900\n",
      "min_samples_leaf = 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1900\n",
      "min_samples_leaf = 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1900\n",
      "min_samples_leaf = 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1900\n",
      "min_samples_leaf = 6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      1.00      1.00        36\n",
      "           3       1.00      0.93      0.96        41\n",
      "           4       0.97      1.00      0.99        38\n",
      "           5       0.97      1.00      0.98        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       1.00      0.97      0.99        37\n",
      "           8       0.94      1.00      0.97        29\n",
      "           9       0.94      0.97      0.96        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "n_estimator = 1900\n",
      "min_samples_leaf = 7\n"
     ]
    }
   ],
   "source": [
    "n_estimator_lists = [i for i in range(100,2000,100)]\n",
    "min_samples_leafs_lists = [1, 2, 3, 4, 5, 6, 7]\n",
    "for n_estimator_list in n_estimator_lists:\n",
    "    for min_samples_leaf_list in min_samples_leafs_lists:\n",
    "        classifier = RandomForestClassifier(\n",
    "            n_estimators=n_estimator_list,\n",
    "            max_features='auto',\n",
    "            min_samples_leaf=min_samples_leaf_list)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print('n_estimator =', n_estimator_list)\n",
    "        print('min_samples_leaf =', min_samples_leaf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d7e16b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        43\n",
      "           1       1.00      1.00      1.00        35\n",
      "           2       1.00      0.97      0.99        36\n",
      "           3       1.00      0.98      0.99        41\n",
      "           4       0.95      1.00      0.97        38\n",
      "           5       0.97      0.97      0.97        30\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       0.97      0.97      0.97        37\n",
      "           8       0.97      1.00      0.98        29\n",
      "           9       0.91      0.94      0.93        34\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de455f",
   "metadata": {},
   "source": [
    "### 회고\n",
    "\n",
    "------\n",
    "\n",
    "머신러닝(, 딥러닝)은 노가다라고 교수님께 배웠다. 최적의 결과를 위해 하이퍼파라미터를 헤매는 과정은 답없는 싸움을 하는 것 같다. \n",
    "프로젝트에서 어려웠던 점은 역시 최적의 결과를 얻기위해 싸우는 과정이다. 해결을 위해 GridSearch방법을 사용해 시도해보았지만, \n",
    "많은 로드가 걸려 OS에서 프로세스를 종료시킨다. 그래서 되도록 많은 경우의 케이스를 고려하기위해 for문을 이용해 n_estimator와\n",
    "min_samples_leaf 파라미터를 튜닝시키다 전부 accuracy가 98 % 여서 한계점이 있었다.\n",
    "\n",
    "새로 배운 것은 프로젝트를 진행하면서 breast_cancer등의 샘플 데이터를 불러오는 것을 배웠다.\n",
    "\n",
    "\n",
    "\n",
    "사실 하이퍼파라미터 튜닝은 우주전파재난환경 대회에서 수 시간동안 했던 일이었다. \n",
    "대회 측의 Weighted RMSE가 내가 직접계산하는 RMSE와 달랐던 탓에 최적의 결과를 구했는지는 아직도 의문이지만,\n",
    "for문을 이용해서 최적의 하이퍼파라미터를 찾으려고 했던 치열했던 노력이 이 프로젝트를 진행하며 생각났다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 출처\n",
    "\n",
    "------\n",
    "\n",
    "- 랜덤포레스트 튜닝 : https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "- lazy predict : https://lazypredict.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd8a367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
