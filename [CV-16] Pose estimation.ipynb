{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3649bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46a36fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n",
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)\n",
    "    \n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)\n",
    "    \n",
    "# TFRecord 파일 생성.\n",
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eaf9bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c5d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 12:18:57,058\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.89gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=152)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=150)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "    \n",
    "# TFRecord 생성.\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae0aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=151)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)\n",
    "\n",
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n",
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b30f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리.\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204555f",
   "metadata": {},
   "source": [
    "# 모델 생성 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e94987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourglass\n",
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n",
    "\n",
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a65cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "# Stacked hourglass\n",
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb66ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 0s 0us/step\n",
      "94781440/94765736 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Simplebaseline\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(2,2), strides=2))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization(momentum=0.9))\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "upconv = _make_deconv_layer(3)\n",
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')\n",
    "def Simplebaseline(input_shape=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, out, name='simple_baseline')\n",
    "    return model\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                model_file_name,\n",
    "                model,\n",
    "                epochs,\n",
    "                global_batch_size,\n",
    "                strategy,\n",
    "                initial_learning_rate):\n",
    "\n",
    "        self.model_file_name = model_file_name\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/{}-epoch-{}-loss-{:.4f}.h5'.format(self.model_file_name, epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4db8ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, model_file_name='model', is_stackedhourglassnetwork=True):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "        \n",
    "        if is_stackedhourglassnetwork:\n",
    "            model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        else:\n",
    "            model = Simplebaseline(IMAGE_SHAPE)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_file_name,\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)\n",
    "    \n",
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd1557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.41861081 epoch total loss 2.41861081\n",
      "Trained batch 2 batch loss 2.58012772 epoch total loss 2.49936914\n",
      "Trained batch 3 batch loss 2.62791896 epoch total loss 2.54221916\n",
      "Trained batch 4 batch loss 2.57787752 epoch total loss 2.55113363\n",
      "Trained batch 5 batch loss 2.47121215 epoch total loss 2.53514934\n",
      "Trained batch 6 batch loss 2.46991253 epoch total loss 2.52427649\n",
      "Trained batch 7 batch loss 2.35001612 epoch total loss 2.49938226\n",
      "Trained batch 8 batch loss 2.26854467 epoch total loss 2.47052765\n",
      "Trained batch 9 batch loss 2.29138732 epoch total loss 2.45062327\n",
      "Trained batch 10 batch loss 2.23984623 epoch total loss 2.42954564\n",
      "Trained batch 11 batch loss 2.26803 epoch total loss 2.41486239\n",
      "Trained batch 12 batch loss 2.07873964 epoch total loss 2.38685203\n",
      "Trained batch 13 batch loss 2.08639765 epoch total loss 2.36374021\n",
      "Trained batch 14 batch loss 1.95714653 epoch total loss 2.33469772\n",
      "Trained batch 15 batch loss 1.89560068 epoch total loss 2.30542445\n",
      "Trained batch 16 batch loss 1.85740733 epoch total loss 2.27742338\n",
      "Trained batch 17 batch loss 2.00894165 epoch total loss 2.2616303\n",
      "Trained batch 18 batch loss 2.03093767 epoch total loss 2.24881411\n",
      "Trained batch 19 batch loss 2.05837631 epoch total loss 2.23879099\n",
      "Trained batch 20 batch loss 1.84869778 epoch total loss 2.21928644\n",
      "Trained batch 21 batch loss 1.75448704 epoch total loss 2.19715309\n",
      "Trained batch 22 batch loss 1.60104322 epoch total loss 2.17005706\n",
      "Trained batch 23 batch loss 1.64028716 epoch total loss 2.14702368\n",
      "Trained batch 24 batch loss 1.70034134 epoch total loss 2.12841177\n",
      "Trained batch 25 batch loss 1.78219903 epoch total loss 2.11456323\n",
      "Trained batch 26 batch loss 1.80880344 epoch total loss 2.10280323\n",
      "Trained batch 27 batch loss 1.93871307 epoch total loss 2.09672594\n",
      "Trained batch 28 batch loss 1.91547632 epoch total loss 2.09025264\n",
      "Trained batch 29 batch loss 1.78254795 epoch total loss 2.0796423\n",
      "Trained batch 30 batch loss 1.8604033 epoch total loss 2.07233429\n",
      "Trained batch 31 batch loss 1.84111404 epoch total loss 2.06487536\n",
      "Trained batch 32 batch loss 1.79203844 epoch total loss 2.05634928\n",
      "Trained batch 33 batch loss 1.78108871 epoch total loss 2.04800797\n",
      "Trained batch 34 batch loss 1.71538055 epoch total loss 2.03822494\n",
      "Trained batch 35 batch loss 1.82842183 epoch total loss 2.03223038\n",
      "Trained batch 36 batch loss 1.75990951 epoch total loss 2.02466607\n",
      "Trained batch 37 batch loss 1.79841566 epoch total loss 2.01855111\n",
      "Trained batch 38 batch loss 1.88239 epoch total loss 2.01496816\n",
      "Trained batch 39 batch loss 1.78490293 epoch total loss 2.00906897\n",
      "Trained batch 40 batch loss 1.7913599 epoch total loss 2.00362635\n",
      "Trained batch 41 batch loss 1.6444757 epoch total loss 1.99486649\n",
      "Trained batch 42 batch loss 1.69927359 epoch total loss 1.98782861\n",
      "Trained batch 43 batch loss 1.55701137 epoch total loss 1.97780967\n",
      "Trained batch 44 batch loss 1.6444993 epoch total loss 1.97023439\n",
      "Trained batch 45 batch loss 1.71582639 epoch total loss 1.96458101\n",
      "Trained batch 46 batch loss 1.75736153 epoch total loss 1.96007621\n",
      "Trained batch 47 batch loss 1.78865921 epoch total loss 1.956429\n",
      "Trained batch 48 batch loss 1.72119367 epoch total loss 1.95152819\n",
      "Trained batch 49 batch loss 1.67094302 epoch total loss 1.94580197\n",
      "Trained batch 50 batch loss 1.66332126 epoch total loss 1.94015241\n",
      "Trained batch 51 batch loss 1.63742936 epoch total loss 1.93421662\n",
      "Trained batch 52 batch loss 1.80345833 epoch total loss 1.93170214\n",
      "Trained batch 53 batch loss 1.73393321 epoch total loss 1.92797065\n",
      "Trained batch 54 batch loss 1.81016159 epoch total loss 1.92578888\n",
      "Trained batch 55 batch loss 1.7512207 epoch total loss 1.92261493\n",
      "Trained batch 56 batch loss 1.71040487 epoch total loss 1.91882539\n",
      "Trained batch 57 batch loss 1.71433532 epoch total loss 1.91523778\n",
      "Trained batch 58 batch loss 1.76567304 epoch total loss 1.91265905\n",
      "Trained batch 59 batch loss 1.74954736 epoch total loss 1.90989447\n",
      "Trained batch 60 batch loss 1.78455591 epoch total loss 1.90780556\n",
      "Trained batch 61 batch loss 1.83251107 epoch total loss 1.90657115\n",
      "Trained batch 62 batch loss 1.79650712 epoch total loss 1.904796\n",
      "Trained batch 63 batch loss 1.63526893 epoch total loss 1.90051782\n",
      "Trained batch 64 batch loss 1.62598 epoch total loss 1.89622808\n",
      "Trained batch 65 batch loss 1.76998043 epoch total loss 1.8942858\n",
      "Trained batch 66 batch loss 1.71146226 epoch total loss 1.89151573\n",
      "Trained batch 67 batch loss 1.71035886 epoch total loss 1.88881195\n",
      "Trained batch 68 batch loss 1.68712723 epoch total loss 1.88584614\n",
      "Trained batch 69 batch loss 1.7413274 epoch total loss 1.88375163\n",
      "Trained batch 70 batch loss 1.73656964 epoch total loss 1.88164914\n",
      "Trained batch 71 batch loss 1.80106 epoch total loss 1.88051403\n",
      "Trained batch 72 batch loss 1.78186226 epoch total loss 1.87914383\n",
      "Trained batch 73 batch loss 1.59813476 epoch total loss 1.87529433\n",
      "Trained batch 74 batch loss 1.67802548 epoch total loss 1.87262845\n",
      "Trained batch 75 batch loss 1.69957697 epoch total loss 1.87032104\n",
      "Trained batch 76 batch loss 1.72353125 epoch total loss 1.86838949\n",
      "Trained batch 77 batch loss 1.60081065 epoch total loss 1.86491454\n",
      "Trained batch 78 batch loss 1.7545706 epoch total loss 1.8635\n",
      "Trained batch 79 batch loss 1.54189062 epoch total loss 1.85942888\n",
      "Trained batch 80 batch loss 1.62999892 epoch total loss 1.85656106\n",
      "Trained batch 81 batch loss 1.65421641 epoch total loss 1.85406303\n",
      "Trained batch 82 batch loss 1.77283311 epoch total loss 1.8530724\n",
      "Trained batch 83 batch loss 1.76531386 epoch total loss 1.85201514\n",
      "Trained batch 84 batch loss 1.7711091 epoch total loss 1.85105193\n",
      "Trained batch 85 batch loss 1.68739712 epoch total loss 1.84912646\n",
      "Trained batch 86 batch loss 1.54592371 epoch total loss 1.84560096\n",
      "Trained batch 87 batch loss 1.69418478 epoch total loss 1.84386051\n",
      "Trained batch 88 batch loss 1.68411195 epoch total loss 1.84204519\n",
      "Trained batch 89 batch loss 1.72199929 epoch total loss 1.84069633\n",
      "Trained batch 90 batch loss 1.5918231 epoch total loss 1.83793116\n",
      "Trained batch 91 batch loss 1.4438076 epoch total loss 1.8336\n",
      "Trained batch 92 batch loss 1.56089973 epoch total loss 1.83063591\n",
      "Trained batch 93 batch loss 1.60578609 epoch total loss 1.82821822\n",
      "Trained batch 94 batch loss 1.59272075 epoch total loss 1.82571292\n",
      "Trained batch 95 batch loss 1.72196937 epoch total loss 1.82462096\n",
      "Trained batch 96 batch loss 1.76193094 epoch total loss 1.82396793\n",
      "Trained batch 97 batch loss 1.72088671 epoch total loss 1.82290518\n",
      "Trained batch 98 batch loss 1.73846388 epoch total loss 1.82204354\n",
      "Trained batch 99 batch loss 1.74070072 epoch total loss 1.82122207\n",
      "Trained batch 100 batch loss 1.71371412 epoch total loss 1.82014692\n",
      "Trained batch 101 batch loss 1.64912796 epoch total loss 1.81845367\n",
      "Trained batch 102 batch loss 1.75706446 epoch total loss 1.81785178\n",
      "Trained batch 103 batch loss 1.70422387 epoch total loss 1.81674862\n",
      "Trained batch 104 batch loss 1.63838243 epoch total loss 1.81503356\n",
      "Trained batch 105 batch loss 1.66239882 epoch total loss 1.81357992\n",
      "Trained batch 106 batch loss 1.68488908 epoch total loss 1.81236589\n",
      "Trained batch 107 batch loss 1.62060416 epoch total loss 1.8105737\n",
      "Trained batch 108 batch loss 1.57272112 epoch total loss 1.80837142\n",
      "Trained batch 109 batch loss 1.64720273 epoch total loss 1.80689275\n",
      "Trained batch 110 batch loss 1.70590866 epoch total loss 1.8059746\n",
      "Trained batch 111 batch loss 1.6806829 epoch total loss 1.80484581\n",
      "Trained batch 112 batch loss 1.69171977 epoch total loss 1.80383587\n",
      "Trained batch 113 batch loss 1.65345538 epoch total loss 1.80250514\n",
      "Trained batch 114 batch loss 1.57321858 epoch total loss 1.80049372\n",
      "Trained batch 115 batch loss 1.67833185 epoch total loss 1.79943144\n",
      "Trained batch 116 batch loss 1.65947235 epoch total loss 1.79822493\n",
      "Trained batch 117 batch loss 1.6218909 epoch total loss 1.79671776\n",
      "Trained batch 118 batch loss 1.73110926 epoch total loss 1.79616177\n",
      "Trained batch 119 batch loss 1.74930739 epoch total loss 1.79576802\n",
      "Trained batch 120 batch loss 1.77238822 epoch total loss 1.79557312\n",
      "Trained batch 121 batch loss 1.73525453 epoch total loss 1.7950747\n",
      "Trained batch 122 batch loss 1.75359154 epoch total loss 1.7947346\n",
      "Trained batch 123 batch loss 1.7603178 epoch total loss 1.79445481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 124 batch loss 1.73369527 epoch total loss 1.79396474\n",
      "Trained batch 125 batch loss 1.68802726 epoch total loss 1.79311728\n",
      "Trained batch 126 batch loss 1.72778428 epoch total loss 1.79259884\n",
      "Trained batch 127 batch loss 1.78926563 epoch total loss 1.7925725\n",
      "Trained batch 128 batch loss 1.76675451 epoch total loss 1.7923708\n",
      "Trained batch 129 batch loss 1.72443092 epoch total loss 1.79184413\n",
      "Trained batch 130 batch loss 1.72480571 epoch total loss 1.79132843\n",
      "Trained batch 131 batch loss 1.69270968 epoch total loss 1.79057562\n",
      "Trained batch 132 batch loss 1.68109465 epoch total loss 1.78974617\n",
      "Trained batch 133 batch loss 1.64995694 epoch total loss 1.7886951\n",
      "Trained batch 134 batch loss 1.64344025 epoch total loss 1.78761113\n",
      "Trained batch 135 batch loss 1.51881778 epoch total loss 1.78562\n",
      "Trained batch 136 batch loss 1.57104301 epoch total loss 1.78404224\n",
      "Trained batch 137 batch loss 1.60941315 epoch total loss 1.78276765\n",
      "Trained batch 138 batch loss 1.58680832 epoch total loss 1.78134763\n",
      "Trained batch 139 batch loss 1.59434712 epoch total loss 1.78000224\n",
      "Trained batch 140 batch loss 1.49865472 epoch total loss 1.77799273\n",
      "Trained batch 141 batch loss 1.57316363 epoch total loss 1.77654\n",
      "Trained batch 142 batch loss 1.56596291 epoch total loss 1.77505708\n",
      "Trained batch 143 batch loss 1.39838374 epoch total loss 1.77242291\n",
      "Trained batch 144 batch loss 1.6488111 epoch total loss 1.77156448\n",
      "Trained batch 145 batch loss 1.59996688 epoch total loss 1.77038109\n",
      "Trained batch 146 batch loss 1.66133809 epoch total loss 1.76963425\n",
      "Trained batch 147 batch loss 1.66385233 epoch total loss 1.7689147\n",
      "Trained batch 148 batch loss 1.72271657 epoch total loss 1.76860249\n",
      "Trained batch 149 batch loss 1.73893297 epoch total loss 1.76840329\n",
      "Trained batch 150 batch loss 1.71675038 epoch total loss 1.7680589\n",
      "Trained batch 151 batch loss 1.74434781 epoch total loss 1.7679019\n",
      "Trained batch 152 batch loss 1.77192736 epoch total loss 1.76792848\n",
      "Trained batch 153 batch loss 1.8270936 epoch total loss 1.76831508\n",
      "Trained batch 154 batch loss 1.75046539 epoch total loss 1.76819921\n",
      "Trained batch 155 batch loss 1.59982085 epoch total loss 1.76711285\n",
      "Trained batch 156 batch loss 1.60281515 epoch total loss 1.76605964\n",
      "Trained batch 157 batch loss 1.66350961 epoch total loss 1.76540649\n",
      "Trained batch 158 batch loss 1.59735107 epoch total loss 1.7643429\n",
      "Trained batch 159 batch loss 1.58638024 epoch total loss 1.76322377\n",
      "Trained batch 160 batch loss 1.61888361 epoch total loss 1.76232171\n",
      "Trained batch 161 batch loss 1.54224288 epoch total loss 1.76095462\n",
      "Trained batch 162 batch loss 1.76019418 epoch total loss 1.76095\n",
      "Trained batch 163 batch loss 1.71713543 epoch total loss 1.76068115\n",
      "Trained batch 164 batch loss 1.72048259 epoch total loss 1.76043606\n",
      "Trained batch 165 batch loss 1.69484401 epoch total loss 1.76003861\n",
      "Trained batch 166 batch loss 1.66089237 epoch total loss 1.75944138\n",
      "Trained batch 167 batch loss 1.67419219 epoch total loss 1.75893092\n",
      "Trained batch 168 batch loss 1.68975639 epoch total loss 1.75851917\n",
      "Trained batch 169 batch loss 1.63506365 epoch total loss 1.75778866\n",
      "Trained batch 170 batch loss 1.67794371 epoch total loss 1.75731897\n",
      "Trained batch 171 batch loss 1.6092118 epoch total loss 1.75645292\n",
      "Trained batch 172 batch loss 1.59849572 epoch total loss 1.75553465\n",
      "Trained batch 173 batch loss 1.58247697 epoch total loss 1.75453436\n",
      "Trained batch 174 batch loss 1.60920823 epoch total loss 1.7536993\n",
      "Trained batch 175 batch loss 1.69325423 epoch total loss 1.75335395\n",
      "Trained batch 176 batch loss 1.75951552 epoch total loss 1.753389\n",
      "Trained batch 177 batch loss 1.64175117 epoch total loss 1.75275826\n",
      "Trained batch 178 batch loss 1.78237164 epoch total loss 1.75292468\n",
      "Trained batch 179 batch loss 1.73369837 epoch total loss 1.75281739\n",
      "Trained batch 180 batch loss 1.65398753 epoch total loss 1.75226831\n",
      "Trained batch 181 batch loss 1.53858149 epoch total loss 1.75108767\n",
      "Trained batch 182 batch loss 1.56612349 epoch total loss 1.75007141\n",
      "Trained batch 183 batch loss 1.3776958 epoch total loss 1.7480365\n",
      "Trained batch 184 batch loss 1.46389222 epoch total loss 1.74649227\n",
      "Trained batch 185 batch loss 1.62509441 epoch total loss 1.74583614\n",
      "Trained batch 186 batch loss 1.36447644 epoch total loss 1.74378574\n",
      "Trained batch 187 batch loss 1.34672308 epoch total loss 1.74166238\n",
      "Trained batch 188 batch loss 1.36379421 epoch total loss 1.7396524\n",
      "Trained batch 189 batch loss 1.41634917 epoch total loss 1.73794186\n",
      "Trained batch 190 batch loss 1.54690421 epoch total loss 1.73693633\n",
      "Trained batch 191 batch loss 1.62876987 epoch total loss 1.73637021\n",
      "Trained batch 192 batch loss 1.68269968 epoch total loss 1.73609066\n",
      "Trained batch 193 batch loss 1.75067353 epoch total loss 1.73616624\n",
      "Trained batch 194 batch loss 1.72574353 epoch total loss 1.73611248\n",
      "Trained batch 195 batch loss 1.62117219 epoch total loss 1.7355231\n",
      "Trained batch 196 batch loss 1.42935777 epoch total loss 1.73396099\n",
      "Trained batch 197 batch loss 1.57111931 epoch total loss 1.73313427\n",
      "Trained batch 198 batch loss 1.69436598 epoch total loss 1.73293853\n",
      "Trained batch 199 batch loss 1.65855038 epoch total loss 1.73256469\n",
      "Trained batch 200 batch loss 1.62860358 epoch total loss 1.73204482\n",
      "Trained batch 201 batch loss 1.61892211 epoch total loss 1.73148203\n",
      "Trained batch 202 batch loss 1.65512621 epoch total loss 1.73110402\n",
      "Trained batch 203 batch loss 1.6811769 epoch total loss 1.73085809\n",
      "Trained batch 204 batch loss 1.68341672 epoch total loss 1.73062551\n",
      "Trained batch 205 batch loss 1.67924619 epoch total loss 1.73037493\n",
      "Trained batch 206 batch loss 1.6549201 epoch total loss 1.7300086\n",
      "Trained batch 207 batch loss 1.68074644 epoch total loss 1.72977066\n",
      "Trained batch 208 batch loss 1.6403991 epoch total loss 1.72934103\n",
      "Trained batch 209 batch loss 1.5965631 epoch total loss 1.72870576\n",
      "Trained batch 210 batch loss 1.55408239 epoch total loss 1.72787416\n",
      "Trained batch 211 batch loss 1.69145787 epoch total loss 1.72770166\n",
      "Trained batch 212 batch loss 1.67754185 epoch total loss 1.72746503\n",
      "Trained batch 213 batch loss 1.64304197 epoch total loss 1.72706866\n",
      "Trained batch 214 batch loss 1.54191017 epoch total loss 1.72620344\n",
      "Trained batch 215 batch loss 1.51317668 epoch total loss 1.72521257\n",
      "Trained batch 216 batch loss 1.61523902 epoch total loss 1.72470343\n",
      "Trained batch 217 batch loss 1.68962419 epoch total loss 1.7245419\n",
      "Trained batch 218 batch loss 1.70950282 epoch total loss 1.72447288\n",
      "Trained batch 219 batch loss 1.77350616 epoch total loss 1.72469676\n",
      "Trained batch 220 batch loss 1.73686612 epoch total loss 1.72475207\n",
      "Trained batch 221 batch loss 1.54391265 epoch total loss 1.72393382\n",
      "Trained batch 222 batch loss 1.60482585 epoch total loss 1.72339737\n",
      "Trained batch 223 batch loss 1.58547843 epoch total loss 1.72277892\n",
      "Trained batch 224 batch loss 1.62206185 epoch total loss 1.72232926\n",
      "Trained batch 225 batch loss 1.61346436 epoch total loss 1.72184539\n",
      "Trained batch 226 batch loss 1.63360631 epoch total loss 1.72145498\n",
      "Trained batch 227 batch loss 1.65677977 epoch total loss 1.72117007\n",
      "Trained batch 228 batch loss 1.61762 epoch total loss 1.72071588\n",
      "Trained batch 229 batch loss 1.61808515 epoch total loss 1.72026765\n",
      "Trained batch 230 batch loss 1.66758299 epoch total loss 1.72003853\n",
      "Trained batch 231 batch loss 1.63202429 epoch total loss 1.71965742\n",
      "Trained batch 232 batch loss 1.65471256 epoch total loss 1.71937764\n",
      "Trained batch 233 batch loss 1.67719793 epoch total loss 1.71919656\n",
      "Trained batch 234 batch loss 1.66875339 epoch total loss 1.71898103\n",
      "Trained batch 235 batch loss 1.58391523 epoch total loss 1.7184062\n",
      "Trained batch 236 batch loss 1.65982592 epoch total loss 1.71815801\n",
      "Trained batch 237 batch loss 1.6180892 epoch total loss 1.71773589\n",
      "Trained batch 238 batch loss 1.64253736 epoch total loss 1.71741986\n",
      "Trained batch 239 batch loss 1.5734992 epoch total loss 1.71681774\n",
      "Trained batch 240 batch loss 1.60931778 epoch total loss 1.71636975\n",
      "Trained batch 241 batch loss 1.63447499 epoch total loss 1.71602988\n",
      "Trained batch 242 batch loss 1.63692403 epoch total loss 1.71570301\n",
      "Trained batch 243 batch loss 1.67062366 epoch total loss 1.71551752\n",
      "Trained batch 244 batch loss 1.64964736 epoch total loss 1.71524763\n",
      "Trained batch 245 batch loss 1.68831408 epoch total loss 1.71513772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 246 batch loss 1.5984807 epoch total loss 1.71466351\n",
      "Trained batch 247 batch loss 1.56826794 epoch total loss 1.7140708\n",
      "Trained batch 248 batch loss 1.58901894 epoch total loss 1.71356654\n",
      "Trained batch 249 batch loss 1.67823577 epoch total loss 1.71342456\n",
      "Trained batch 250 batch loss 1.63928592 epoch total loss 1.71312809\n",
      "Trained batch 251 batch loss 1.75545657 epoch total loss 1.71329677\n",
      "Trained batch 252 batch loss 1.71479034 epoch total loss 1.71330261\n",
      "Trained batch 253 batch loss 1.73568702 epoch total loss 1.71339107\n",
      "Trained batch 254 batch loss 1.74922431 epoch total loss 1.71353221\n",
      "Trained batch 255 batch loss 1.72498357 epoch total loss 1.71357703\n",
      "Trained batch 256 batch loss 1.76546836 epoch total loss 1.71377981\n",
      "Trained batch 257 batch loss 1.79186463 epoch total loss 1.71408367\n",
      "Trained batch 258 batch loss 1.70741212 epoch total loss 1.7140578\n",
      "Trained batch 259 batch loss 1.6593895 epoch total loss 1.71384668\n",
      "Trained batch 260 batch loss 1.62863278 epoch total loss 1.71351898\n",
      "Trained batch 261 batch loss 1.64201045 epoch total loss 1.71324492\n",
      "Trained batch 262 batch loss 1.70999122 epoch total loss 1.71323252\n",
      "Trained batch 263 batch loss 1.58960807 epoch total loss 1.71276236\n",
      "Trained batch 264 batch loss 1.5524857 epoch total loss 1.71215534\n",
      "Trained batch 265 batch loss 1.63055432 epoch total loss 1.71184742\n",
      "Trained batch 266 batch loss 1.61566854 epoch total loss 1.71148574\n",
      "Trained batch 267 batch loss 1.67166448 epoch total loss 1.71133661\n",
      "Trained batch 268 batch loss 1.51167846 epoch total loss 1.71059167\n",
      "Trained batch 269 batch loss 1.67660165 epoch total loss 1.71046531\n",
      "Trained batch 270 batch loss 1.47383177 epoch total loss 1.709589\n",
      "Trained batch 271 batch loss 1.64432406 epoch total loss 1.70934808\n",
      "Trained batch 272 batch loss 1.64613819 epoch total loss 1.70911574\n",
      "Trained batch 273 batch loss 1.67882895 epoch total loss 1.70900488\n",
      "Trained batch 274 batch loss 1.6891135 epoch total loss 1.70893228\n",
      "Trained batch 275 batch loss 1.7530688 epoch total loss 1.70909274\n",
      "Trained batch 276 batch loss 1.57064366 epoch total loss 1.70859122\n",
      "Trained batch 277 batch loss 1.51411986 epoch total loss 1.7078892\n",
      "Trained batch 278 batch loss 1.37648368 epoch total loss 1.70669711\n",
      "Trained batch 279 batch loss 1.66732216 epoch total loss 1.70655596\n",
      "Trained batch 280 batch loss 1.68135405 epoch total loss 1.70646608\n",
      "Trained batch 281 batch loss 1.6212604 epoch total loss 1.70616269\n",
      "Trained batch 282 batch loss 1.70411491 epoch total loss 1.70615542\n",
      "Trained batch 283 batch loss 1.68503511 epoch total loss 1.70608079\n",
      "Trained batch 284 batch loss 1.62440801 epoch total loss 1.70579326\n",
      "Trained batch 285 batch loss 1.60883522 epoch total loss 1.70545304\n",
      "Trained batch 286 batch loss 1.61906457 epoch total loss 1.70515096\n",
      "Trained batch 287 batch loss 1.65961933 epoch total loss 1.70499229\n",
      "Trained batch 288 batch loss 1.69936013 epoch total loss 1.70497274\n",
      "Trained batch 289 batch loss 1.65347433 epoch total loss 1.70479465\n",
      "Trained batch 290 batch loss 1.60283613 epoch total loss 1.7044431\n",
      "Trained batch 291 batch loss 1.54855704 epoch total loss 1.70390737\n",
      "Trained batch 292 batch loss 1.37896895 epoch total loss 1.70279455\n",
      "Trained batch 293 batch loss 1.4679203 epoch total loss 1.70199299\n",
      "Trained batch 294 batch loss 1.4958055 epoch total loss 1.70129168\n",
      "Trained batch 295 batch loss 1.39934158 epoch total loss 1.70026815\n",
      "Trained batch 296 batch loss 1.35733616 epoch total loss 1.69910955\n",
      "Trained batch 297 batch loss 1.31178665 epoch total loss 1.69780552\n",
      "Trained batch 298 batch loss 1.34110749 epoch total loss 1.69660842\n",
      "Trained batch 299 batch loss 1.64685071 epoch total loss 1.69644201\n",
      "Trained batch 300 batch loss 1.58903313 epoch total loss 1.69608402\n",
      "Trained batch 301 batch loss 1.5645411 epoch total loss 1.695647\n",
      "Trained batch 302 batch loss 1.57873154 epoch total loss 1.69525981\n",
      "Trained batch 303 batch loss 1.61837173 epoch total loss 1.69500613\n",
      "Trained batch 304 batch loss 1.43771935 epoch total loss 1.69415987\n",
      "Trained batch 305 batch loss 1.38631892 epoch total loss 1.6931504\n",
      "Trained batch 306 batch loss 1.28268957 epoch total loss 1.69180918\n",
      "Trained batch 307 batch loss 1.52181864 epoch total loss 1.69125533\n",
      "Trained batch 308 batch loss 1.7577635 epoch total loss 1.69147122\n",
      "Trained batch 309 batch loss 1.76797223 epoch total loss 1.6917187\n",
      "Trained batch 310 batch loss 1.61556697 epoch total loss 1.69147301\n",
      "Trained batch 311 batch loss 1.49020612 epoch total loss 1.69082594\n",
      "Trained batch 312 batch loss 1.59299946 epoch total loss 1.69051242\n",
      "Trained batch 313 batch loss 1.62095332 epoch total loss 1.69029021\n",
      "Trained batch 314 batch loss 1.73125064 epoch total loss 1.69042075\n",
      "Trained batch 315 batch loss 1.68633175 epoch total loss 1.69040775\n",
      "Trained batch 316 batch loss 1.65280485 epoch total loss 1.6902889\n",
      "Trained batch 317 batch loss 1.6610198 epoch total loss 1.69019651\n",
      "Trained batch 318 batch loss 1.62301862 epoch total loss 1.68998539\n",
      "Trained batch 319 batch loss 1.63906121 epoch total loss 1.68982565\n",
      "Trained batch 320 batch loss 1.64429665 epoch total loss 1.68968332\n",
      "Trained batch 321 batch loss 1.51918638 epoch total loss 1.68915212\n",
      "Trained batch 322 batch loss 1.63321102 epoch total loss 1.68897843\n",
      "Trained batch 323 batch loss 1.64867663 epoch total loss 1.68885374\n",
      "Trained batch 324 batch loss 1.51078224 epoch total loss 1.68830419\n",
      "Trained batch 325 batch loss 1.55364501 epoch total loss 1.68788981\n",
      "Trained batch 326 batch loss 1.53537011 epoch total loss 1.68742216\n",
      "Trained batch 327 batch loss 1.61405253 epoch total loss 1.6871978\n",
      "Trained batch 328 batch loss 1.67877293 epoch total loss 1.68717217\n",
      "Trained batch 329 batch loss 1.56695044 epoch total loss 1.68680668\n",
      "Trained batch 330 batch loss 1.67868185 epoch total loss 1.68678224\n",
      "Trained batch 331 batch loss 1.60498619 epoch total loss 1.686535\n",
      "Trained batch 332 batch loss 1.65799785 epoch total loss 1.68644917\n",
      "Trained batch 333 batch loss 1.63888156 epoch total loss 1.68630624\n",
      "Trained batch 334 batch loss 1.63715184 epoch total loss 1.68615901\n",
      "Trained batch 335 batch loss 1.59872508 epoch total loss 1.68589818\n",
      "Trained batch 336 batch loss 1.66263986 epoch total loss 1.68582892\n",
      "Trained batch 337 batch loss 1.64534342 epoch total loss 1.68570876\n",
      "Trained batch 338 batch loss 1.6557889 epoch total loss 1.68562019\n",
      "Trained batch 339 batch loss 1.65339494 epoch total loss 1.68552506\n",
      "Trained batch 340 batch loss 1.63114786 epoch total loss 1.6853652\n",
      "Trained batch 341 batch loss 1.60539269 epoch total loss 1.68513072\n",
      "Trained batch 342 batch loss 1.59238625 epoch total loss 1.68485963\n",
      "Trained batch 343 batch loss 1.61883616 epoch total loss 1.68466711\n",
      "Trained batch 344 batch loss 1.5874207 epoch total loss 1.68438435\n",
      "Trained batch 345 batch loss 1.46389675 epoch total loss 1.68374515\n",
      "Trained batch 346 batch loss 1.42946696 epoch total loss 1.68301022\n",
      "Trained batch 347 batch loss 1.4621557 epoch total loss 1.68237376\n",
      "Trained batch 348 batch loss 1.53581619 epoch total loss 1.6819526\n",
      "Trained batch 349 batch loss 1.54059637 epoch total loss 1.68154764\n",
      "Trained batch 350 batch loss 1.49742353 epoch total loss 1.68102157\n",
      "Trained batch 351 batch loss 1.60063469 epoch total loss 1.68079257\n",
      "Trained batch 352 batch loss 1.39574718 epoch total loss 1.67998278\n",
      "Trained batch 353 batch loss 1.44794512 epoch total loss 1.67932546\n",
      "Trained batch 354 batch loss 1.47449124 epoch total loss 1.67874682\n",
      "Trained batch 355 batch loss 1.42880011 epoch total loss 1.67804265\n",
      "Trained batch 356 batch loss 1.4948734 epoch total loss 1.67752814\n",
      "Trained batch 357 batch loss 1.59782887 epoch total loss 1.67730486\n",
      "Trained batch 358 batch loss 1.61486173 epoch total loss 1.67713046\n",
      "Trained batch 359 batch loss 1.59662199 epoch total loss 1.67690623\n",
      "Trained batch 360 batch loss 1.4867146 epoch total loss 1.67637789\n",
      "Trained batch 361 batch loss 1.4904989 epoch total loss 1.67586291\n",
      "Trained batch 362 batch loss 1.59970343 epoch total loss 1.67565262\n",
      "Trained batch 363 batch loss 1.6180048 epoch total loss 1.67549372\n",
      "Trained batch 364 batch loss 1.50819325 epoch total loss 1.67503405\n",
      "Trained batch 365 batch loss 1.59870458 epoch total loss 1.67482495\n",
      "Trained batch 366 batch loss 1.55116785 epoch total loss 1.67448699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 367 batch loss 1.53638172 epoch total loss 1.67411065\n",
      "Trained batch 368 batch loss 1.58618057 epoch total loss 1.67387176\n",
      "Trained batch 369 batch loss 1.53487062 epoch total loss 1.67349505\n",
      "Trained batch 370 batch loss 1.56887412 epoch total loss 1.67321217\n",
      "Trained batch 371 batch loss 1.56986165 epoch total loss 1.6729337\n",
      "Trained batch 372 batch loss 1.59860682 epoch total loss 1.6727339\n",
      "Trained batch 373 batch loss 1.63344026 epoch total loss 1.67262852\n",
      "Trained batch 374 batch loss 1.65685964 epoch total loss 1.67258632\n",
      "Trained batch 375 batch loss 1.74927795 epoch total loss 1.67279088\n",
      "Trained batch 376 batch loss 1.56212473 epoch total loss 1.67249656\n",
      "Trained batch 377 batch loss 1.50432348 epoch total loss 1.67205048\n",
      "Trained batch 378 batch loss 1.550946 epoch total loss 1.67173016\n",
      "Trained batch 379 batch loss 1.52350569 epoch total loss 1.67133904\n",
      "Trained batch 380 batch loss 1.57683587 epoch total loss 1.67109036\n",
      "Trained batch 381 batch loss 1.61033082 epoch total loss 1.67093098\n",
      "Trained batch 382 batch loss 1.52900338 epoch total loss 1.67055941\n",
      "Trained batch 383 batch loss 1.60004568 epoch total loss 1.67037523\n",
      "Trained batch 384 batch loss 1.61208189 epoch total loss 1.67022336\n",
      "Trained batch 385 batch loss 1.59576607 epoch total loss 1.67003\n",
      "Trained batch 386 batch loss 1.59554744 epoch total loss 1.669837\n",
      "Trained batch 387 batch loss 1.61963689 epoch total loss 1.66970718\n",
      "Trained batch 388 batch loss 1.58458805 epoch total loss 1.66948783\n",
      "Trained batch 389 batch loss 1.58771133 epoch total loss 1.66927767\n",
      "Trained batch 390 batch loss 1.55719566 epoch total loss 1.66899025\n",
      "Trained batch 391 batch loss 1.51597428 epoch total loss 1.66859889\n",
      "Trained batch 392 batch loss 1.59578526 epoch total loss 1.66841316\n",
      "Trained batch 393 batch loss 1.578969 epoch total loss 1.66818559\n",
      "Trained batch 394 batch loss 1.80449 epoch total loss 1.66853154\n",
      "Trained batch 395 batch loss 1.75811052 epoch total loss 1.66875839\n",
      "Trained batch 396 batch loss 1.70976663 epoch total loss 1.66886199\n",
      "Trained batch 397 batch loss 1.75892758 epoch total loss 1.66908872\n",
      "Trained batch 398 batch loss 1.62393224 epoch total loss 1.66897535\n",
      "Trained batch 399 batch loss 1.4855901 epoch total loss 1.6685158\n",
      "Trained batch 400 batch loss 1.4586761 epoch total loss 1.66799116\n",
      "Trained batch 401 batch loss 1.60981357 epoch total loss 1.66784608\n",
      "Trained batch 402 batch loss 1.62712383 epoch total loss 1.66774476\n",
      "Trained batch 403 batch loss 1.65651131 epoch total loss 1.66771686\n",
      "Trained batch 404 batch loss 1.53041577 epoch total loss 1.667377\n",
      "Trained batch 405 batch loss 1.49969959 epoch total loss 1.66696298\n",
      "Trained batch 406 batch loss 1.53535926 epoch total loss 1.66663873\n",
      "Trained batch 407 batch loss 1.61113107 epoch total loss 1.66650236\n",
      "Trained batch 408 batch loss 1.52083659 epoch total loss 1.66614532\n",
      "Trained batch 409 batch loss 1.553442 epoch total loss 1.66586983\n",
      "Trained batch 410 batch loss 1.5215652 epoch total loss 1.66551781\n",
      "Trained batch 411 batch loss 1.51637328 epoch total loss 1.66515493\n",
      "Trained batch 412 batch loss 1.57690692 epoch total loss 1.66494071\n",
      "Trained batch 413 batch loss 1.40817368 epoch total loss 1.66431904\n",
      "Trained batch 414 batch loss 1.43802285 epoch total loss 1.66377246\n",
      "Trained batch 415 batch loss 1.61600912 epoch total loss 1.66365743\n",
      "Trained batch 416 batch loss 1.57911301 epoch total loss 1.66345417\n",
      "Trained batch 417 batch loss 1.68282676 epoch total loss 1.66350055\n",
      "Trained batch 418 batch loss 1.66112196 epoch total loss 1.66349494\n",
      "Trained batch 419 batch loss 1.70672226 epoch total loss 1.66359806\n",
      "Trained batch 420 batch loss 1.62141895 epoch total loss 1.66349769\n",
      "Trained batch 421 batch loss 1.69289887 epoch total loss 1.66356742\n",
      "Trained batch 422 batch loss 1.67076898 epoch total loss 1.66358447\n",
      "Trained batch 423 batch loss 1.68612313 epoch total loss 1.66363776\n",
      "Trained batch 424 batch loss 1.67258227 epoch total loss 1.66365886\n",
      "Trained batch 425 batch loss 1.57747078 epoch total loss 1.66345608\n",
      "Trained batch 426 batch loss 1.56301391 epoch total loss 1.66322017\n",
      "Trained batch 427 batch loss 1.46649194 epoch total loss 1.66275942\n",
      "Trained batch 428 batch loss 1.45659399 epoch total loss 1.66227782\n",
      "Trained batch 429 batch loss 1.58570874 epoch total loss 1.66209924\n",
      "Trained batch 430 batch loss 1.67385721 epoch total loss 1.66212654\n",
      "Trained batch 431 batch loss 1.52141476 epoch total loss 1.66180015\n",
      "Trained batch 432 batch loss 1.61093712 epoch total loss 1.66168237\n",
      "Trained batch 433 batch loss 1.55825782 epoch total loss 1.66144347\n",
      "Trained batch 434 batch loss 1.59099817 epoch total loss 1.66128123\n",
      "Trained batch 435 batch loss 1.65131426 epoch total loss 1.66125822\n",
      "Trained batch 436 batch loss 1.67238355 epoch total loss 1.66128373\n",
      "Trained batch 437 batch loss 1.63514709 epoch total loss 1.66122389\n",
      "Trained batch 438 batch loss 1.60253298 epoch total loss 1.6610899\n",
      "Trained batch 439 batch loss 1.52940488 epoch total loss 1.66079\n",
      "Trained batch 440 batch loss 1.55202937 epoch total loss 1.66054273\n",
      "Trained batch 441 batch loss 1.63835335 epoch total loss 1.66049242\n",
      "Trained batch 442 batch loss 1.66536641 epoch total loss 1.66050339\n",
      "Trained batch 443 batch loss 1.73288119 epoch total loss 1.66066682\n",
      "Trained batch 444 batch loss 1.66239476 epoch total loss 1.66067076\n",
      "Trained batch 445 batch loss 1.68790841 epoch total loss 1.66073203\n",
      "Trained batch 446 batch loss 1.63118649 epoch total loss 1.66066575\n",
      "Trained batch 447 batch loss 1.60129738 epoch total loss 1.66053295\n",
      "Trained batch 448 batch loss 1.56072521 epoch total loss 1.66031015\n",
      "Trained batch 449 batch loss 1.51662707 epoch total loss 1.65999019\n",
      "Trained batch 450 batch loss 1.67137051 epoch total loss 1.66001546\n",
      "Trained batch 451 batch loss 1.57633543 epoch total loss 1.65983\n",
      "Trained batch 452 batch loss 1.6161319 epoch total loss 1.6597333\n",
      "Trained batch 453 batch loss 1.61502755 epoch total loss 1.65963471\n",
      "Trained batch 454 batch loss 1.56967306 epoch total loss 1.65943658\n",
      "Trained batch 455 batch loss 1.59845722 epoch total loss 1.65930259\n",
      "Trained batch 456 batch loss 1.58313608 epoch total loss 1.65913546\n",
      "Trained batch 457 batch loss 1.47650063 epoch total loss 1.65873587\n",
      "Trained batch 458 batch loss 1.64865327 epoch total loss 1.65871394\n",
      "Trained batch 459 batch loss 1.46600819 epoch total loss 1.65829408\n",
      "Trained batch 460 batch loss 1.39486146 epoch total loss 1.6577214\n",
      "Trained batch 461 batch loss 1.47832823 epoch total loss 1.65733218\n",
      "Trained batch 462 batch loss 1.43945158 epoch total loss 1.65686059\n",
      "Trained batch 463 batch loss 1.49641466 epoch total loss 1.65651405\n",
      "Trained batch 464 batch loss 1.62092805 epoch total loss 1.65643728\n",
      "Trained batch 465 batch loss 1.60455942 epoch total loss 1.6563257\n",
      "Trained batch 466 batch loss 1.62697601 epoch total loss 1.65626276\n",
      "Trained batch 467 batch loss 1.59835804 epoch total loss 1.65613866\n",
      "Trained batch 468 batch loss 1.57149 epoch total loss 1.6559577\n",
      "Trained batch 469 batch loss 1.55018592 epoch total loss 1.65573215\n",
      "Trained batch 470 batch loss 1.49893 epoch total loss 1.65539849\n",
      "Trained batch 471 batch loss 1.39004838 epoch total loss 1.65483522\n",
      "Trained batch 472 batch loss 1.52780104 epoch total loss 1.65456593\n",
      "Trained batch 473 batch loss 1.57888436 epoch total loss 1.65440595\n",
      "Trained batch 474 batch loss 1.56851888 epoch total loss 1.65422475\n",
      "Trained batch 475 batch loss 1.58567953 epoch total loss 1.65408051\n",
      "Trained batch 476 batch loss 1.57307482 epoch total loss 1.65391028\n",
      "Trained batch 477 batch loss 1.57107878 epoch total loss 1.65373671\n",
      "Trained batch 478 batch loss 1.56531811 epoch total loss 1.6535517\n",
      "Trained batch 479 batch loss 1.52347958 epoch total loss 1.65328014\n",
      "Trained batch 480 batch loss 1.56270361 epoch total loss 1.65309143\n",
      "Trained batch 481 batch loss 1.53863335 epoch total loss 1.65285349\n",
      "Trained batch 482 batch loss 1.66375756 epoch total loss 1.65287614\n",
      "Trained batch 483 batch loss 1.62849879 epoch total loss 1.65282559\n",
      "Trained batch 484 batch loss 1.49104047 epoch total loss 1.65249133\n",
      "Trained batch 485 batch loss 1.59323299 epoch total loss 1.65236914\n",
      "Trained batch 486 batch loss 1.61995363 epoch total loss 1.65230238\n",
      "Trained batch 487 batch loss 1.62100172 epoch total loss 1.65223813\n",
      "Trained batch 488 batch loss 1.54404426 epoch total loss 1.6520164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 489 batch loss 1.52815151 epoch total loss 1.65176308\n",
      "Trained batch 490 batch loss 1.45729887 epoch total loss 1.65136623\n",
      "Trained batch 491 batch loss 1.55878031 epoch total loss 1.65117764\n",
      "Trained batch 492 batch loss 1.5829438 epoch total loss 1.65103889\n",
      "Trained batch 493 batch loss 1.54176307 epoch total loss 1.65081728\n",
      "Trained batch 494 batch loss 1.53751147 epoch total loss 1.65058792\n",
      "Trained batch 495 batch loss 1.49491906 epoch total loss 1.65027344\n",
      "Trained batch 496 batch loss 1.5102421 epoch total loss 1.64999115\n",
      "Trained batch 497 batch loss 1.50495124 epoch total loss 1.64969933\n",
      "Trained batch 498 batch loss 1.54802728 epoch total loss 1.64949524\n",
      "Trained batch 499 batch loss 1.52241826 epoch total loss 1.64924049\n",
      "Trained batch 500 batch loss 1.60813117 epoch total loss 1.64915836\n",
      "Trained batch 501 batch loss 1.52063704 epoch total loss 1.64890182\n",
      "Trained batch 502 batch loss 1.56770968 epoch total loss 1.64874\n",
      "Trained batch 503 batch loss 1.45614791 epoch total loss 1.64835715\n",
      "Trained batch 504 batch loss 1.59018636 epoch total loss 1.64824176\n",
      "Trained batch 505 batch loss 1.71303666 epoch total loss 1.64837\n",
      "Trained batch 506 batch loss 1.7068541 epoch total loss 1.64848566\n",
      "Trained batch 507 batch loss 1.65734792 epoch total loss 1.64850307\n",
      "Trained batch 508 batch loss 1.607059 epoch total loss 1.64842153\n",
      "Trained batch 509 batch loss 1.5965426 epoch total loss 1.6483196\n",
      "Trained batch 510 batch loss 1.51250911 epoch total loss 1.64805329\n",
      "Trained batch 511 batch loss 1.6008116 epoch total loss 1.6479609\n",
      "Trained batch 512 batch loss 1.55280495 epoch total loss 1.64777505\n",
      "Trained batch 513 batch loss 1.59532416 epoch total loss 1.64767289\n",
      "Trained batch 514 batch loss 1.55964708 epoch total loss 1.64750159\n",
      "Trained batch 515 batch loss 1.66077125 epoch total loss 1.64752734\n",
      "Trained batch 516 batch loss 1.68219662 epoch total loss 1.64759445\n",
      "Trained batch 517 batch loss 1.60794389 epoch total loss 1.6475178\n",
      "Trained batch 518 batch loss 1.52223885 epoch total loss 1.64727592\n",
      "Trained batch 519 batch loss 1.52360332 epoch total loss 1.64703774\n",
      "Trained batch 520 batch loss 1.56208503 epoch total loss 1.64687431\n",
      "Trained batch 521 batch loss 1.48336673 epoch total loss 1.64656043\n",
      "Trained batch 522 batch loss 1.57179868 epoch total loss 1.64641714\n",
      "Trained batch 523 batch loss 1.64414966 epoch total loss 1.64641285\n",
      "Trained batch 524 batch loss 1.59489155 epoch total loss 1.6463145\n",
      "Trained batch 525 batch loss 1.48257542 epoch total loss 1.64600277\n",
      "Trained batch 526 batch loss 1.32312846 epoch total loss 1.64538884\n",
      "Trained batch 527 batch loss 1.42928696 epoch total loss 1.64497876\n",
      "Trained batch 528 batch loss 1.59605694 epoch total loss 1.64488614\n",
      "Trained batch 529 batch loss 1.57153416 epoch total loss 1.6447475\n",
      "Trained batch 530 batch loss 1.68041897 epoch total loss 1.64481473\n",
      "Trained batch 531 batch loss 1.64913106 epoch total loss 1.64482284\n",
      "Trained batch 532 batch loss 1.64291573 epoch total loss 1.64481938\n",
      "Trained batch 533 batch loss 1.6922133 epoch total loss 1.64490819\n",
      "Trained batch 534 batch loss 1.59631407 epoch total loss 1.64481723\n",
      "Trained batch 535 batch loss 1.60898602 epoch total loss 1.64475024\n",
      "Trained batch 536 batch loss 1.63590837 epoch total loss 1.64473379\n",
      "Trained batch 537 batch loss 1.48114109 epoch total loss 1.64442921\n",
      "Trained batch 538 batch loss 1.55259693 epoch total loss 1.6442585\n",
      "Trained batch 539 batch loss 1.63443184 epoch total loss 1.64424038\n",
      "Trained batch 540 batch loss 1.60966682 epoch total loss 1.64417636\n",
      "Trained batch 541 batch loss 1.59663856 epoch total loss 1.64408839\n",
      "Trained batch 542 batch loss 1.49521399 epoch total loss 1.64381385\n",
      "Trained batch 543 batch loss 1.58779216 epoch total loss 1.64371061\n",
      "Trained batch 544 batch loss 1.52940726 epoch total loss 1.64350045\n",
      "Trained batch 545 batch loss 1.52488422 epoch total loss 1.64328289\n",
      "Trained batch 546 batch loss 1.49166846 epoch total loss 1.64300513\n",
      "Trained batch 547 batch loss 1.55957508 epoch total loss 1.64285266\n",
      "Trained batch 548 batch loss 1.49348712 epoch total loss 1.64258\n",
      "Trained batch 549 batch loss 1.39222157 epoch total loss 1.64212394\n",
      "Trained batch 550 batch loss 1.54105663 epoch total loss 1.64194024\n",
      "Trained batch 551 batch loss 1.59195399 epoch total loss 1.64184964\n",
      "Trained batch 552 batch loss 1.59284186 epoch total loss 1.64176083\n",
      "Trained batch 553 batch loss 1.52996206 epoch total loss 1.64155865\n",
      "Trained batch 554 batch loss 1.49488604 epoch total loss 1.64129388\n",
      "Trained batch 555 batch loss 1.49004149 epoch total loss 1.64102137\n",
      "Trained batch 556 batch loss 1.47973716 epoch total loss 1.64073122\n",
      "Trained batch 557 batch loss 1.38523638 epoch total loss 1.64027262\n",
      "Trained batch 558 batch loss 1.52087831 epoch total loss 1.64005864\n",
      "Trained batch 559 batch loss 1.50611448 epoch total loss 1.63981903\n",
      "Trained batch 560 batch loss 1.51930892 epoch total loss 1.63960373\n",
      "Trained batch 561 batch loss 1.60027254 epoch total loss 1.63953364\n",
      "Trained batch 562 batch loss 1.53478229 epoch total loss 1.63934731\n",
      "Trained batch 563 batch loss 1.54165268 epoch total loss 1.63917375\n",
      "Trained batch 564 batch loss 1.51779974 epoch total loss 1.63895857\n",
      "Trained batch 565 batch loss 1.58997428 epoch total loss 1.63887179\n",
      "Trained batch 566 batch loss 1.48222291 epoch total loss 1.6385951\n",
      "Trained batch 567 batch loss 1.66152883 epoch total loss 1.63863552\n",
      "Trained batch 568 batch loss 1.50510621 epoch total loss 1.63840044\n",
      "Trained batch 569 batch loss 1.64372063 epoch total loss 1.63840985\n",
      "Trained batch 570 batch loss 1.60099602 epoch total loss 1.63834417\n",
      "Trained batch 571 batch loss 1.57376158 epoch total loss 1.63823116\n",
      "Trained batch 572 batch loss 1.51340139 epoch total loss 1.63801301\n",
      "Trained batch 573 batch loss 1.43800616 epoch total loss 1.63766384\n",
      "Trained batch 574 batch loss 1.54691887 epoch total loss 1.63750577\n",
      "Trained batch 575 batch loss 1.57463908 epoch total loss 1.63739645\n",
      "Trained batch 576 batch loss 1.54047465 epoch total loss 1.63722825\n",
      "Trained batch 577 batch loss 1.54156268 epoch total loss 1.63706243\n",
      "Trained batch 578 batch loss 1.52508318 epoch total loss 1.63686872\n",
      "Trained batch 579 batch loss 1.57832956 epoch total loss 1.63676751\n",
      "Trained batch 580 batch loss 1.5313648 epoch total loss 1.63658583\n",
      "Trained batch 581 batch loss 1.49626708 epoch total loss 1.63634431\n",
      "Trained batch 582 batch loss 1.47420263 epoch total loss 1.63606572\n",
      "Trained batch 583 batch loss 1.50627565 epoch total loss 1.63584316\n",
      "Trained batch 584 batch loss 1.6596905 epoch total loss 1.63588393\n",
      "Trained batch 585 batch loss 1.59374201 epoch total loss 1.63581192\n",
      "Trained batch 586 batch loss 1.53420985 epoch total loss 1.63563848\n",
      "Trained batch 587 batch loss 1.56379986 epoch total loss 1.63551605\n",
      "Trained batch 588 batch loss 1.51120949 epoch total loss 1.63530469\n",
      "Trained batch 589 batch loss 1.46563148 epoch total loss 1.63501656\n",
      "Trained batch 590 batch loss 1.61018598 epoch total loss 1.63497448\n",
      "Trained batch 591 batch loss 1.59509015 epoch total loss 1.63490701\n",
      "Trained batch 592 batch loss 1.54856753 epoch total loss 1.63476121\n",
      "Trained batch 593 batch loss 1.56296623 epoch total loss 1.63464022\n",
      "Trained batch 594 batch loss 1.45982647 epoch total loss 1.63434589\n",
      "Trained batch 595 batch loss 1.4430896 epoch total loss 1.6340245\n",
      "Trained batch 596 batch loss 1.49290419 epoch total loss 1.63378775\n",
      "Trained batch 597 batch loss 1.52576685 epoch total loss 1.63360679\n",
      "Trained batch 598 batch loss 1.59318089 epoch total loss 1.6335392\n",
      "Trained batch 599 batch loss 1.65716684 epoch total loss 1.63357866\n",
      "Trained batch 600 batch loss 1.70081377 epoch total loss 1.63369071\n",
      "Trained batch 601 batch loss 1.60158634 epoch total loss 1.63363719\n",
      "Trained batch 602 batch loss 1.74157572 epoch total loss 1.63381648\n",
      "Trained batch 603 batch loss 1.47586513 epoch total loss 1.6335547\n",
      "Trained batch 604 batch loss 1.37251675 epoch total loss 1.63312244\n",
      "Trained batch 605 batch loss 1.33701861 epoch total loss 1.63263297\n",
      "Trained batch 606 batch loss 1.24562502 epoch total loss 1.63199437\n",
      "Trained batch 607 batch loss 1.40573764 epoch total loss 1.63162172\n",
      "Trained batch 608 batch loss 1.58545387 epoch total loss 1.63154566\n",
      "Trained batch 609 batch loss 1.43267763 epoch total loss 1.63121915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 610 batch loss 1.54656923 epoch total loss 1.63108039\n",
      "Trained batch 611 batch loss 1.5438453 epoch total loss 1.63093758\n",
      "Trained batch 612 batch loss 1.52283013 epoch total loss 1.63076091\n",
      "Trained batch 613 batch loss 1.56884742 epoch total loss 1.63065994\n",
      "Trained batch 614 batch loss 1.41686726 epoch total loss 1.63031173\n",
      "Trained batch 615 batch loss 1.60596108 epoch total loss 1.63027215\n",
      "Trained batch 616 batch loss 1.5749681 epoch total loss 1.63018239\n",
      "Trained batch 617 batch loss 1.62101102 epoch total loss 1.63016748\n",
      "Trained batch 618 batch loss 1.64091456 epoch total loss 1.63018489\n",
      "Trained batch 619 batch loss 1.52355623 epoch total loss 1.63001263\n",
      "Trained batch 620 batch loss 1.53095984 epoch total loss 1.62985289\n",
      "Trained batch 621 batch loss 1.40883541 epoch total loss 1.62949693\n",
      "Trained batch 622 batch loss 1.56665206 epoch total loss 1.62939596\n",
      "Trained batch 623 batch loss 1.47828364 epoch total loss 1.62915337\n",
      "Trained batch 624 batch loss 1.50751507 epoch total loss 1.62895834\n",
      "Trained batch 625 batch loss 1.45037436 epoch total loss 1.6286726\n",
      "Trained batch 626 batch loss 1.51046896 epoch total loss 1.62848389\n",
      "Trained batch 627 batch loss 1.5425266 epoch total loss 1.6283468\n",
      "Trained batch 628 batch loss 1.52986 epoch total loss 1.62818992\n",
      "Trained batch 629 batch loss 1.4940958 epoch total loss 1.62797678\n",
      "Trained batch 630 batch loss 1.54407597 epoch total loss 1.62784362\n",
      "Trained batch 631 batch loss 1.54981136 epoch total loss 1.62772\n",
      "Trained batch 632 batch loss 1.51857018 epoch total loss 1.62754726\n",
      "Trained batch 633 batch loss 1.57670331 epoch total loss 1.62746692\n",
      "Trained batch 634 batch loss 1.52879977 epoch total loss 1.62731123\n",
      "Trained batch 635 batch loss 1.50000501 epoch total loss 1.62711072\n",
      "Trained batch 636 batch loss 1.48136926 epoch total loss 1.62688148\n",
      "Trained batch 637 batch loss 1.47681963 epoch total loss 1.62664592\n",
      "Trained batch 638 batch loss 1.51974893 epoch total loss 1.62647843\n",
      "Trained batch 639 batch loss 1.57864285 epoch total loss 1.62640357\n",
      "Trained batch 640 batch loss 1.55305374 epoch total loss 1.62628901\n",
      "Trained batch 641 batch loss 1.57502902 epoch total loss 1.62620914\n",
      "Trained batch 642 batch loss 1.61139226 epoch total loss 1.62618613\n",
      "Trained batch 643 batch loss 1.61676407 epoch total loss 1.62617159\n",
      "Trained batch 644 batch loss 1.50911009 epoch total loss 1.62598979\n",
      "Trained batch 645 batch loss 1.54734731 epoch total loss 1.62586796\n",
      "Trained batch 646 batch loss 1.50318193 epoch total loss 1.62567794\n",
      "Trained batch 647 batch loss 1.54267311 epoch total loss 1.62554979\n",
      "Trained batch 648 batch loss 1.48998976 epoch total loss 1.62534058\n",
      "Trained batch 649 batch loss 1.41275012 epoch total loss 1.62501299\n",
      "Trained batch 650 batch loss 1.47490823 epoch total loss 1.62478197\n",
      "Trained batch 651 batch loss 1.60971534 epoch total loss 1.62475884\n",
      "Trained batch 652 batch loss 1.49314237 epoch total loss 1.62455702\n",
      "Trained batch 653 batch loss 1.48651445 epoch total loss 1.62434566\n",
      "Trained batch 654 batch loss 1.63984668 epoch total loss 1.6243695\n",
      "Trained batch 655 batch loss 1.46930814 epoch total loss 1.62413287\n",
      "Trained batch 656 batch loss 1.47825551 epoch total loss 1.62391043\n",
      "Trained batch 657 batch loss 1.4345417 epoch total loss 1.6236223\n",
      "Trained batch 658 batch loss 1.49890983 epoch total loss 1.62343276\n",
      "Trained batch 659 batch loss 1.5539335 epoch total loss 1.62332737\n",
      "Trained batch 660 batch loss 1.49839127 epoch total loss 1.62313807\n",
      "Trained batch 661 batch loss 1.4973197 epoch total loss 1.62294769\n",
      "Trained batch 662 batch loss 1.46734571 epoch total loss 1.62271261\n",
      "Trained batch 663 batch loss 1.48700476 epoch total loss 1.62250793\n",
      "Trained batch 664 batch loss 1.39592648 epoch total loss 1.62216663\n",
      "Trained batch 665 batch loss 1.45301735 epoch total loss 1.62191224\n",
      "Trained batch 666 batch loss 1.51117587 epoch total loss 1.62174606\n",
      "Trained batch 667 batch loss 1.53642249 epoch total loss 1.62161803\n",
      "Trained batch 668 batch loss 1.60788274 epoch total loss 1.62159753\n",
      "Trained batch 669 batch loss 1.71318781 epoch total loss 1.62173438\n",
      "Trained batch 670 batch loss 1.63250601 epoch total loss 1.62175035\n",
      "Trained batch 671 batch loss 1.53472972 epoch total loss 1.62162077\n",
      "Trained batch 672 batch loss 1.48283613 epoch total loss 1.62141418\n",
      "Trained batch 673 batch loss 1.54157162 epoch total loss 1.62129557\n",
      "Trained batch 674 batch loss 1.54164231 epoch total loss 1.62117743\n",
      "Trained batch 675 batch loss 1.48150086 epoch total loss 1.62097037\n",
      "Trained batch 676 batch loss 1.51786494 epoch total loss 1.62081778\n",
      "Trained batch 677 batch loss 1.55547428 epoch total loss 1.62072122\n",
      "Trained batch 678 batch loss 1.50072372 epoch total loss 1.6205442\n",
      "Trained batch 679 batch loss 1.53023827 epoch total loss 1.62041128\n",
      "Trained batch 680 batch loss 1.4969269 epoch total loss 1.62022972\n",
      "Trained batch 681 batch loss 1.54217064 epoch total loss 1.62011504\n",
      "Trained batch 682 batch loss 1.46011508 epoch total loss 1.61988044\n",
      "Trained batch 683 batch loss 1.50282741 epoch total loss 1.61970901\n",
      "Trained batch 684 batch loss 1.48852861 epoch total loss 1.61951721\n",
      "Trained batch 685 batch loss 1.45099878 epoch total loss 1.61927128\n",
      "Trained batch 686 batch loss 1.61193514 epoch total loss 1.61926055\n",
      "Trained batch 687 batch loss 1.58540344 epoch total loss 1.61921132\n",
      "Trained batch 688 batch loss 1.58829236 epoch total loss 1.61916637\n",
      "Trained batch 689 batch loss 1.69856441 epoch total loss 1.61928165\n",
      "Trained batch 690 batch loss 1.7266084 epoch total loss 1.6194371\n",
      "Trained batch 691 batch loss 1.66294289 epoch total loss 1.61950016\n",
      "Trained batch 692 batch loss 1.6254549 epoch total loss 1.61950874\n",
      "Trained batch 693 batch loss 1.53099394 epoch total loss 1.61938107\n",
      "Trained batch 694 batch loss 1.4708482 epoch total loss 1.61916697\n",
      "Trained batch 695 batch loss 1.57300663 epoch total loss 1.61910057\n",
      "Trained batch 696 batch loss 1.46662962 epoch total loss 1.61888158\n",
      "Trained batch 697 batch loss 1.34949243 epoch total loss 1.61849511\n",
      "Trained batch 698 batch loss 1.33491302 epoch total loss 1.61808884\n",
      "Trained batch 699 batch loss 1.33873916 epoch total loss 1.61768925\n",
      "Trained batch 700 batch loss 1.49847388 epoch total loss 1.61751878\n",
      "Trained batch 701 batch loss 1.55591893 epoch total loss 1.61743093\n",
      "Trained batch 702 batch loss 1.69946146 epoch total loss 1.61754775\n",
      "Trained batch 703 batch loss 1.61795759 epoch total loss 1.61754835\n",
      "Trained batch 704 batch loss 1.63436365 epoch total loss 1.61757231\n",
      "Trained batch 705 batch loss 1.56354296 epoch total loss 1.61749566\n",
      "Trained batch 706 batch loss 1.63239479 epoch total loss 1.61751688\n",
      "Trained batch 707 batch loss 1.56853259 epoch total loss 1.6174475\n",
      "Trained batch 708 batch loss 1.56158662 epoch total loss 1.6173687\n",
      "Trained batch 709 batch loss 1.57879972 epoch total loss 1.61731434\n",
      "Trained batch 710 batch loss 1.63981581 epoch total loss 1.61734605\n",
      "Trained batch 711 batch loss 1.6775136 epoch total loss 1.61743057\n",
      "Trained batch 712 batch loss 1.60434079 epoch total loss 1.61741221\n",
      "Trained batch 713 batch loss 1.5199759 epoch total loss 1.61727571\n",
      "Trained batch 714 batch loss 1.41594064 epoch total loss 1.61699367\n",
      "Trained batch 715 batch loss 1.47735572 epoch total loss 1.61679828\n",
      "Trained batch 716 batch loss 1.51579654 epoch total loss 1.61665714\n",
      "Trained batch 717 batch loss 1.51388633 epoch total loss 1.61651385\n",
      "Trained batch 718 batch loss 1.43366289 epoch total loss 1.61625922\n",
      "Trained batch 719 batch loss 1.30998194 epoch total loss 1.61583316\n",
      "Trained batch 720 batch loss 1.39235246 epoch total loss 1.61552274\n",
      "Trained batch 721 batch loss 1.52951419 epoch total loss 1.61540353\n",
      "Trained batch 722 batch loss 1.61521649 epoch total loss 1.61540329\n",
      "Trained batch 723 batch loss 1.56454587 epoch total loss 1.61533296\n",
      "Trained batch 724 batch loss 1.54947948 epoch total loss 1.615242\n",
      "Trained batch 725 batch loss 1.5275135 epoch total loss 1.61512089\n",
      "Trained batch 726 batch loss 1.49059272 epoch total loss 1.61494935\n",
      "Trained batch 727 batch loss 1.52332973 epoch total loss 1.61482334\n",
      "Trained batch 728 batch loss 1.44208527 epoch total loss 1.61458611\n",
      "Trained batch 729 batch loss 1.40588236 epoch total loss 1.61429989\n",
      "Trained batch 730 batch loss 1.50175285 epoch total loss 1.61414564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 731 batch loss 1.49203038 epoch total loss 1.61397862\n",
      "Trained batch 732 batch loss 1.49888706 epoch total loss 1.61382139\n",
      "Trained batch 733 batch loss 1.52443421 epoch total loss 1.61369944\n",
      "Trained batch 734 batch loss 1.52727389 epoch total loss 1.61358166\n",
      "Trained batch 735 batch loss 1.51647568 epoch total loss 1.61344945\n",
      "Trained batch 736 batch loss 1.55522847 epoch total loss 1.6133703\n",
      "Trained batch 737 batch loss 1.56713438 epoch total loss 1.6133076\n",
      "Trained batch 738 batch loss 1.53100204 epoch total loss 1.61319602\n",
      "Trained batch 739 batch loss 1.48198295 epoch total loss 1.61301839\n",
      "Trained batch 740 batch loss 1.49433529 epoch total loss 1.61285818\n",
      "Trained batch 741 batch loss 1.42188406 epoch total loss 1.61260045\n",
      "Trained batch 742 batch loss 1.48273516 epoch total loss 1.61242545\n",
      "Trained batch 743 batch loss 1.56376219 epoch total loss 1.61235988\n",
      "Trained batch 744 batch loss 1.5630784 epoch total loss 1.61229372\n",
      "Trained batch 745 batch loss 1.51502275 epoch total loss 1.61216307\n",
      "Trained batch 746 batch loss 1.47707212 epoch total loss 1.61198199\n",
      "Trained batch 747 batch loss 1.40070724 epoch total loss 1.61169922\n",
      "Trained batch 748 batch loss 1.53290141 epoch total loss 1.61159396\n",
      "Trained batch 749 batch loss 1.46061754 epoch total loss 1.61139238\n",
      "Trained batch 750 batch loss 1.45422411 epoch total loss 1.61118281\n",
      "Trained batch 751 batch loss 1.39637756 epoch total loss 1.61089671\n",
      "Trained batch 752 batch loss 1.54116154 epoch total loss 1.61080396\n",
      "Trained batch 753 batch loss 1.53074992 epoch total loss 1.61069763\n",
      "Trained batch 754 batch loss 1.49415576 epoch total loss 1.61054313\n",
      "Trained batch 755 batch loss 1.5225594 epoch total loss 1.61042655\n",
      "Trained batch 756 batch loss 1.58800352 epoch total loss 1.61039698\n",
      "Trained batch 757 batch loss 1.54971969 epoch total loss 1.61031675\n",
      "Trained batch 758 batch loss 1.39051628 epoch total loss 1.61002672\n",
      "Trained batch 759 batch loss 1.53324473 epoch total loss 1.60992551\n",
      "Trained batch 760 batch loss 1.44378912 epoch total loss 1.609707\n",
      "Trained batch 761 batch loss 1.59934628 epoch total loss 1.60969341\n",
      "Trained batch 762 batch loss 1.69331145 epoch total loss 1.6098032\n",
      "Trained batch 763 batch loss 1.64410508 epoch total loss 1.60984826\n",
      "Trained batch 764 batch loss 1.65781951 epoch total loss 1.60991108\n",
      "Trained batch 765 batch loss 1.50619602 epoch total loss 1.60977554\n",
      "Trained batch 766 batch loss 1.53830898 epoch total loss 1.6096822\n",
      "Trained batch 767 batch loss 1.508147 epoch total loss 1.60954988\n",
      "Trained batch 768 batch loss 1.51920474 epoch total loss 1.60943222\n",
      "Trained batch 769 batch loss 1.54766726 epoch total loss 1.60935187\n",
      "Trained batch 770 batch loss 1.60709476 epoch total loss 1.60934889\n",
      "Trained batch 771 batch loss 1.45957279 epoch total loss 1.60915458\n",
      "Trained batch 772 batch loss 1.44907296 epoch total loss 1.60894728\n",
      "Trained batch 773 batch loss 1.52125371 epoch total loss 1.60883379\n",
      "Trained batch 774 batch loss 1.52237177 epoch total loss 1.60872209\n",
      "Trained batch 775 batch loss 1.63019121 epoch total loss 1.60874987\n",
      "Trained batch 776 batch loss 1.68808675 epoch total loss 1.60885215\n",
      "Trained batch 777 batch loss 1.66537452 epoch total loss 1.60892487\n",
      "Trained batch 778 batch loss 1.54946876 epoch total loss 1.60884845\n",
      "Trained batch 779 batch loss 1.58017755 epoch total loss 1.60881162\n",
      "Trained batch 780 batch loss 1.65024567 epoch total loss 1.60886478\n",
      "Trained batch 781 batch loss 1.68294036 epoch total loss 1.60895967\n",
      "Trained batch 782 batch loss 1.57243347 epoch total loss 1.60891294\n",
      "Trained batch 783 batch loss 1.50698471 epoch total loss 1.60878277\n",
      "Trained batch 784 batch loss 1.60833848 epoch total loss 1.60878229\n",
      "Trained batch 785 batch loss 1.59454942 epoch total loss 1.60876417\n",
      "Trained batch 786 batch loss 1.51911044 epoch total loss 1.60865021\n",
      "Trained batch 787 batch loss 1.51622498 epoch total loss 1.60853279\n",
      "Trained batch 788 batch loss 1.53750193 epoch total loss 1.60844254\n",
      "Trained batch 789 batch loss 1.52343965 epoch total loss 1.6083349\n",
      "Trained batch 790 batch loss 1.52249503 epoch total loss 1.60822618\n",
      "Trained batch 791 batch loss 1.52771056 epoch total loss 1.60812438\n",
      "Trained batch 792 batch loss 1.60539675 epoch total loss 1.6081208\n",
      "Trained batch 793 batch loss 1.63202929 epoch total loss 1.60815108\n",
      "Trained batch 794 batch loss 1.61315215 epoch total loss 1.6081574\n",
      "Trained batch 795 batch loss 1.57594872 epoch total loss 1.60811687\n",
      "Trained batch 796 batch loss 1.59622562 epoch total loss 1.60810184\n",
      "Trained batch 797 batch loss 1.58973861 epoch total loss 1.60807884\n",
      "Trained batch 798 batch loss 1.57548809 epoch total loss 1.60803783\n",
      "Trained batch 799 batch loss 1.4891485 epoch total loss 1.60788906\n",
      "Trained batch 800 batch loss 1.52563202 epoch total loss 1.6077863\n",
      "Trained batch 801 batch loss 1.51767397 epoch total loss 1.60767376\n",
      "Trained batch 802 batch loss 1.52016675 epoch total loss 1.60756469\n",
      "Trained batch 803 batch loss 1.51623166 epoch total loss 1.60745096\n",
      "Trained batch 804 batch loss 1.57181442 epoch total loss 1.6074065\n",
      "Trained batch 805 batch loss 1.55875754 epoch total loss 1.60734606\n",
      "Trained batch 806 batch loss 1.45676565 epoch total loss 1.60715926\n",
      "Trained batch 807 batch loss 1.49423933 epoch total loss 1.60701931\n",
      "Trained batch 808 batch loss 1.46169722 epoch total loss 1.60683942\n",
      "Trained batch 809 batch loss 1.48753858 epoch total loss 1.60669208\n",
      "Trained batch 810 batch loss 1.43889463 epoch total loss 1.60648477\n",
      "Trained batch 811 batch loss 1.45303893 epoch total loss 1.60629559\n",
      "Trained batch 812 batch loss 1.48055601 epoch total loss 1.60614073\n",
      "Trained batch 813 batch loss 1.51079631 epoch total loss 1.60602343\n",
      "Trained batch 814 batch loss 1.44425297 epoch total loss 1.60582459\n",
      "Trained batch 815 batch loss 1.43963075 epoch total loss 1.60562062\n",
      "Trained batch 816 batch loss 1.50988197 epoch total loss 1.60550332\n",
      "Trained batch 817 batch loss 1.56049848 epoch total loss 1.60544825\n",
      "Trained batch 818 batch loss 1.5527041 epoch total loss 1.60538387\n",
      "Trained batch 819 batch loss 1.61873174 epoch total loss 1.6054002\n",
      "Trained batch 820 batch loss 1.59727669 epoch total loss 1.60539031\n",
      "Trained batch 821 batch loss 1.5111742 epoch total loss 1.60527563\n",
      "Trained batch 822 batch loss 1.51677477 epoch total loss 1.60516787\n",
      "Trained batch 823 batch loss 1.55277503 epoch total loss 1.60510421\n",
      "Trained batch 824 batch loss 1.57241333 epoch total loss 1.60506451\n",
      "Trained batch 825 batch loss 1.48263168 epoch total loss 1.6049161\n",
      "Trained batch 826 batch loss 1.34183729 epoch total loss 1.60459757\n",
      "Trained batch 827 batch loss 1.57532454 epoch total loss 1.60456216\n",
      "Trained batch 828 batch loss 1.45756662 epoch total loss 1.60438454\n",
      "Trained batch 829 batch loss 1.53552222 epoch total loss 1.60430145\n",
      "Trained batch 830 batch loss 1.42389834 epoch total loss 1.60408425\n",
      "Trained batch 831 batch loss 1.42169631 epoch total loss 1.60386479\n",
      "Trained batch 832 batch loss 1.41869426 epoch total loss 1.60364223\n",
      "Trained batch 833 batch loss 1.4319526 epoch total loss 1.60343623\n",
      "Trained batch 834 batch loss 1.40251541 epoch total loss 1.60319519\n",
      "Trained batch 835 batch loss 1.43282497 epoch total loss 1.60299122\n",
      "Trained batch 836 batch loss 1.45028019 epoch total loss 1.60280859\n",
      "Trained batch 837 batch loss 1.49959838 epoch total loss 1.60268533\n",
      "Trained batch 838 batch loss 1.39261937 epoch total loss 1.60243464\n",
      "Trained batch 839 batch loss 1.36709011 epoch total loss 1.60215414\n",
      "Trained batch 840 batch loss 1.46488059 epoch total loss 1.60199058\n",
      "Trained batch 841 batch loss 1.34813046 epoch total loss 1.60168874\n",
      "Trained batch 842 batch loss 1.48062944 epoch total loss 1.60154498\n",
      "Trained batch 843 batch loss 1.50700724 epoch total loss 1.6014328\n",
      "Trained batch 844 batch loss 1.5456146 epoch total loss 1.60136664\n",
      "Trained batch 845 batch loss 1.48068595 epoch total loss 1.60122383\n",
      "Trained batch 846 batch loss 1.45164502 epoch total loss 1.60104704\n",
      "Trained batch 847 batch loss 1.53171587 epoch total loss 1.60096526\n",
      "Trained batch 848 batch loss 1.49495888 epoch total loss 1.60084033\n",
      "Trained batch 849 batch loss 1.55003846 epoch total loss 1.60078049\n",
      "Trained batch 850 batch loss 1.5071702 epoch total loss 1.60067034\n",
      "Trained batch 851 batch loss 1.50899792 epoch total loss 1.60056269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 852 batch loss 1.42226899 epoch total loss 1.60035336\n",
      "Trained batch 853 batch loss 1.45048952 epoch total loss 1.60017765\n",
      "Trained batch 854 batch loss 1.51323581 epoch total loss 1.60007584\n",
      "Trained batch 855 batch loss 1.52100515 epoch total loss 1.59998333\n",
      "Trained batch 856 batch loss 1.52252936 epoch total loss 1.59989285\n",
      "Trained batch 857 batch loss 1.47106051 epoch total loss 1.59974253\n",
      "Trained batch 858 batch loss 1.46646655 epoch total loss 1.5995872\n",
      "Trained batch 859 batch loss 1.43645859 epoch total loss 1.59939718\n",
      "Trained batch 860 batch loss 1.48613167 epoch total loss 1.59926546\n",
      "Trained batch 861 batch loss 1.54981112 epoch total loss 1.599208\n",
      "Trained batch 862 batch loss 1.54609 epoch total loss 1.59914649\n",
      "Trained batch 863 batch loss 1.54319382 epoch total loss 1.59908164\n",
      "Trained batch 864 batch loss 1.6106286 epoch total loss 1.59909499\n",
      "Trained batch 865 batch loss 1.52918506 epoch total loss 1.59901416\n",
      "Trained batch 866 batch loss 1.56714988 epoch total loss 1.59897733\n",
      "Trained batch 867 batch loss 1.44507051 epoch total loss 1.59879982\n",
      "Trained batch 868 batch loss 1.51897657 epoch total loss 1.5987078\n",
      "Trained batch 869 batch loss 1.41925025 epoch total loss 1.59850121\n",
      "Trained batch 870 batch loss 1.38846135 epoch total loss 1.59825969\n",
      "Trained batch 871 batch loss 1.39477825 epoch total loss 1.59802616\n",
      "Trained batch 872 batch loss 1.29746449 epoch total loss 1.5976814\n",
      "Trained batch 873 batch loss 1.46583724 epoch total loss 1.59753036\n",
      "Trained batch 874 batch loss 1.72922981 epoch total loss 1.59768116\n",
      "Trained batch 875 batch loss 1.57105589 epoch total loss 1.59765065\n",
      "Trained batch 876 batch loss 1.49425483 epoch total loss 1.59753263\n",
      "Trained batch 877 batch loss 1.51697206 epoch total loss 1.59744084\n",
      "Trained batch 878 batch loss 1.57190681 epoch total loss 1.59741175\n",
      "Trained batch 879 batch loss 1.48282814 epoch total loss 1.59728134\n",
      "Trained batch 880 batch loss 1.46895623 epoch total loss 1.59713554\n",
      "Trained batch 881 batch loss 1.46951783 epoch total loss 1.59699059\n",
      "Trained batch 882 batch loss 1.48281467 epoch total loss 1.59686112\n",
      "Trained batch 883 batch loss 1.4824183 epoch total loss 1.59673154\n",
      "Trained batch 884 batch loss 1.52713156 epoch total loss 1.59665275\n",
      "Trained batch 885 batch loss 1.50531411 epoch total loss 1.59654963\n",
      "Trained batch 886 batch loss 1.47200727 epoch total loss 1.59640908\n",
      "Trained batch 887 batch loss 1.45757627 epoch total loss 1.59625256\n",
      "Trained batch 888 batch loss 1.48632193 epoch total loss 1.5961287\n",
      "Trained batch 889 batch loss 1.44187713 epoch total loss 1.59595525\n",
      "Trained batch 890 batch loss 1.49128151 epoch total loss 1.59583771\n",
      "Trained batch 891 batch loss 1.41762877 epoch total loss 1.59563768\n",
      "Trained batch 892 batch loss 1.43838298 epoch total loss 1.59546137\n",
      "Trained batch 893 batch loss 1.48357272 epoch total loss 1.59533596\n",
      "Trained batch 894 batch loss 1.44001293 epoch total loss 1.59516227\n",
      "Trained batch 895 batch loss 1.44354677 epoch total loss 1.59499288\n",
      "Trained batch 896 batch loss 1.47472 epoch total loss 1.59485877\n",
      "Trained batch 897 batch loss 1.36424375 epoch total loss 1.59460163\n",
      "Trained batch 898 batch loss 1.27863121 epoch total loss 1.59424984\n",
      "Trained batch 899 batch loss 1.26893616 epoch total loss 1.59388793\n",
      "Trained batch 900 batch loss 1.31676888 epoch total loss 1.59358\n",
      "Trained batch 901 batch loss 1.47572351 epoch total loss 1.59344923\n",
      "Trained batch 902 batch loss 1.57533944 epoch total loss 1.59342909\n",
      "Trained batch 903 batch loss 1.42199147 epoch total loss 1.59323931\n",
      "Trained batch 904 batch loss 1.57135451 epoch total loss 1.59321511\n",
      "Trained batch 905 batch loss 1.48291326 epoch total loss 1.59309328\n",
      "Trained batch 906 batch loss 1.42459369 epoch total loss 1.59290719\n",
      "Trained batch 907 batch loss 1.3621242 epoch total loss 1.5926528\n",
      "Trained batch 908 batch loss 1.44116271 epoch total loss 1.59248602\n",
      "Trained batch 909 batch loss 1.45154834 epoch total loss 1.59233093\n",
      "Trained batch 910 batch loss 1.48143923 epoch total loss 1.5922091\n",
      "Trained batch 911 batch loss 1.30398345 epoch total loss 1.59189272\n",
      "Trained batch 912 batch loss 1.40481615 epoch total loss 1.59168756\n",
      "Trained batch 913 batch loss 1.3581934 epoch total loss 1.59143174\n",
      "Trained batch 914 batch loss 1.50577354 epoch total loss 1.59133792\n",
      "Trained batch 915 batch loss 1.57568669 epoch total loss 1.59132087\n",
      "Trained batch 916 batch loss 1.48434091 epoch total loss 1.59120417\n",
      "Trained batch 917 batch loss 1.45805526 epoch total loss 1.59105885\n",
      "Trained batch 918 batch loss 1.35368681 epoch total loss 1.59080029\n",
      "Trained batch 919 batch loss 1.45591605 epoch total loss 1.59065342\n",
      "Trained batch 920 batch loss 1.52710283 epoch total loss 1.5905844\n",
      "Trained batch 921 batch loss 1.54433703 epoch total loss 1.59053421\n",
      "Trained batch 922 batch loss 1.46102679 epoch total loss 1.59039378\n",
      "Trained batch 923 batch loss 1.44057262 epoch total loss 1.59023142\n",
      "Trained batch 924 batch loss 1.4409678 epoch total loss 1.59006977\n",
      "Trained batch 925 batch loss 1.40425444 epoch total loss 1.5898689\n",
      "Trained batch 926 batch loss 1.52712142 epoch total loss 1.58980119\n",
      "Trained batch 927 batch loss 1.51998317 epoch total loss 1.58972585\n",
      "Trained batch 928 batch loss 1.52073383 epoch total loss 1.58965158\n",
      "Trained batch 929 batch loss 1.39592469 epoch total loss 1.58944297\n",
      "Trained batch 930 batch loss 1.40514326 epoch total loss 1.58924484\n",
      "Trained batch 931 batch loss 1.36238265 epoch total loss 1.58900118\n",
      "Trained batch 932 batch loss 1.4011066 epoch total loss 1.5887996\n",
      "Trained batch 933 batch loss 1.55355251 epoch total loss 1.58876181\n",
      "Trained batch 934 batch loss 1.51821303 epoch total loss 1.58868635\n",
      "Trained batch 935 batch loss 1.47119188 epoch total loss 1.5885607\n",
      "Trained batch 936 batch loss 1.34626877 epoch total loss 1.58830178\n",
      "Trained batch 937 batch loss 1.39209545 epoch total loss 1.58809245\n",
      "Trained batch 938 batch loss 1.51206779 epoch total loss 1.58801138\n",
      "Trained batch 939 batch loss 1.50960398 epoch total loss 1.58792794\n",
      "Trained batch 940 batch loss 1.32679796 epoch total loss 1.58765018\n",
      "Trained batch 941 batch loss 1.2743932 epoch total loss 1.58731723\n",
      "Trained batch 942 batch loss 1.28687322 epoch total loss 1.58699834\n",
      "Trained batch 943 batch loss 1.37105227 epoch total loss 1.58676934\n",
      "Trained batch 944 batch loss 1.37664378 epoch total loss 1.58654666\n",
      "Trained batch 945 batch loss 1.29802179 epoch total loss 1.58624136\n",
      "Trained batch 946 batch loss 1.42443025 epoch total loss 1.5860703\n",
      "Trained batch 947 batch loss 1.36717165 epoch total loss 1.58583915\n",
      "Trained batch 948 batch loss 1.42809021 epoch total loss 1.58567274\n",
      "Trained batch 949 batch loss 1.47879076 epoch total loss 1.58556008\n",
      "Trained batch 950 batch loss 1.3019855 epoch total loss 1.58526158\n",
      "Trained batch 951 batch loss 1.44470096 epoch total loss 1.58511376\n",
      "Trained batch 952 batch loss 1.51044047 epoch total loss 1.58503544\n",
      "Trained batch 953 batch loss 1.55294263 epoch total loss 1.58500183\n",
      "Trained batch 954 batch loss 1.5273037 epoch total loss 1.58494139\n",
      "Trained batch 955 batch loss 1.45169723 epoch total loss 1.58480179\n",
      "Trained batch 956 batch loss 1.42247093 epoch total loss 1.58463204\n",
      "Trained batch 957 batch loss 1.42509949 epoch total loss 1.58446527\n",
      "Trained batch 958 batch loss 1.47059429 epoch total loss 1.58434641\n",
      "Trained batch 959 batch loss 1.44716418 epoch total loss 1.58420336\n",
      "Trained batch 960 batch loss 1.40343904 epoch total loss 1.58401501\n",
      "Trained batch 961 batch loss 1.37218809 epoch total loss 1.58379459\n",
      "Trained batch 962 batch loss 1.42979681 epoch total loss 1.5836345\n",
      "Trained batch 963 batch loss 1.4335252 epoch total loss 1.58347857\n",
      "Trained batch 964 batch loss 1.52861762 epoch total loss 1.58342159\n",
      "Trained batch 965 batch loss 1.47543621 epoch total loss 1.58330977\n",
      "Trained batch 966 batch loss 1.36405945 epoch total loss 1.5830828\n",
      "Trained batch 967 batch loss 1.38548958 epoch total loss 1.58287847\n",
      "Trained batch 968 batch loss 1.44408941 epoch total loss 1.58273506\n",
      "Trained batch 969 batch loss 1.47501874 epoch total loss 1.58262384\n",
      "Trained batch 970 batch loss 1.48635101 epoch total loss 1.58252454\n",
      "Trained batch 971 batch loss 1.50328 epoch total loss 1.582443\n",
      "Trained batch 972 batch loss 1.4617027 epoch total loss 1.58231866\n",
      "Trained batch 973 batch loss 1.37464988 epoch total loss 1.58210528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 974 batch loss 1.36252606 epoch total loss 1.58187985\n",
      "Trained batch 975 batch loss 1.41486263 epoch total loss 1.58170855\n",
      "Trained batch 976 batch loss 1.42530763 epoch total loss 1.58154833\n",
      "Trained batch 977 batch loss 1.42922401 epoch total loss 1.58139241\n",
      "Trained batch 978 batch loss 1.5131824 epoch total loss 1.58132267\n",
      "Trained batch 979 batch loss 1.47549939 epoch total loss 1.58121455\n",
      "Trained batch 980 batch loss 1.48163021 epoch total loss 1.58111298\n",
      "Trained batch 981 batch loss 1.48208165 epoch total loss 1.58101201\n",
      "Trained batch 982 batch loss 1.51970983 epoch total loss 1.58094954\n",
      "Trained batch 983 batch loss 1.49326694 epoch total loss 1.58086038\n",
      "Trained batch 984 batch loss 1.51456261 epoch total loss 1.5807929\n",
      "Trained batch 985 batch loss 1.45988512 epoch total loss 1.58067012\n",
      "Trained batch 986 batch loss 1.59862924 epoch total loss 1.58068836\n",
      "Trained batch 987 batch loss 1.53652835 epoch total loss 1.58064353\n",
      "Trained batch 988 batch loss 1.44267786 epoch total loss 1.58050394\n",
      "Trained batch 989 batch loss 1.55369616 epoch total loss 1.58047676\n",
      "Trained batch 990 batch loss 1.49664617 epoch total loss 1.58039212\n",
      "Trained batch 991 batch loss 1.56615758 epoch total loss 1.58037782\n",
      "Trained batch 992 batch loss 1.58661807 epoch total loss 1.58038414\n",
      "Trained batch 993 batch loss 1.49658823 epoch total loss 1.58029974\n",
      "Trained batch 994 batch loss 1.54608893 epoch total loss 1.5802654\n",
      "Trained batch 995 batch loss 1.46935296 epoch total loss 1.58015394\n",
      "Trained batch 996 batch loss 1.47052813 epoch total loss 1.58004391\n",
      "Trained batch 997 batch loss 1.56277823 epoch total loss 1.58002663\n",
      "Trained batch 998 batch loss 1.46350896 epoch total loss 1.5799098\n",
      "Trained batch 999 batch loss 1.54782689 epoch total loss 1.57987773\n",
      "Trained batch 1000 batch loss 1.58565021 epoch total loss 1.57988358\n",
      "Trained batch 1001 batch loss 1.58829606 epoch total loss 1.57989192\n",
      "Trained batch 1002 batch loss 1.47322035 epoch total loss 1.57978547\n",
      "Trained batch 1003 batch loss 1.45835102 epoch total loss 1.57966447\n",
      "Trained batch 1004 batch loss 1.45824814 epoch total loss 1.57954347\n",
      "Trained batch 1005 batch loss 1.37955725 epoch total loss 1.57934451\n",
      "Trained batch 1006 batch loss 1.50906837 epoch total loss 1.57927465\n",
      "Trained batch 1007 batch loss 1.53208363 epoch total loss 1.57922781\n",
      "Trained batch 1008 batch loss 1.44775534 epoch total loss 1.57909727\n",
      "Trained batch 1009 batch loss 1.43959141 epoch total loss 1.57895899\n",
      "Trained batch 1010 batch loss 1.41842556 epoch total loss 1.57880008\n",
      "Trained batch 1011 batch loss 1.42751729 epoch total loss 1.57865047\n",
      "Trained batch 1012 batch loss 1.39915323 epoch total loss 1.57847309\n",
      "Trained batch 1013 batch loss 1.43847871 epoch total loss 1.57833493\n",
      "Trained batch 1014 batch loss 1.44349527 epoch total loss 1.57820189\n",
      "Trained batch 1015 batch loss 1.43894649 epoch total loss 1.5780648\n",
      "Trained batch 1016 batch loss 1.37412357 epoch total loss 1.57786405\n",
      "Trained batch 1017 batch loss 1.49405766 epoch total loss 1.57778156\n",
      "Trained batch 1018 batch loss 1.4072566 epoch total loss 1.57761407\n",
      "Trained batch 1019 batch loss 1.52601552 epoch total loss 1.57756341\n",
      "Trained batch 1020 batch loss 1.2953341 epoch total loss 1.57728672\n",
      "Trained batch 1021 batch loss 1.59810686 epoch total loss 1.57730711\n",
      "Trained batch 1022 batch loss 1.55544972 epoch total loss 1.57728565\n",
      "Trained batch 1023 batch loss 1.56553936 epoch total loss 1.5772742\n",
      "Trained batch 1024 batch loss 1.58048177 epoch total loss 1.5772773\n",
      "Trained batch 1025 batch loss 1.64654219 epoch total loss 1.57734478\n",
      "Trained batch 1026 batch loss 1.5547626 epoch total loss 1.57732284\n",
      "Trained batch 1027 batch loss 1.45961022 epoch total loss 1.57720828\n",
      "Trained batch 1028 batch loss 1.42186522 epoch total loss 1.57705712\n",
      "Trained batch 1029 batch loss 1.49909627 epoch total loss 1.57698143\n",
      "Trained batch 1030 batch loss 1.46916032 epoch total loss 1.57687664\n",
      "Trained batch 1031 batch loss 1.43780541 epoch total loss 1.57674181\n",
      "Trained batch 1032 batch loss 1.55646217 epoch total loss 1.57672226\n",
      "Trained batch 1033 batch loss 1.48970163 epoch total loss 1.5766381\n",
      "Trained batch 1034 batch loss 1.52824855 epoch total loss 1.57659125\n",
      "Trained batch 1035 batch loss 1.53783298 epoch total loss 1.57655382\n",
      "Trained batch 1036 batch loss 1.56680059 epoch total loss 1.57654428\n",
      "Trained batch 1037 batch loss 1.60012937 epoch total loss 1.57656705\n",
      "Trained batch 1038 batch loss 1.494205 epoch total loss 1.57648778\n",
      "Trained batch 1039 batch loss 1.61179662 epoch total loss 1.57652175\n",
      "Trained batch 1040 batch loss 1.58697271 epoch total loss 1.57653177\n",
      "Trained batch 1041 batch loss 1.52162504 epoch total loss 1.57647896\n",
      "Trained batch 1042 batch loss 1.51132357 epoch total loss 1.57641649\n",
      "Trained batch 1043 batch loss 1.48767567 epoch total loss 1.57633138\n",
      "Trained batch 1044 batch loss 1.56851137 epoch total loss 1.57632387\n",
      "Trained batch 1045 batch loss 1.42540431 epoch total loss 1.5761795\n",
      "Trained batch 1046 batch loss 1.44713 epoch total loss 1.57605612\n",
      "Trained batch 1047 batch loss 1.47315991 epoch total loss 1.57595778\n",
      "Trained batch 1048 batch loss 1.44295 epoch total loss 1.57583094\n",
      "Trained batch 1049 batch loss 1.45672631 epoch total loss 1.57571745\n",
      "Trained batch 1050 batch loss 1.43945241 epoch total loss 1.57558763\n",
      "Trained batch 1051 batch loss 1.40723133 epoch total loss 1.57542753\n",
      "Trained batch 1052 batch loss 1.4477638 epoch total loss 1.57530618\n",
      "Trained batch 1053 batch loss 1.55997121 epoch total loss 1.57529151\n",
      "Trained batch 1054 batch loss 1.54474521 epoch total loss 1.57526255\n",
      "Trained batch 1055 batch loss 1.46325541 epoch total loss 1.57515645\n",
      "Trained batch 1056 batch loss 1.50430954 epoch total loss 1.57508934\n",
      "Trained batch 1057 batch loss 1.27335203 epoch total loss 1.57480383\n",
      "Trained batch 1058 batch loss 1.437289 epoch total loss 1.57467377\n",
      "Trained batch 1059 batch loss 1.44336116 epoch total loss 1.57454979\n",
      "Trained batch 1060 batch loss 1.40282941 epoch total loss 1.57438779\n",
      "Trained batch 1061 batch loss 1.50967431 epoch total loss 1.57432675\n",
      "Trained batch 1062 batch loss 1.4387424 epoch total loss 1.57419908\n",
      "Trained batch 1063 batch loss 1.59418976 epoch total loss 1.57421792\n",
      "Trained batch 1064 batch loss 1.51525497 epoch total loss 1.57416248\n",
      "Trained batch 1065 batch loss 1.62032104 epoch total loss 1.57420588\n",
      "Trained batch 1066 batch loss 1.50773621 epoch total loss 1.57414353\n",
      "Trained batch 1067 batch loss 1.49268866 epoch total loss 1.57406712\n",
      "Trained batch 1068 batch loss 1.5661664 epoch total loss 1.57405972\n",
      "Trained batch 1069 batch loss 1.5700345 epoch total loss 1.57405603\n",
      "Trained batch 1070 batch loss 1.45703673 epoch total loss 1.57394671\n",
      "Trained batch 1071 batch loss 1.40739095 epoch total loss 1.57379115\n",
      "Trained batch 1072 batch loss 1.52855539 epoch total loss 1.57374895\n",
      "Trained batch 1073 batch loss 1.61711192 epoch total loss 1.57378924\n",
      "Trained batch 1074 batch loss 1.62310874 epoch total loss 1.57383525\n",
      "Trained batch 1075 batch loss 1.53502202 epoch total loss 1.57379913\n",
      "Trained batch 1076 batch loss 1.45053887 epoch total loss 1.57368469\n",
      "Trained batch 1077 batch loss 1.53581023 epoch total loss 1.57364941\n",
      "Trained batch 1078 batch loss 1.42187428 epoch total loss 1.57350862\n",
      "Trained batch 1079 batch loss 1.60771513 epoch total loss 1.57354033\n",
      "Trained batch 1080 batch loss 1.56030953 epoch total loss 1.57352805\n",
      "Trained batch 1081 batch loss 1.56935239 epoch total loss 1.57352412\n",
      "Trained batch 1082 batch loss 1.48930073 epoch total loss 1.57344627\n",
      "Trained batch 1083 batch loss 1.35842061 epoch total loss 1.57324767\n",
      "Trained batch 1084 batch loss 1.25432742 epoch total loss 1.57295346\n",
      "Trained batch 1085 batch loss 1.34633303 epoch total loss 1.57274461\n",
      "Trained batch 1086 batch loss 1.52978063 epoch total loss 1.57270503\n",
      "Trained batch 1087 batch loss 1.51971292 epoch total loss 1.57265615\n",
      "Trained batch 1088 batch loss 1.49223781 epoch total loss 1.57258224\n",
      "Trained batch 1089 batch loss 1.48773694 epoch total loss 1.5725044\n",
      "Trained batch 1090 batch loss 1.5237602 epoch total loss 1.5724597\n",
      "Trained batch 1091 batch loss 1.49248898 epoch total loss 1.57238638\n",
      "Trained batch 1092 batch loss 1.42015886 epoch total loss 1.57224691\n",
      "Trained batch 1093 batch loss 1.46876812 epoch total loss 1.57215226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1094 batch loss 1.56342983 epoch total loss 1.57214439\n",
      "Trained batch 1095 batch loss 1.3470962 epoch total loss 1.57193875\n",
      "Trained batch 1096 batch loss 1.46678329 epoch total loss 1.57184279\n",
      "Trained batch 1097 batch loss 1.5164609 epoch total loss 1.57179236\n",
      "Trained batch 1098 batch loss 1.44944143 epoch total loss 1.5716809\n",
      "Trained batch 1099 batch loss 1.45349896 epoch total loss 1.57157338\n",
      "Trained batch 1100 batch loss 1.45196462 epoch total loss 1.57146466\n",
      "Trained batch 1101 batch loss 1.47120881 epoch total loss 1.57137358\n",
      "Trained batch 1102 batch loss 1.52696133 epoch total loss 1.57133329\n",
      "Trained batch 1103 batch loss 1.47206283 epoch total loss 1.57124329\n",
      "Trained batch 1104 batch loss 1.51707256 epoch total loss 1.57119417\n",
      "Trained batch 1105 batch loss 1.57821584 epoch total loss 1.57120061\n",
      "Trained batch 1106 batch loss 1.36073756 epoch total loss 1.57101023\n",
      "Trained batch 1107 batch loss 1.34781361 epoch total loss 1.57080865\n",
      "Trained batch 1108 batch loss 1.50009239 epoch total loss 1.57074487\n",
      "Trained batch 1109 batch loss 1.54840231 epoch total loss 1.57072473\n",
      "Trained batch 1110 batch loss 1.57265377 epoch total loss 1.57072639\n",
      "Trained batch 1111 batch loss 1.48196721 epoch total loss 1.57064652\n",
      "Trained batch 1112 batch loss 1.45760477 epoch total loss 1.57054484\n",
      "Trained batch 1113 batch loss 1.45351827 epoch total loss 1.5704397\n",
      "Trained batch 1114 batch loss 1.43452954 epoch total loss 1.57031775\n",
      "Trained batch 1115 batch loss 1.51313841 epoch total loss 1.57026649\n",
      "Trained batch 1116 batch loss 1.42384887 epoch total loss 1.57013535\n",
      "Trained batch 1117 batch loss 1.4487747 epoch total loss 1.57002664\n",
      "Trained batch 1118 batch loss 1.48104525 epoch total loss 1.569947\n",
      "Trained batch 1119 batch loss 1.4436121 epoch total loss 1.56983411\n",
      "Trained batch 1120 batch loss 1.3745544 epoch total loss 1.56965971\n",
      "Trained batch 1121 batch loss 1.29558277 epoch total loss 1.56941521\n",
      "Trained batch 1122 batch loss 1.34056938 epoch total loss 1.56921124\n",
      "Trained batch 1123 batch loss 1.27756953 epoch total loss 1.56895161\n",
      "Trained batch 1124 batch loss 1.52761388 epoch total loss 1.56891477\n",
      "Trained batch 1125 batch loss 1.49333751 epoch total loss 1.56884754\n",
      "Trained batch 1126 batch loss 1.55864859 epoch total loss 1.56883848\n",
      "Trained batch 1127 batch loss 1.61208463 epoch total loss 1.56887674\n",
      "Trained batch 1128 batch loss 1.40171242 epoch total loss 1.56872857\n",
      "Trained batch 1129 batch loss 1.37811565 epoch total loss 1.56855989\n",
      "Trained batch 1130 batch loss 1.36777782 epoch total loss 1.56838214\n",
      "Trained batch 1131 batch loss 1.47777247 epoch total loss 1.56830204\n",
      "Trained batch 1132 batch loss 1.42323208 epoch total loss 1.56817389\n",
      "Trained batch 1133 batch loss 1.52661026 epoch total loss 1.56813717\n",
      "Trained batch 1134 batch loss 1.53717291 epoch total loss 1.56811\n",
      "Trained batch 1135 batch loss 1.49156 epoch total loss 1.56804252\n",
      "Trained batch 1136 batch loss 1.54639 epoch total loss 1.56802344\n",
      "Trained batch 1137 batch loss 1.5781157 epoch total loss 1.56803238\n",
      "Trained batch 1138 batch loss 1.57268524 epoch total loss 1.56803644\n",
      "Trained batch 1139 batch loss 1.42919827 epoch total loss 1.56791449\n",
      "Trained batch 1140 batch loss 1.4505949 epoch total loss 1.56781161\n",
      "Trained batch 1141 batch loss 1.49706078 epoch total loss 1.5677495\n",
      "Trained batch 1142 batch loss 1.55618262 epoch total loss 1.56773937\n",
      "Trained batch 1143 batch loss 1.45731342 epoch total loss 1.56764281\n",
      "Trained batch 1144 batch loss 1.41321516 epoch total loss 1.56750774\n",
      "Trained batch 1145 batch loss 1.42815518 epoch total loss 1.56738603\n",
      "Trained batch 1146 batch loss 1.42025018 epoch total loss 1.56725764\n",
      "Trained batch 1147 batch loss 1.3895669 epoch total loss 1.56710267\n",
      "Trained batch 1148 batch loss 1.46698749 epoch total loss 1.56701553\n",
      "Trained batch 1149 batch loss 1.47684157 epoch total loss 1.56693697\n",
      "Trained batch 1150 batch loss 1.49463487 epoch total loss 1.56687415\n",
      "Trained batch 1151 batch loss 1.38264465 epoch total loss 1.56671417\n",
      "Trained batch 1152 batch loss 1.51006544 epoch total loss 1.56666493\n",
      "Trained batch 1153 batch loss 1.48498559 epoch total loss 1.56659412\n",
      "Trained batch 1154 batch loss 1.33551848 epoch total loss 1.56639385\n",
      "Trained batch 1155 batch loss 1.47337461 epoch total loss 1.56631339\n",
      "Trained batch 1156 batch loss 1.42383432 epoch total loss 1.56619012\n",
      "Trained batch 1157 batch loss 1.5184586 epoch total loss 1.56614876\n",
      "Trained batch 1158 batch loss 1.46260238 epoch total loss 1.56605947\n",
      "Trained batch 1159 batch loss 1.31221867 epoch total loss 1.56584048\n",
      "Trained batch 1160 batch loss 1.23743558 epoch total loss 1.56555736\n",
      "Trained batch 1161 batch loss 1.35476446 epoch total loss 1.5653758\n",
      "Trained batch 1162 batch loss 1.39106536 epoch total loss 1.56522584\n",
      "Trained batch 1163 batch loss 1.46612656 epoch total loss 1.5651406\n",
      "Trained batch 1164 batch loss 1.38632011 epoch total loss 1.56498706\n",
      "Trained batch 1165 batch loss 1.42137074 epoch total loss 1.5648638\n",
      "Trained batch 1166 batch loss 1.45373154 epoch total loss 1.56476843\n",
      "Trained batch 1167 batch loss 1.59170699 epoch total loss 1.56479156\n",
      "Trained batch 1168 batch loss 1.40287304 epoch total loss 1.5646528\n",
      "Trained batch 1169 batch loss 1.57508159 epoch total loss 1.56466174\n",
      "Trained batch 1170 batch loss 1.63428152 epoch total loss 1.56472123\n",
      "Trained batch 1171 batch loss 1.67763901 epoch total loss 1.56481767\n",
      "Trained batch 1172 batch loss 1.46567237 epoch total loss 1.56473315\n",
      "Trained batch 1173 batch loss 1.4428376 epoch total loss 1.5646292\n",
      "Trained batch 1174 batch loss 1.28777325 epoch total loss 1.56439328\n",
      "Trained batch 1175 batch loss 1.23704517 epoch total loss 1.56411481\n",
      "Trained batch 1176 batch loss 1.34651673 epoch total loss 1.5639298\n",
      "Trained batch 1177 batch loss 1.42163706 epoch total loss 1.56380892\n",
      "Trained batch 1178 batch loss 1.37666214 epoch total loss 1.56365\n",
      "Trained batch 1179 batch loss 1.42586207 epoch total loss 1.56353319\n",
      "Trained batch 1180 batch loss 1.33426976 epoch total loss 1.56333888\n",
      "Trained batch 1181 batch loss 1.46661496 epoch total loss 1.56325698\n",
      "Trained batch 1182 batch loss 1.36665297 epoch total loss 1.56309068\n",
      "Trained batch 1183 batch loss 1.45441818 epoch total loss 1.56299889\n",
      "Trained batch 1184 batch loss 1.46103787 epoch total loss 1.56291282\n",
      "Trained batch 1185 batch loss 1.49710417 epoch total loss 1.56285727\n",
      "Trained batch 1186 batch loss 1.25074053 epoch total loss 1.56259406\n",
      "Trained batch 1187 batch loss 1.19464695 epoch total loss 1.56228411\n",
      "Trained batch 1188 batch loss 1.09072149 epoch total loss 1.56188715\n",
      "Trained batch 1189 batch loss 1.52849901 epoch total loss 1.56185901\n",
      "Trained batch 1190 batch loss 1.55384159 epoch total loss 1.56185234\n",
      "Trained batch 1191 batch loss 1.66044331 epoch total loss 1.56193507\n",
      "Trained batch 1192 batch loss 1.5444895 epoch total loss 1.5619204\n",
      "Trained batch 1193 batch loss 1.55251741 epoch total loss 1.56191242\n",
      "Trained batch 1194 batch loss 1.3613441 epoch total loss 1.56174445\n",
      "Trained batch 1195 batch loss 1.48393202 epoch total loss 1.56167936\n",
      "Trained batch 1196 batch loss 1.49885273 epoch total loss 1.56162679\n",
      "Trained batch 1197 batch loss 1.46813476 epoch total loss 1.56154871\n",
      "Trained batch 1198 batch loss 1.59717858 epoch total loss 1.56157851\n",
      "Trained batch 1199 batch loss 1.58207297 epoch total loss 1.56159556\n",
      "Trained batch 1200 batch loss 1.45937228 epoch total loss 1.56151032\n",
      "Trained batch 1201 batch loss 1.37338114 epoch total loss 1.56135368\n",
      "Trained batch 1202 batch loss 1.35553575 epoch total loss 1.5611825\n",
      "Trained batch 1203 batch loss 1.46208715 epoch total loss 1.56110013\n",
      "Trained batch 1204 batch loss 1.46404588 epoch total loss 1.56101942\n",
      "Trained batch 1205 batch loss 1.55074275 epoch total loss 1.56101096\n",
      "Trained batch 1206 batch loss 1.51115251 epoch total loss 1.56096959\n",
      "Trained batch 1207 batch loss 1.58771789 epoch total loss 1.56099176\n",
      "Trained batch 1208 batch loss 1.61430454 epoch total loss 1.56103587\n",
      "Trained batch 1209 batch loss 1.5960784 epoch total loss 1.56106484\n",
      "Trained batch 1210 batch loss 1.58490026 epoch total loss 1.56108463\n",
      "Trained batch 1211 batch loss 1.54399979 epoch total loss 1.56107044\n",
      "Trained batch 1212 batch loss 1.47745 epoch total loss 1.56100142\n",
      "Trained batch 1213 batch loss 1.5374825 epoch total loss 1.56098199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1214 batch loss 1.50729525 epoch total loss 1.56093776\n",
      "Trained batch 1215 batch loss 1.3979224 epoch total loss 1.56080365\n",
      "Trained batch 1216 batch loss 1.43427944 epoch total loss 1.5606997\n",
      "Trained batch 1217 batch loss 1.43824 epoch total loss 1.56059909\n",
      "Trained batch 1218 batch loss 1.49316263 epoch total loss 1.56054366\n",
      "Trained batch 1219 batch loss 1.40089476 epoch total loss 1.56041265\n",
      "Trained batch 1220 batch loss 1.48698 epoch total loss 1.56035244\n",
      "Trained batch 1221 batch loss 1.52686167 epoch total loss 1.56032503\n",
      "Trained batch 1222 batch loss 1.51962888 epoch total loss 1.56029177\n",
      "Trained batch 1223 batch loss 1.49154556 epoch total loss 1.56023562\n",
      "Trained batch 1224 batch loss 1.44502497 epoch total loss 1.56014144\n",
      "Trained batch 1225 batch loss 1.46100485 epoch total loss 1.56006062\n",
      "Trained batch 1226 batch loss 1.39468908 epoch total loss 1.55992568\n",
      "Trained batch 1227 batch loss 1.28820109 epoch total loss 1.55970418\n",
      "Trained batch 1228 batch loss 1.35386896 epoch total loss 1.55953658\n",
      "Trained batch 1229 batch loss 1.33532667 epoch total loss 1.55935419\n",
      "Trained batch 1230 batch loss 1.52884841 epoch total loss 1.55932939\n",
      "Trained batch 1231 batch loss 1.4255085 epoch total loss 1.55922067\n",
      "Trained batch 1232 batch loss 1.39462805 epoch total loss 1.55908704\n",
      "Trained batch 1233 batch loss 1.38441634 epoch total loss 1.55894542\n",
      "Trained batch 1234 batch loss 1.49371362 epoch total loss 1.55889261\n",
      "Trained batch 1235 batch loss 1.27689302 epoch total loss 1.5586642\n",
      "Trained batch 1236 batch loss 1.3844434 epoch total loss 1.55852318\n",
      "Trained batch 1237 batch loss 1.51204479 epoch total loss 1.55848575\n",
      "Trained batch 1238 batch loss 1.50046957 epoch total loss 1.5584389\n",
      "Trained batch 1239 batch loss 1.46422446 epoch total loss 1.55836284\n",
      "Trained batch 1240 batch loss 1.39896679 epoch total loss 1.55823421\n",
      "Trained batch 1241 batch loss 1.42458749 epoch total loss 1.55812657\n",
      "Trained batch 1242 batch loss 1.30727196 epoch total loss 1.55792451\n",
      "Trained batch 1243 batch loss 1.34570026 epoch total loss 1.5577538\n",
      "Trained batch 1244 batch loss 1.40553451 epoch total loss 1.55763137\n",
      "Trained batch 1245 batch loss 1.37203526 epoch total loss 1.55748236\n",
      "Trained batch 1246 batch loss 1.36687839 epoch total loss 1.5573293\n",
      "Trained batch 1247 batch loss 1.50256109 epoch total loss 1.55728543\n",
      "Trained batch 1248 batch loss 1.36689472 epoch total loss 1.55713296\n",
      "Trained batch 1249 batch loss 1.49624825 epoch total loss 1.5570842\n",
      "Trained batch 1250 batch loss 1.43443131 epoch total loss 1.55698609\n",
      "Trained batch 1251 batch loss 1.39681602 epoch total loss 1.55685806\n",
      "Trained batch 1252 batch loss 1.40157819 epoch total loss 1.55673409\n",
      "Trained batch 1253 batch loss 1.42075348 epoch total loss 1.55662549\n",
      "Trained batch 1254 batch loss 1.38425267 epoch total loss 1.55648804\n",
      "Trained batch 1255 batch loss 1.46158957 epoch total loss 1.55641246\n",
      "Trained batch 1256 batch loss 1.47727895 epoch total loss 1.5563494\n",
      "Trained batch 1257 batch loss 1.57919288 epoch total loss 1.55636764\n",
      "Trained batch 1258 batch loss 1.56624842 epoch total loss 1.5563755\n",
      "Trained batch 1259 batch loss 1.52752972 epoch total loss 1.55635262\n",
      "Trained batch 1260 batch loss 1.49309051 epoch total loss 1.55630243\n",
      "Trained batch 1261 batch loss 1.49656546 epoch total loss 1.5562551\n",
      "Trained batch 1262 batch loss 1.4880147 epoch total loss 1.55620098\n",
      "Trained batch 1263 batch loss 1.5134269 epoch total loss 1.55616713\n",
      "Trained batch 1264 batch loss 1.48428345 epoch total loss 1.55611026\n",
      "Trained batch 1265 batch loss 1.49665892 epoch total loss 1.55606329\n",
      "Trained batch 1266 batch loss 1.51147187 epoch total loss 1.55602801\n",
      "Trained batch 1267 batch loss 1.46502006 epoch total loss 1.55595613\n",
      "Trained batch 1268 batch loss 1.45322216 epoch total loss 1.55587518\n",
      "Trained batch 1269 batch loss 1.44793892 epoch total loss 1.55579019\n",
      "Trained batch 1270 batch loss 1.42503095 epoch total loss 1.55568719\n",
      "Trained batch 1271 batch loss 1.41149354 epoch total loss 1.55557382\n",
      "Trained batch 1272 batch loss 1.45065904 epoch total loss 1.55549133\n",
      "Trained batch 1273 batch loss 1.44438648 epoch total loss 1.55540395\n",
      "Trained batch 1274 batch loss 1.3558706 epoch total loss 1.55524731\n",
      "Trained batch 1275 batch loss 1.46831715 epoch total loss 1.55517912\n",
      "Trained batch 1276 batch loss 1.50113273 epoch total loss 1.5551368\n",
      "Trained batch 1277 batch loss 1.48045206 epoch total loss 1.55507827\n",
      "Trained batch 1278 batch loss 1.39114928 epoch total loss 1.55495\n",
      "Trained batch 1279 batch loss 1.53098893 epoch total loss 1.55493128\n",
      "Trained batch 1280 batch loss 1.76741934 epoch total loss 1.55509734\n",
      "Trained batch 1281 batch loss 1.54594564 epoch total loss 1.55509007\n",
      "Trained batch 1282 batch loss 1.58426452 epoch total loss 1.55511284\n",
      "Trained batch 1283 batch loss 1.61337042 epoch total loss 1.55515826\n",
      "Trained batch 1284 batch loss 1.48650169 epoch total loss 1.55510473\n",
      "Trained batch 1285 batch loss 1.58531952 epoch total loss 1.55512834\n",
      "Trained batch 1286 batch loss 1.41394305 epoch total loss 1.55501854\n",
      "Trained batch 1287 batch loss 1.36651981 epoch total loss 1.55487204\n",
      "Trained batch 1288 batch loss 1.3518064 epoch total loss 1.55471444\n",
      "Trained batch 1289 batch loss 1.42260933 epoch total loss 1.55461192\n",
      "Trained batch 1290 batch loss 1.50141096 epoch total loss 1.55457067\n",
      "Trained batch 1291 batch loss 1.50116706 epoch total loss 1.55452943\n",
      "Trained batch 1292 batch loss 1.56011426 epoch total loss 1.55453372\n",
      "Trained batch 1293 batch loss 1.50767088 epoch total loss 1.55449748\n",
      "Trained batch 1294 batch loss 1.51441503 epoch total loss 1.55446649\n",
      "Trained batch 1295 batch loss 1.48698282 epoch total loss 1.55441427\n",
      "Trained batch 1296 batch loss 1.49372041 epoch total loss 1.55436754\n",
      "Trained batch 1297 batch loss 1.47394252 epoch total loss 1.55430555\n",
      "Trained batch 1298 batch loss 1.36391366 epoch total loss 1.55415893\n",
      "Trained batch 1299 batch loss 1.41887152 epoch total loss 1.55405474\n",
      "Trained batch 1300 batch loss 1.37872934 epoch total loss 1.55391991\n",
      "Trained batch 1301 batch loss 1.46602356 epoch total loss 1.55385232\n",
      "Trained batch 1302 batch loss 1.34474719 epoch total loss 1.55369174\n",
      "Trained batch 1303 batch loss 1.37034035 epoch total loss 1.55355108\n",
      "Trained batch 1304 batch loss 1.37621653 epoch total loss 1.55341506\n",
      "Trained batch 1305 batch loss 1.39157367 epoch total loss 1.55329108\n",
      "Trained batch 1306 batch loss 1.43635356 epoch total loss 1.55320156\n",
      "Trained batch 1307 batch loss 1.45780706 epoch total loss 1.55312848\n",
      "Trained batch 1308 batch loss 1.47534883 epoch total loss 1.553069\n",
      "Trained batch 1309 batch loss 1.50434196 epoch total loss 1.5530318\n",
      "Trained batch 1310 batch loss 1.48146677 epoch total loss 1.5529772\n",
      "Trained batch 1311 batch loss 1.41007555 epoch total loss 1.55286813\n",
      "Trained batch 1312 batch loss 1.5671308 epoch total loss 1.5528791\n",
      "Trained batch 1313 batch loss 1.42192912 epoch total loss 1.55277932\n",
      "Trained batch 1314 batch loss 1.32737446 epoch total loss 1.55260777\n",
      "Trained batch 1315 batch loss 1.32879245 epoch total loss 1.55243754\n",
      "Trained batch 1316 batch loss 1.41960347 epoch total loss 1.55233657\n",
      "Trained batch 1317 batch loss 1.43603075 epoch total loss 1.55224824\n",
      "Trained batch 1318 batch loss 1.51169848 epoch total loss 1.55221748\n",
      "Trained batch 1319 batch loss 1.43993795 epoch total loss 1.55213237\n",
      "Trained batch 1320 batch loss 1.45098794 epoch total loss 1.55205572\n",
      "Trained batch 1321 batch loss 1.27986348 epoch total loss 1.55184972\n",
      "Trained batch 1322 batch loss 1.4024961 epoch total loss 1.55173671\n",
      "Trained batch 1323 batch loss 1.50190461 epoch total loss 1.55169916\n",
      "Trained batch 1324 batch loss 1.47932148 epoch total loss 1.55164444\n",
      "Trained batch 1325 batch loss 1.39090025 epoch total loss 1.55152309\n",
      "Trained batch 1326 batch loss 1.44100678 epoch total loss 1.55143964\n",
      "Trained batch 1327 batch loss 1.43574345 epoch total loss 1.5513525\n",
      "Trained batch 1328 batch loss 1.46540892 epoch total loss 1.55128777\n",
      "Trained batch 1329 batch loss 1.41618311 epoch total loss 1.55118608\n",
      "Trained batch 1330 batch loss 1.36663234 epoch total loss 1.55104744\n",
      "Trained batch 1331 batch loss 1.41208243 epoch total loss 1.55094302\n",
      "Trained batch 1332 batch loss 1.54076028 epoch total loss 1.55093539\n",
      "Trained batch 1333 batch loss 1.37725198 epoch total loss 1.55080509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1334 batch loss 1.45796943 epoch total loss 1.55073547\n",
      "Trained batch 1335 batch loss 1.56710839 epoch total loss 1.55074775\n",
      "Trained batch 1336 batch loss 1.44514751 epoch total loss 1.55066872\n",
      "Trained batch 1337 batch loss 1.5209 epoch total loss 1.55064654\n",
      "Trained batch 1338 batch loss 1.55644631 epoch total loss 1.55065084\n",
      "Trained batch 1339 batch loss 1.44881916 epoch total loss 1.55057466\n",
      "Trained batch 1340 batch loss 1.48947179 epoch total loss 1.55052912\n",
      "Trained batch 1341 batch loss 1.40114546 epoch total loss 1.55041766\n",
      "Trained batch 1342 batch loss 1.3744657 epoch total loss 1.55028665\n",
      "Trained batch 1343 batch loss 1.31285834 epoch total loss 1.55010974\n",
      "Trained batch 1344 batch loss 1.44471979 epoch total loss 1.55003142\n",
      "Trained batch 1345 batch loss 1.34051108 epoch total loss 1.54987562\n",
      "Trained batch 1346 batch loss 1.40372205 epoch total loss 1.54976714\n",
      "Trained batch 1347 batch loss 1.34839797 epoch total loss 1.54961765\n",
      "Trained batch 1348 batch loss 1.35994673 epoch total loss 1.54947686\n",
      "Trained batch 1349 batch loss 1.4483819 epoch total loss 1.549402\n",
      "Trained batch 1350 batch loss 1.50796938 epoch total loss 1.54937136\n",
      "Trained batch 1351 batch loss 1.46812069 epoch total loss 1.54931116\n",
      "Trained batch 1352 batch loss 1.44546628 epoch total loss 1.54923439\n",
      "Trained batch 1353 batch loss 1.30573416 epoch total loss 1.54905438\n",
      "Trained batch 1354 batch loss 1.28221154 epoch total loss 1.54885733\n",
      "Trained batch 1355 batch loss 1.35874915 epoch total loss 1.5487169\n",
      "Trained batch 1356 batch loss 1.36233354 epoch total loss 1.54857945\n",
      "Trained batch 1357 batch loss 1.43137252 epoch total loss 1.54849315\n",
      "Trained batch 1358 batch loss 1.54537642 epoch total loss 1.54849088\n",
      "Trained batch 1359 batch loss 1.38266397 epoch total loss 1.54836881\n",
      "Trained batch 1360 batch loss 1.42423308 epoch total loss 1.5482775\n",
      "Trained batch 1361 batch loss 1.416502 epoch total loss 1.5481807\n",
      "Trained batch 1362 batch loss 1.36882687 epoch total loss 1.54804909\n",
      "Trained batch 1363 batch loss 1.47448277 epoch total loss 1.54799509\n",
      "Trained batch 1364 batch loss 1.3748492 epoch total loss 1.54786801\n",
      "Trained batch 1365 batch loss 1.41301322 epoch total loss 1.54776931\n",
      "Trained batch 1366 batch loss 1.57909358 epoch total loss 1.5477922\n",
      "Trained batch 1367 batch loss 1.56781435 epoch total loss 1.54780686\n",
      "Trained batch 1368 batch loss 1.47324944 epoch total loss 1.54775238\n",
      "Trained batch 1369 batch loss 1.46672559 epoch total loss 1.54769325\n",
      "Trained batch 1370 batch loss 1.55424702 epoch total loss 1.5476979\n",
      "Trained batch 1371 batch loss 1.4228574 epoch total loss 1.54760695\n",
      "Trained batch 1372 batch loss 1.56046009 epoch total loss 1.54761636\n",
      "Trained batch 1373 batch loss 1.43511736 epoch total loss 1.54753435\n",
      "Trained batch 1374 batch loss 1.48644352 epoch total loss 1.54748976\n",
      "Trained batch 1375 batch loss 1.45277762 epoch total loss 1.54742098\n",
      "Trained batch 1376 batch loss 1.37638581 epoch total loss 1.54729676\n",
      "Trained batch 1377 batch loss 1.45553505 epoch total loss 1.54723012\n",
      "Trained batch 1378 batch loss 1.44217992 epoch total loss 1.54715383\n",
      "Trained batch 1379 batch loss 1.50166261 epoch total loss 1.54712093\n",
      "Trained batch 1380 batch loss 1.36041951 epoch total loss 1.54698563\n",
      "Trained batch 1381 batch loss 1.40224493 epoch total loss 1.54688084\n",
      "Trained batch 1382 batch loss 1.3441304 epoch total loss 1.54673421\n",
      "Trained batch 1383 batch loss 1.31243098 epoch total loss 1.54656482\n",
      "Trained batch 1384 batch loss 1.39646912 epoch total loss 1.54645646\n",
      "Trained batch 1385 batch loss 1.38090134 epoch total loss 1.54633689\n",
      "Trained batch 1386 batch loss 1.27414846 epoch total loss 1.54614043\n",
      "Trained batch 1387 batch loss 1.29313385 epoch total loss 1.54595816\n",
      "Trained batch 1388 batch loss 1.33167911 epoch total loss 1.54580379\n",
      "Epoch 1 train loss 1.5458037853240967\n",
      "Validated batch 1 batch loss 1.52807808\n",
      "Validated batch 2 batch loss 1.44276643\n",
      "Validated batch 3 batch loss 1.3865943\n",
      "Validated batch 4 batch loss 1.32701516\n",
      "Validated batch 5 batch loss 1.40090775\n",
      "Validated batch 6 batch loss 1.4433037\n",
      "Validated batch 7 batch loss 1.36708379\n",
      "Validated batch 8 batch loss 1.37019634\n",
      "Validated batch 9 batch loss 1.45885766\n",
      "Validated batch 10 batch loss 1.46149898\n",
      "Validated batch 11 batch loss 1.35623646\n",
      "Validated batch 12 batch loss 1.31644464\n",
      "Validated batch 13 batch loss 1.44829965\n",
      "Validated batch 14 batch loss 1.48693466\n",
      "Validated batch 15 batch loss 1.51181877\n",
      "Validated batch 16 batch loss 1.46506929\n",
      "Validated batch 17 batch loss 1.40120828\n",
      "Validated batch 18 batch loss 1.51767421\n",
      "Validated batch 19 batch loss 1.45846319\n",
      "Validated batch 20 batch loss 1.41535544\n",
      "Validated batch 21 batch loss 1.49008632\n",
      "Validated batch 22 batch loss 1.19975209\n",
      "Validated batch 23 batch loss 1.5143826\n",
      "Validated batch 24 batch loss 1.49758697\n",
      "Validated batch 25 batch loss 1.43904066\n",
      "Validated batch 26 batch loss 1.35520923\n",
      "Validated batch 27 batch loss 1.36364603\n",
      "Validated batch 28 batch loss 1.40889776\n",
      "Validated batch 29 batch loss 1.55236292\n",
      "Validated batch 30 batch loss 1.34304273\n",
      "Validated batch 31 batch loss 1.474473\n",
      "Validated batch 32 batch loss 1.4772923\n",
      "Validated batch 33 batch loss 1.39491832\n",
      "Validated batch 34 batch loss 1.41308856\n",
      "Validated batch 35 batch loss 1.3017801\n",
      "Validated batch 36 batch loss 1.56158197\n",
      "Validated batch 37 batch loss 1.35545182\n",
      "Validated batch 38 batch loss 1.46924043\n",
      "Validated batch 39 batch loss 1.41541362\n",
      "Validated batch 40 batch loss 1.47721386\n",
      "Validated batch 41 batch loss 1.2707\n",
      "Validated batch 42 batch loss 1.38407671\n",
      "Validated batch 43 batch loss 1.39586163\n",
      "Validated batch 44 batch loss 1.39364052\n",
      "Validated batch 45 batch loss 1.41617179\n",
      "Validated batch 46 batch loss 1.42227888\n",
      "Validated batch 47 batch loss 1.40715039\n",
      "Validated batch 48 batch loss 1.35864091\n",
      "Validated batch 49 batch loss 1.41746175\n",
      "Validated batch 50 batch loss 1.33524859\n",
      "Validated batch 51 batch loss 1.42147779\n",
      "Validated batch 52 batch loss 1.4677906\n",
      "Validated batch 53 batch loss 1.46411872\n",
      "Validated batch 54 batch loss 1.51625669\n",
      "Validated batch 55 batch loss 1.48145282\n",
      "Validated batch 56 batch loss 1.46099448\n",
      "Validated batch 57 batch loss 1.41935122\n",
      "Validated batch 58 batch loss 1.49965131\n",
      "Validated batch 59 batch loss 1.4563117\n",
      "Validated batch 60 batch loss 1.4996984\n",
      "Validated batch 61 batch loss 1.50657296\n",
      "Validated batch 62 batch loss 1.45435023\n",
      "Validated batch 63 batch loss 1.50195515\n",
      "Validated batch 64 batch loss 1.27022481\n",
      "Validated batch 65 batch loss 1.39070761\n",
      "Validated batch 66 batch loss 1.44232261\n",
      "Validated batch 67 batch loss 1.44946957\n",
      "Validated batch 68 batch loss 1.45963168\n",
      "Validated batch 69 batch loss 1.41040158\n",
      "Validated batch 70 batch loss 1.55078578\n",
      "Validated batch 71 batch loss 1.40809345\n",
      "Validated batch 72 batch loss 1.42032599\n",
      "Validated batch 73 batch loss 1.41110909\n",
      "Validated batch 74 batch loss 1.3910948\n",
      "Validated batch 75 batch loss 1.47487521\n",
      "Validated batch 76 batch loss 1.4259969\n",
      "Validated batch 77 batch loss 1.34572446\n",
      "Validated batch 78 batch loss 1.37472594\n",
      "Validated batch 79 batch loss 1.38379145\n",
      "Validated batch 80 batch loss 1.45366454\n",
      "Validated batch 81 batch loss 1.42351818\n",
      "Validated batch 82 batch loss 1.40413916\n",
      "Validated batch 83 batch loss 1.3520416\n",
      "Validated batch 84 batch loss 1.39423394\n",
      "Validated batch 85 batch loss 1.4676379\n",
      "Validated batch 86 batch loss 1.45234823\n",
      "Validated batch 87 batch loss 1.44244075\n",
      "Validated batch 88 batch loss 1.54547393\n",
      "Validated batch 89 batch loss 1.69823\n",
      "Validated batch 90 batch loss 1.5512495\n",
      "Validated batch 91 batch loss 1.40709186\n",
      "Validated batch 92 batch loss 1.31381571\n",
      "Validated batch 93 batch loss 1.33389187\n",
      "Validated batch 94 batch loss 1.46569395\n",
      "Validated batch 95 batch loss 1.34418571\n",
      "Validated batch 96 batch loss 1.43973064\n",
      "Validated batch 97 batch loss 1.45670056\n",
      "Validated batch 98 batch loss 1.47213435\n",
      "Validated batch 99 batch loss 1.48992968\n",
      "Validated batch 100 batch loss 1.50552964\n",
      "Validated batch 101 batch loss 1.49683118\n",
      "Validated batch 102 batch loss 1.455199\n",
      "Validated batch 103 batch loss 1.51122022\n",
      "Validated batch 104 batch loss 1.53965855\n",
      "Validated batch 105 batch loss 1.35892522\n",
      "Validated batch 106 batch loss 1.50886726\n",
      "Validated batch 107 batch loss 1.4678688\n",
      "Validated batch 108 batch loss 1.47549427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 109 batch loss 1.53246617\n",
      "Validated batch 110 batch loss 1.26541793\n",
      "Validated batch 111 batch loss 1.39883256\n",
      "Validated batch 112 batch loss 1.4027499\n",
      "Validated batch 113 batch loss 1.36251819\n",
      "Validated batch 114 batch loss 1.56949329\n",
      "Validated batch 115 batch loss 1.35939956\n",
      "Validated batch 116 batch loss 1.46281791\n",
      "Validated batch 117 batch loss 1.33313406\n",
      "Validated batch 118 batch loss 1.43440437\n",
      "Validated batch 119 batch loss 1.37173796\n",
      "Validated batch 120 batch loss 1.37071991\n",
      "Validated batch 121 batch loss 1.43054092\n",
      "Validated batch 122 batch loss 1.41060638\n",
      "Validated batch 123 batch loss 1.44437242\n",
      "Validated batch 124 batch loss 1.4459722\n",
      "Validated batch 125 batch loss 1.38281918\n",
      "Validated batch 126 batch loss 1.54521954\n",
      "Validated batch 127 batch loss 1.50124371\n",
      "Validated batch 128 batch loss 1.35385871\n",
      "Validated batch 129 batch loss 1.38961172\n",
      "Validated batch 130 batch loss 1.43649495\n",
      "Validated batch 131 batch loss 1.42348123\n",
      "Validated batch 132 batch loss 1.54055536\n",
      "Validated batch 133 batch loss 1.41705489\n",
      "Validated batch 134 batch loss 1.45130372\n",
      "Validated batch 135 batch loss 1.42375469\n",
      "Validated batch 136 batch loss 1.40692747\n",
      "Validated batch 137 batch loss 1.42455363\n",
      "Validated batch 138 batch loss 1.41849971\n",
      "Validated batch 139 batch loss 1.40832412\n",
      "Validated batch 140 batch loss 1.4658581\n",
      "Validated batch 141 batch loss 1.42212856\n",
      "Validated batch 142 batch loss 1.2590642\n",
      "Validated batch 143 batch loss 1.36347604\n",
      "Validated batch 144 batch loss 1.50675559\n",
      "Validated batch 145 batch loss 1.34805048\n",
      "Validated batch 146 batch loss 1.39080024\n",
      "Validated batch 147 batch loss 1.40586638\n",
      "Validated batch 148 batch loss 1.47310615\n",
      "Validated batch 149 batch loss 1.36920834\n",
      "Validated batch 150 batch loss 1.46838331\n",
      "Validated batch 151 batch loss 1.32934928\n",
      "Validated batch 152 batch loss 1.44676471\n",
      "Validated batch 153 batch loss 1.47403038\n",
      "Validated batch 154 batch loss 1.5326966\n",
      "Validated batch 155 batch loss 1.38391459\n",
      "Validated batch 156 batch loss 1.54445875\n",
      "Validated batch 157 batch loss 1.3654803\n",
      "Validated batch 158 batch loss 1.34131551\n",
      "Validated batch 159 batch loss 1.43039417\n",
      "Validated batch 160 batch loss 1.4317286\n",
      "Validated batch 161 batch loss 1.51232541\n",
      "Validated batch 162 batch loss 1.40311885\n",
      "Validated batch 163 batch loss 1.44808388\n",
      "Validated batch 164 batch loss 1.47383261\n",
      "Validated batch 165 batch loss 1.41897154\n",
      "Validated batch 166 batch loss 1.42337334\n",
      "Validated batch 167 batch loss 1.56372726\n",
      "Validated batch 168 batch loss 1.3196615\n",
      "Validated batch 169 batch loss 1.45844281\n",
      "Validated batch 170 batch loss 1.43655539\n",
      "Validated batch 171 batch loss 1.42761528\n",
      "Validated batch 172 batch loss 1.42670941\n",
      "Validated batch 173 batch loss 1.39632249\n",
      "Validated batch 174 batch loss 1.16570878\n",
      "Validated batch 175 batch loss 1.44143343\n",
      "Validated batch 176 batch loss 1.42218137\n",
      "Validated batch 177 batch loss 1.44189167\n",
      "Validated batch 178 batch loss 1.41872215\n",
      "Validated batch 179 batch loss 1.36981094\n",
      "Validated batch 180 batch loss 1.49241233\n",
      "Validated batch 181 batch loss 1.51721108\n",
      "Validated batch 182 batch loss 1.43458748\n",
      "Validated batch 183 batch loss 1.45658493\n",
      "Validated batch 184 batch loss 1.3293587\n",
      "Validated batch 185 batch loss 1.38724756\n",
      "Epoch 1 val loss 1.4287246465682983\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-1-loss-1.4287.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.52620769 epoch total loss 1.52620769\n",
      "Trained batch 2 batch loss 1.46538019 epoch total loss 1.49579394\n",
      "Trained batch 3 batch loss 1.49937797 epoch total loss 1.49698865\n",
      "Trained batch 4 batch loss 1.39207971 epoch total loss 1.47076142\n",
      "Trained batch 5 batch loss 1.41693592 epoch total loss 1.45999634\n",
      "Trained batch 6 batch loss 1.24709296 epoch total loss 1.42451239\n",
      "Trained batch 7 batch loss 1.47364 epoch total loss 1.43153059\n",
      "Trained batch 8 batch loss 1.39454675 epoch total loss 1.42690754\n",
      "Trained batch 9 batch loss 1.46998429 epoch total loss 1.43169379\n",
      "Trained batch 10 batch loss 1.33170271 epoch total loss 1.42169476\n",
      "Trained batch 11 batch loss 1.47000241 epoch total loss 1.42608631\n",
      "Trained batch 12 batch loss 1.43956852 epoch total loss 1.42720985\n",
      "Trained batch 13 batch loss 1.31208134 epoch total loss 1.4183538\n",
      "Trained batch 14 batch loss 1.42278981 epoch total loss 1.41867065\n",
      "Trained batch 15 batch loss 1.45987725 epoch total loss 1.42141771\n",
      "Trained batch 16 batch loss 1.48803139 epoch total loss 1.4255811\n",
      "Trained batch 17 batch loss 1.43508434 epoch total loss 1.42614007\n",
      "Trained batch 18 batch loss 1.45858026 epoch total loss 1.42794228\n",
      "Trained batch 19 batch loss 1.29542565 epoch total loss 1.4209677\n",
      "Trained batch 20 batch loss 1.34018111 epoch total loss 1.41692841\n",
      "Trained batch 21 batch loss 1.5242095 epoch total loss 1.42203701\n",
      "Trained batch 22 batch loss 1.39950824 epoch total loss 1.421013\n",
      "Trained batch 23 batch loss 1.37666559 epoch total loss 1.41908479\n",
      "Trained batch 24 batch loss 1.47876716 epoch total loss 1.42157161\n",
      "Trained batch 25 batch loss 1.46215618 epoch total loss 1.42319489\n",
      "Trained batch 26 batch loss 1.40915084 epoch total loss 1.42265463\n",
      "Trained batch 27 batch loss 1.34626794 epoch total loss 1.41982555\n",
      "Trained batch 28 batch loss 1.40078592 epoch total loss 1.41914558\n",
      "Trained batch 29 batch loss 1.3598597 epoch total loss 1.41710126\n",
      "Trained batch 30 batch loss 1.36705828 epoch total loss 1.41543317\n",
      "Trained batch 31 batch loss 1.4152236 epoch total loss 1.41542637\n",
      "Trained batch 32 batch loss 1.40672064 epoch total loss 1.41515422\n",
      "Trained batch 33 batch loss 1.41253281 epoch total loss 1.41507483\n",
      "Trained batch 34 batch loss 1.31874335 epoch total loss 1.41224158\n",
      "Trained batch 35 batch loss 1.34968519 epoch total loss 1.41045427\n",
      "Trained batch 36 batch loss 1.42359018 epoch total loss 1.41081917\n",
      "Trained batch 37 batch loss 1.38078535 epoch total loss 1.41000748\n",
      "Trained batch 38 batch loss 1.3161031 epoch total loss 1.40753639\n",
      "Trained batch 39 batch loss 1.33036304 epoch total loss 1.40555763\n",
      "Trained batch 40 batch loss 1.3889643 epoch total loss 1.40514278\n",
      "Trained batch 41 batch loss 1.34545016 epoch total loss 1.40368688\n",
      "Trained batch 42 batch loss 1.39936566 epoch total loss 1.403584\n",
      "Trained batch 43 batch loss 1.43927288 epoch total loss 1.40441394\n",
      "Trained batch 44 batch loss 1.41421342 epoch total loss 1.40463674\n",
      "Trained batch 45 batch loss 1.40114164 epoch total loss 1.40455902\n",
      "Trained batch 46 batch loss 1.46227932 epoch total loss 1.40581381\n",
      "Trained batch 47 batch loss 1.43125033 epoch total loss 1.40635502\n",
      "Trained batch 48 batch loss 1.35399663 epoch total loss 1.40526426\n",
      "Trained batch 49 batch loss 1.43843937 epoch total loss 1.40594125\n",
      "Trained batch 50 batch loss 1.50014293 epoch total loss 1.40782535\n",
      "Trained batch 51 batch loss 1.38797 epoch total loss 1.40743601\n",
      "Trained batch 52 batch loss 1.36866391 epoch total loss 1.40669048\n",
      "Trained batch 53 batch loss 1.40804064 epoch total loss 1.40671599\n",
      "Trained batch 54 batch loss 1.35930765 epoch total loss 1.40583801\n",
      "Trained batch 55 batch loss 1.44324338 epoch total loss 1.4065181\n",
      "Trained batch 56 batch loss 1.52536249 epoch total loss 1.40864027\n",
      "Trained batch 57 batch loss 1.4674747 epoch total loss 1.4096725\n",
      "Trained batch 58 batch loss 1.43502271 epoch total loss 1.41010952\n",
      "Trained batch 59 batch loss 1.33058882 epoch total loss 1.40876174\n",
      "Trained batch 60 batch loss 1.27274644 epoch total loss 1.40649474\n",
      "Trained batch 61 batch loss 1.44122171 epoch total loss 1.40706408\n",
      "Trained batch 62 batch loss 1.33980882 epoch total loss 1.40597928\n",
      "Trained batch 63 batch loss 1.33625937 epoch total loss 1.40487254\n",
      "Trained batch 64 batch loss 1.47528446 epoch total loss 1.40597272\n",
      "Trained batch 65 batch loss 1.46811664 epoch total loss 1.40692878\n",
      "Trained batch 66 batch loss 1.42706692 epoch total loss 1.40723395\n",
      "Trained batch 67 batch loss 1.44932055 epoch total loss 1.40786207\n",
      "Trained batch 68 batch loss 1.39305234 epoch total loss 1.40764427\n",
      "Trained batch 69 batch loss 1.49842966 epoch total loss 1.40896\n",
      "Trained batch 70 batch loss 1.55225229 epoch total loss 1.41100705\n",
      "Trained batch 71 batch loss 1.55616724 epoch total loss 1.41305161\n",
      "Trained batch 72 batch loss 1.66379631 epoch total loss 1.41653407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 73 batch loss 1.38320112 epoch total loss 1.41607749\n",
      "Trained batch 74 batch loss 1.35595846 epoch total loss 1.41526508\n",
      "Trained batch 75 batch loss 1.39801 epoch total loss 1.41503501\n",
      "Trained batch 76 batch loss 1.29643297 epoch total loss 1.41347444\n",
      "Trained batch 77 batch loss 1.35754514 epoch total loss 1.4127481\n",
      "Trained batch 78 batch loss 1.34699655 epoch total loss 1.41190517\n",
      "Trained batch 79 batch loss 1.40438986 epoch total loss 1.41181\n",
      "Trained batch 80 batch loss 1.39777112 epoch total loss 1.41163456\n",
      "Trained batch 81 batch loss 1.35140395 epoch total loss 1.41089094\n",
      "Trained batch 82 batch loss 1.3005408 epoch total loss 1.40954518\n",
      "Trained batch 83 batch loss 1.4344157 epoch total loss 1.40984488\n",
      "Trained batch 84 batch loss 1.44500494 epoch total loss 1.41026342\n",
      "Trained batch 85 batch loss 1.35208511 epoch total loss 1.40957892\n",
      "Trained batch 86 batch loss 1.31934679 epoch total loss 1.40852964\n",
      "Trained batch 87 batch loss 1.34957099 epoch total loss 1.40785205\n",
      "Trained batch 88 batch loss 1.30477905 epoch total loss 1.4066807\n",
      "Trained batch 89 batch loss 1.41349947 epoch total loss 1.40675735\n",
      "Trained batch 90 batch loss 1.35767496 epoch total loss 1.40621197\n",
      "Trained batch 91 batch loss 1.53068209 epoch total loss 1.40757966\n",
      "Trained batch 92 batch loss 1.63626742 epoch total loss 1.41006541\n",
      "Trained batch 93 batch loss 1.57383871 epoch total loss 1.41182637\n",
      "Trained batch 94 batch loss 1.4439894 epoch total loss 1.4121685\n",
      "Trained batch 95 batch loss 1.39126205 epoch total loss 1.41194844\n",
      "Trained batch 96 batch loss 1.36563444 epoch total loss 1.411466\n",
      "Trained batch 97 batch loss 1.35835421 epoch total loss 1.41091835\n",
      "Trained batch 98 batch loss 1.47222018 epoch total loss 1.41154385\n",
      "Trained batch 99 batch loss 1.45462084 epoch total loss 1.41197896\n",
      "Trained batch 100 batch loss 1.53028834 epoch total loss 1.41316211\n",
      "Trained batch 101 batch loss 1.55682635 epoch total loss 1.41458452\n",
      "Trained batch 102 batch loss 1.59632015 epoch total loss 1.4163661\n",
      "Trained batch 103 batch loss 1.5366019 epoch total loss 1.41753352\n",
      "Trained batch 104 batch loss 1.57792497 epoch total loss 1.41907573\n",
      "Trained batch 105 batch loss 1.53567064 epoch total loss 1.42018616\n",
      "Trained batch 106 batch loss 1.48923743 epoch total loss 1.42083764\n",
      "Trained batch 107 batch loss 1.48759103 epoch total loss 1.42146158\n",
      "Trained batch 108 batch loss 1.45025229 epoch total loss 1.42172825\n",
      "Trained batch 109 batch loss 1.56920552 epoch total loss 1.42308116\n",
      "Trained batch 110 batch loss 1.56854486 epoch total loss 1.42440355\n",
      "Trained batch 111 batch loss 1.34947109 epoch total loss 1.42372847\n",
      "Trained batch 112 batch loss 1.1888237 epoch total loss 1.4216311\n",
      "Trained batch 113 batch loss 1.36344528 epoch total loss 1.42111623\n",
      "Trained batch 114 batch loss 1.45552778 epoch total loss 1.42141807\n",
      "Trained batch 115 batch loss 1.45486259 epoch total loss 1.42170882\n",
      "Trained batch 116 batch loss 1.48595762 epoch total loss 1.42226279\n",
      "Trained batch 117 batch loss 1.42426372 epoch total loss 1.42228\n",
      "Trained batch 118 batch loss 1.41659915 epoch total loss 1.42223179\n",
      "Trained batch 119 batch loss 1.44168627 epoch total loss 1.42239523\n",
      "Trained batch 120 batch loss 1.39472389 epoch total loss 1.42216468\n",
      "Trained batch 121 batch loss 1.37626338 epoch total loss 1.42178535\n",
      "Trained batch 122 batch loss 1.39820814 epoch total loss 1.42159212\n",
      "Trained batch 123 batch loss 1.33943391 epoch total loss 1.42092407\n",
      "Trained batch 124 batch loss 1.44190454 epoch total loss 1.42109334\n",
      "Trained batch 125 batch loss 1.42542064 epoch total loss 1.42112792\n",
      "Trained batch 126 batch loss 1.40982556 epoch total loss 1.42103815\n",
      "Trained batch 127 batch loss 1.37321758 epoch total loss 1.42066157\n",
      "Trained batch 128 batch loss 1.42911565 epoch total loss 1.42072773\n",
      "Trained batch 129 batch loss 1.46672583 epoch total loss 1.42108428\n",
      "Trained batch 130 batch loss 1.44614315 epoch total loss 1.42127693\n",
      "Trained batch 131 batch loss 1.40724635 epoch total loss 1.42116988\n",
      "Trained batch 132 batch loss 1.37305892 epoch total loss 1.42080534\n",
      "Trained batch 133 batch loss 1.38983154 epoch total loss 1.42057252\n",
      "Trained batch 134 batch loss 1.34450603 epoch total loss 1.42000484\n",
      "Trained batch 135 batch loss 1.39391768 epoch total loss 1.41981173\n",
      "Trained batch 136 batch loss 1.33281708 epoch total loss 1.41917193\n",
      "Trained batch 137 batch loss 1.35355091 epoch total loss 1.41869295\n",
      "Trained batch 138 batch loss 1.35376728 epoch total loss 1.41822243\n",
      "Trained batch 139 batch loss 1.33079743 epoch total loss 1.41759348\n",
      "Trained batch 140 batch loss 1.35921705 epoch total loss 1.41717649\n",
      "Trained batch 141 batch loss 1.35633922 epoch total loss 1.41674507\n",
      "Trained batch 142 batch loss 1.33034551 epoch total loss 1.41613662\n",
      "Trained batch 143 batch loss 1.32718587 epoch total loss 1.41551459\n",
      "Trained batch 144 batch loss 1.59275663 epoch total loss 1.41674542\n",
      "Trained batch 145 batch loss 1.56466079 epoch total loss 1.4177655\n",
      "Trained batch 146 batch loss 1.42467272 epoch total loss 1.41781282\n",
      "Trained batch 147 batch loss 1.65252 epoch total loss 1.41940951\n",
      "Trained batch 148 batch loss 1.62223542 epoch total loss 1.42078\n",
      "Trained batch 149 batch loss 1.55433512 epoch total loss 1.42167628\n",
      "Trained batch 150 batch loss 1.50126421 epoch total loss 1.42220688\n",
      "Trained batch 151 batch loss 1.36751699 epoch total loss 1.42184484\n",
      "Trained batch 152 batch loss 1.48712301 epoch total loss 1.42227423\n",
      "Trained batch 153 batch loss 1.60415864 epoch total loss 1.42346299\n",
      "Trained batch 154 batch loss 1.62266767 epoch total loss 1.42475653\n",
      "Trained batch 155 batch loss 1.40101147 epoch total loss 1.42460334\n",
      "Trained batch 156 batch loss 1.37363684 epoch total loss 1.42427671\n",
      "Trained batch 157 batch loss 1.33393407 epoch total loss 1.42370129\n",
      "Trained batch 158 batch loss 1.37044096 epoch total loss 1.42336416\n",
      "Trained batch 159 batch loss 1.42968631 epoch total loss 1.42340398\n",
      "Trained batch 160 batch loss 1.44899082 epoch total loss 1.42356384\n",
      "Trained batch 161 batch loss 1.48664427 epoch total loss 1.42395568\n",
      "Trained batch 162 batch loss 1.48778975 epoch total loss 1.42434978\n",
      "Trained batch 163 batch loss 1.54509854 epoch total loss 1.42509055\n",
      "Trained batch 164 batch loss 1.48885918 epoch total loss 1.42547941\n",
      "Trained batch 165 batch loss 1.50271487 epoch total loss 1.42594755\n",
      "Trained batch 166 batch loss 1.33918929 epoch total loss 1.42542493\n",
      "Trained batch 167 batch loss 1.42594874 epoch total loss 1.42542803\n",
      "Trained batch 168 batch loss 1.4347415 epoch total loss 1.42548347\n",
      "Trained batch 169 batch loss 1.41683328 epoch total loss 1.42543232\n",
      "Trained batch 170 batch loss 1.34289932 epoch total loss 1.42494678\n",
      "Trained batch 171 batch loss 1.38835144 epoch total loss 1.4247328\n",
      "Trained batch 172 batch loss 1.29647827 epoch total loss 1.42398715\n",
      "Trained batch 173 batch loss 1.30399573 epoch total loss 1.42329359\n",
      "Trained batch 174 batch loss 1.41137481 epoch total loss 1.42322505\n",
      "Trained batch 175 batch loss 1.4647826 epoch total loss 1.42346251\n",
      "Trained batch 176 batch loss 1.5216198 epoch total loss 1.42402029\n",
      "Trained batch 177 batch loss 1.49346805 epoch total loss 1.42441261\n",
      "Trained batch 178 batch loss 1.48490763 epoch total loss 1.42475247\n",
      "Trained batch 179 batch loss 1.53201556 epoch total loss 1.42535174\n",
      "Trained batch 180 batch loss 1.44715011 epoch total loss 1.42547286\n",
      "Trained batch 181 batch loss 1.43935847 epoch total loss 1.42554963\n",
      "Trained batch 182 batch loss 1.40798652 epoch total loss 1.42545307\n",
      "Trained batch 183 batch loss 1.49962938 epoch total loss 1.4258585\n",
      "Trained batch 184 batch loss 1.61033678 epoch total loss 1.42686117\n",
      "Trained batch 185 batch loss 1.44649458 epoch total loss 1.42696726\n",
      "Trained batch 186 batch loss 1.45811439 epoch total loss 1.42713463\n",
      "Trained batch 187 batch loss 1.38222814 epoch total loss 1.42689455\n",
      "Trained batch 188 batch loss 1.50250065 epoch total loss 1.42729676\n",
      "Trained batch 189 batch loss 1.44619501 epoch total loss 1.42739677\n",
      "Trained batch 190 batch loss 1.5308497 epoch total loss 1.4279412\n",
      "Trained batch 191 batch loss 1.50612116 epoch total loss 1.42835069\n",
      "Trained batch 192 batch loss 1.49688852 epoch total loss 1.4287076\n",
      "Trained batch 193 batch loss 1.44479537 epoch total loss 1.42879093\n",
      "Trained batch 194 batch loss 1.25663793 epoch total loss 1.42790365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 195 batch loss 1.33599985 epoch total loss 1.4274323\n",
      "Trained batch 196 batch loss 1.54552543 epoch total loss 1.4280349\n",
      "Trained batch 197 batch loss 1.48900211 epoch total loss 1.42834437\n",
      "Trained batch 198 batch loss 1.51740623 epoch total loss 1.42879415\n",
      "Trained batch 199 batch loss 1.44060659 epoch total loss 1.42885351\n",
      "Trained batch 200 batch loss 1.42143011 epoch total loss 1.42881632\n",
      "Trained batch 201 batch loss 1.50763142 epoch total loss 1.42920852\n",
      "Trained batch 202 batch loss 1.39058137 epoch total loss 1.42901731\n",
      "Trained batch 203 batch loss 1.3642863 epoch total loss 1.42869842\n",
      "Trained batch 204 batch loss 1.43074083 epoch total loss 1.42870855\n",
      "Trained batch 205 batch loss 1.44099486 epoch total loss 1.42876852\n",
      "Trained batch 206 batch loss 1.43466651 epoch total loss 1.42879713\n",
      "Trained batch 207 batch loss 1.43109977 epoch total loss 1.42880821\n",
      "Trained batch 208 batch loss 1.42154241 epoch total loss 1.42877328\n",
      "Trained batch 209 batch loss 1.45537663 epoch total loss 1.4289006\n",
      "Trained batch 210 batch loss 1.55494761 epoch total loss 1.42950094\n",
      "Trained batch 211 batch loss 1.41375256 epoch total loss 1.42942631\n",
      "Trained batch 212 batch loss 1.30484819 epoch total loss 1.42883861\n",
      "Trained batch 213 batch loss 1.40302038 epoch total loss 1.42871737\n",
      "Trained batch 214 batch loss 1.32378185 epoch total loss 1.42822707\n",
      "Trained batch 215 batch loss 1.35991716 epoch total loss 1.42790937\n",
      "Trained batch 216 batch loss 1.41502 epoch total loss 1.42784965\n",
      "Trained batch 217 batch loss 1.29952323 epoch total loss 1.42725837\n",
      "Trained batch 218 batch loss 1.33968902 epoch total loss 1.42685664\n",
      "Trained batch 219 batch loss 1.28027856 epoch total loss 1.42618728\n",
      "Trained batch 220 batch loss 1.47878265 epoch total loss 1.42642641\n",
      "Trained batch 221 batch loss 1.38324165 epoch total loss 1.42623103\n",
      "Trained batch 222 batch loss 1.39338338 epoch total loss 1.42608297\n",
      "Trained batch 223 batch loss 1.41604543 epoch total loss 1.42603803\n",
      "Trained batch 224 batch loss 1.30322158 epoch total loss 1.42548966\n",
      "Trained batch 225 batch loss 1.34666884 epoch total loss 1.42513943\n",
      "Trained batch 226 batch loss 1.39852476 epoch total loss 1.42502165\n",
      "Trained batch 227 batch loss 1.39644814 epoch total loss 1.42489588\n",
      "Trained batch 228 batch loss 1.29137444 epoch total loss 1.42431021\n",
      "Trained batch 229 batch loss 1.30066085 epoch total loss 1.42377031\n",
      "Trained batch 230 batch loss 1.42557383 epoch total loss 1.42377806\n",
      "Trained batch 231 batch loss 1.32683933 epoch total loss 1.42335844\n",
      "Trained batch 232 batch loss 1.42088866 epoch total loss 1.42334783\n",
      "Trained batch 233 batch loss 1.42201555 epoch total loss 1.42334223\n",
      "Trained batch 234 batch loss 1.41757488 epoch total loss 1.42331755\n",
      "Trained batch 235 batch loss 1.24759912 epoch total loss 1.42256975\n",
      "Trained batch 236 batch loss 1.26034093 epoch total loss 1.42188239\n",
      "Trained batch 237 batch loss 1.29981244 epoch total loss 1.42136729\n",
      "Trained batch 238 batch loss 1.269238 epoch total loss 1.42072797\n",
      "Trained batch 239 batch loss 1.31333113 epoch total loss 1.42027867\n",
      "Trained batch 240 batch loss 1.40095711 epoch total loss 1.4201982\n",
      "Trained batch 241 batch loss 1.42547631 epoch total loss 1.42022014\n",
      "Trained batch 242 batch loss 1.45474517 epoch total loss 1.42036271\n",
      "Trained batch 243 batch loss 1.33279681 epoch total loss 1.42000234\n",
      "Trained batch 244 batch loss 1.41806626 epoch total loss 1.41999447\n",
      "Trained batch 245 batch loss 1.30303288 epoch total loss 1.41951704\n",
      "Trained batch 246 batch loss 1.45848012 epoch total loss 1.41967535\n",
      "Trained batch 247 batch loss 1.40289044 epoch total loss 1.4196074\n",
      "Trained batch 248 batch loss 1.38707268 epoch total loss 1.41947627\n",
      "Trained batch 249 batch loss 1.49297619 epoch total loss 1.41977155\n",
      "Trained batch 250 batch loss 1.40775156 epoch total loss 1.41972339\n",
      "Trained batch 251 batch loss 1.40142906 epoch total loss 1.41965055\n",
      "Trained batch 252 batch loss 1.35219264 epoch total loss 1.41938281\n",
      "Trained batch 253 batch loss 1.38952708 epoch total loss 1.41926479\n",
      "Trained batch 254 batch loss 1.40978897 epoch total loss 1.41922748\n",
      "Trained batch 255 batch loss 1.54976785 epoch total loss 1.41973948\n",
      "Trained batch 256 batch loss 1.42082715 epoch total loss 1.41974378\n",
      "Trained batch 257 batch loss 1.50930619 epoch total loss 1.42009223\n",
      "Trained batch 258 batch loss 1.40839338 epoch total loss 1.42004693\n",
      "Trained batch 259 batch loss 1.43779659 epoch total loss 1.42011547\n",
      "Trained batch 260 batch loss 1.44225919 epoch total loss 1.42020059\n",
      "Trained batch 261 batch loss 1.38091075 epoch total loss 1.42005014\n",
      "Trained batch 262 batch loss 1.42769372 epoch total loss 1.42007935\n",
      "Trained batch 263 batch loss 1.44286489 epoch total loss 1.42016602\n",
      "Trained batch 264 batch loss 1.46568918 epoch total loss 1.42033851\n",
      "Trained batch 265 batch loss 1.46970367 epoch total loss 1.42052472\n",
      "Trained batch 266 batch loss 1.43176174 epoch total loss 1.42056704\n",
      "Trained batch 267 batch loss 1.4285121 epoch total loss 1.42059672\n",
      "Trained batch 268 batch loss 1.49194574 epoch total loss 1.42086291\n",
      "Trained batch 269 batch loss 1.43459582 epoch total loss 1.42091393\n",
      "Trained batch 270 batch loss 1.37076616 epoch total loss 1.42072821\n",
      "Trained batch 271 batch loss 1.3907603 epoch total loss 1.42061758\n",
      "Trained batch 272 batch loss 1.4132545 epoch total loss 1.42059052\n",
      "Trained batch 273 batch loss 1.37231517 epoch total loss 1.42041373\n",
      "Trained batch 274 batch loss 1.37852097 epoch total loss 1.42026079\n",
      "Trained batch 275 batch loss 1.30918169 epoch total loss 1.41985679\n",
      "Trained batch 276 batch loss 1.37887669 epoch total loss 1.41970837\n",
      "Trained batch 277 batch loss 1.33260107 epoch total loss 1.4193939\n",
      "Trained batch 278 batch loss 1.35486782 epoch total loss 1.4191618\n",
      "Trained batch 279 batch loss 1.44119775 epoch total loss 1.41924071\n",
      "Trained batch 280 batch loss 1.36597 epoch total loss 1.41905046\n",
      "Trained batch 281 batch loss 1.47920299 epoch total loss 1.41926455\n",
      "Trained batch 282 batch loss 1.46294487 epoch total loss 1.41941953\n",
      "Trained batch 283 batch loss 1.4070648 epoch total loss 1.4193759\n",
      "Trained batch 284 batch loss 1.45588541 epoch total loss 1.4195044\n",
      "Trained batch 285 batch loss 1.50365555 epoch total loss 1.41979969\n",
      "Trained batch 286 batch loss 1.47879148 epoch total loss 1.42000592\n",
      "Trained batch 287 batch loss 1.65579164 epoch total loss 1.42082751\n",
      "Trained batch 288 batch loss 1.56066072 epoch total loss 1.42131305\n",
      "Trained batch 289 batch loss 1.433846 epoch total loss 1.42135644\n",
      "Trained batch 290 batch loss 1.41890645 epoch total loss 1.42134798\n",
      "Trained batch 291 batch loss 1.25042427 epoch total loss 1.42076063\n",
      "Trained batch 292 batch loss 1.17084861 epoch total loss 1.41990471\n",
      "Trained batch 293 batch loss 1.43910491 epoch total loss 1.41997027\n",
      "Trained batch 294 batch loss 1.22585535 epoch total loss 1.41931009\n",
      "Trained batch 295 batch loss 1.1722008 epoch total loss 1.41847241\n",
      "Trained batch 296 batch loss 1.25613952 epoch total loss 1.41792405\n",
      "Trained batch 297 batch loss 1.22256827 epoch total loss 1.41726625\n",
      "Trained batch 298 batch loss 1.23939705 epoch total loss 1.41666937\n",
      "Trained batch 299 batch loss 1.39298296 epoch total loss 1.41659009\n",
      "Trained batch 300 batch loss 1.51587677 epoch total loss 1.41692114\n",
      "Trained batch 301 batch loss 1.37962615 epoch total loss 1.41679716\n",
      "Trained batch 302 batch loss 1.4807725 epoch total loss 1.41700912\n",
      "Trained batch 303 batch loss 1.54188788 epoch total loss 1.41742122\n",
      "Trained batch 304 batch loss 1.5273149 epoch total loss 1.41778278\n",
      "Trained batch 305 batch loss 1.58168256 epoch total loss 1.41832018\n",
      "Trained batch 306 batch loss 1.53880215 epoch total loss 1.41871381\n",
      "Trained batch 307 batch loss 1.49289274 epoch total loss 1.41895545\n",
      "Trained batch 308 batch loss 1.37462425 epoch total loss 1.41881156\n",
      "Trained batch 309 batch loss 1.42232943 epoch total loss 1.418823\n",
      "Trained batch 310 batch loss 1.41856694 epoch total loss 1.41882217\n",
      "Trained batch 311 batch loss 1.39357305 epoch total loss 1.41874099\n",
      "Trained batch 312 batch loss 1.3392849 epoch total loss 1.41848636\n",
      "Trained batch 313 batch loss 1.4514538 epoch total loss 1.41859174\n",
      "Trained batch 314 batch loss 1.44418979 epoch total loss 1.41867316\n",
      "Trained batch 315 batch loss 1.39137149 epoch total loss 1.41858649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 316 batch loss 1.42006195 epoch total loss 1.41859114\n",
      "Trained batch 317 batch loss 1.49112976 epoch total loss 1.41882\n",
      "Trained batch 318 batch loss 1.2853992 epoch total loss 1.41840041\n",
      "Trained batch 319 batch loss 1.38713193 epoch total loss 1.41830242\n",
      "Trained batch 320 batch loss 1.42000771 epoch total loss 1.41830778\n",
      "Trained batch 321 batch loss 1.37834966 epoch total loss 1.41818333\n",
      "Trained batch 322 batch loss 1.36678433 epoch total loss 1.41802371\n",
      "Trained batch 323 batch loss 1.32929969 epoch total loss 1.41774893\n",
      "Trained batch 324 batch loss 1.34572494 epoch total loss 1.41752672\n",
      "Trained batch 325 batch loss 1.48126113 epoch total loss 1.41772282\n",
      "Trained batch 326 batch loss 1.27506554 epoch total loss 1.4172852\n",
      "Trained batch 327 batch loss 1.27707243 epoch total loss 1.41685641\n",
      "Trained batch 328 batch loss 1.35726643 epoch total loss 1.41667473\n",
      "Trained batch 329 batch loss 1.34576356 epoch total loss 1.4164592\n",
      "Trained batch 330 batch loss 1.35872436 epoch total loss 1.41628432\n",
      "Trained batch 331 batch loss 1.40408134 epoch total loss 1.41624737\n",
      "Trained batch 332 batch loss 1.48883247 epoch total loss 1.416466\n",
      "Trained batch 333 batch loss 1.47456872 epoch total loss 1.41664052\n",
      "Trained batch 334 batch loss 1.52761137 epoch total loss 1.41697288\n",
      "Trained batch 335 batch loss 1.55877852 epoch total loss 1.41739607\n",
      "Trained batch 336 batch loss 1.34816313 epoch total loss 1.41719007\n",
      "Trained batch 337 batch loss 1.39118338 epoch total loss 1.41711295\n",
      "Trained batch 338 batch loss 1.37118101 epoch total loss 1.41697705\n",
      "Trained batch 339 batch loss 1.41885114 epoch total loss 1.41698253\n",
      "Trained batch 340 batch loss 1.38658237 epoch total loss 1.41689324\n",
      "Trained batch 341 batch loss 1.36264658 epoch total loss 1.4167341\n",
      "Trained batch 342 batch loss 1.29918456 epoch total loss 1.41639042\n",
      "Trained batch 343 batch loss 1.31456149 epoch total loss 1.41609359\n",
      "Trained batch 344 batch loss 1.3148272 epoch total loss 1.41579914\n",
      "Trained batch 345 batch loss 1.35442376 epoch total loss 1.41562128\n",
      "Trained batch 346 batch loss 1.32015705 epoch total loss 1.41534543\n",
      "Trained batch 347 batch loss 1.31345391 epoch total loss 1.4150517\n",
      "Trained batch 348 batch loss 1.36297154 epoch total loss 1.41490209\n",
      "Trained batch 349 batch loss 1.34486628 epoch total loss 1.41470146\n",
      "Trained batch 350 batch loss 1.32356787 epoch total loss 1.41444111\n",
      "Trained batch 351 batch loss 1.22969103 epoch total loss 1.4139148\n",
      "Trained batch 352 batch loss 1.33758688 epoch total loss 1.41369796\n",
      "Trained batch 353 batch loss 1.3956039 epoch total loss 1.4136467\n",
      "Trained batch 354 batch loss 1.39219832 epoch total loss 1.41358614\n",
      "Trained batch 355 batch loss 1.38817704 epoch total loss 1.41351461\n",
      "Trained batch 356 batch loss 1.4425 epoch total loss 1.41359603\n",
      "Trained batch 357 batch loss 1.31309617 epoch total loss 1.41331446\n",
      "Trained batch 358 batch loss 1.40223 epoch total loss 1.41328359\n",
      "Trained batch 359 batch loss 1.46493769 epoch total loss 1.41342735\n",
      "Trained batch 360 batch loss 1.40497398 epoch total loss 1.41340387\n",
      "Trained batch 361 batch loss 1.41514671 epoch total loss 1.41340876\n",
      "Trained batch 362 batch loss 1.42777777 epoch total loss 1.41344845\n",
      "Trained batch 363 batch loss 1.37020469 epoch total loss 1.41332924\n",
      "Trained batch 364 batch loss 1.29951 epoch total loss 1.41301656\n",
      "Trained batch 365 batch loss 1.2500757 epoch total loss 1.41257012\n",
      "Trained batch 366 batch loss 1.38455558 epoch total loss 1.41249359\n",
      "Trained batch 367 batch loss 1.32179689 epoch total loss 1.41224647\n",
      "Trained batch 368 batch loss 1.38822126 epoch total loss 1.41218114\n",
      "Trained batch 369 batch loss 1.38286877 epoch total loss 1.41210175\n",
      "Trained batch 370 batch loss 1.35664606 epoch total loss 1.41195178\n",
      "Trained batch 371 batch loss 1.4101181 epoch total loss 1.41194689\n",
      "Trained batch 372 batch loss 1.38727009 epoch total loss 1.41188049\n",
      "Trained batch 373 batch loss 1.47439718 epoch total loss 1.41204822\n",
      "Trained batch 374 batch loss 1.52280724 epoch total loss 1.41234434\n",
      "Trained batch 375 batch loss 1.32146132 epoch total loss 1.4121021\n",
      "Trained batch 376 batch loss 1.41961837 epoch total loss 1.41212201\n",
      "Trained batch 377 batch loss 1.45812583 epoch total loss 1.41224408\n",
      "Trained batch 378 batch loss 1.53459024 epoch total loss 1.41256773\n",
      "Trained batch 379 batch loss 1.42688847 epoch total loss 1.41260552\n",
      "Trained batch 380 batch loss 1.395401 epoch total loss 1.41256022\n",
      "Trained batch 381 batch loss 1.33037543 epoch total loss 1.41234457\n",
      "Trained batch 382 batch loss 1.32023406 epoch total loss 1.41210341\n",
      "Trained batch 383 batch loss 1.35920954 epoch total loss 1.41196537\n",
      "Trained batch 384 batch loss 1.44294119 epoch total loss 1.41204596\n",
      "Trained batch 385 batch loss 1.41864109 epoch total loss 1.41206312\n",
      "Trained batch 386 batch loss 1.37012959 epoch total loss 1.4119544\n",
      "Trained batch 387 batch loss 1.26634789 epoch total loss 1.41157818\n",
      "Trained batch 388 batch loss 1.35549462 epoch total loss 1.41143358\n",
      "Trained batch 389 batch loss 1.37647092 epoch total loss 1.41134369\n",
      "Trained batch 390 batch loss 1.29880667 epoch total loss 1.41105521\n",
      "Trained batch 391 batch loss 1.16951883 epoch total loss 1.41043735\n",
      "Trained batch 392 batch loss 1.1654824 epoch total loss 1.40981245\n",
      "Trained batch 393 batch loss 1.26913691 epoch total loss 1.40945458\n",
      "Trained batch 394 batch loss 1.27451658 epoch total loss 1.4091121\n",
      "Trained batch 395 batch loss 1.39271235 epoch total loss 1.40907061\n",
      "Trained batch 396 batch loss 1.32608962 epoch total loss 1.40886116\n",
      "Trained batch 397 batch loss 1.35322261 epoch total loss 1.40872097\n",
      "Trained batch 398 batch loss 1.36986303 epoch total loss 1.40862334\n",
      "Trained batch 399 batch loss 1.35786486 epoch total loss 1.40849602\n",
      "Trained batch 400 batch loss 1.28095961 epoch total loss 1.40817714\n",
      "Trained batch 401 batch loss 1.51453972 epoch total loss 1.40844238\n",
      "Trained batch 402 batch loss 1.30018926 epoch total loss 1.40817308\n",
      "Trained batch 403 batch loss 1.25560403 epoch total loss 1.40779448\n",
      "Trained batch 404 batch loss 1.35558283 epoch total loss 1.40766525\n",
      "Trained batch 405 batch loss 1.37265325 epoch total loss 1.40757895\n",
      "Trained batch 406 batch loss 1.45004892 epoch total loss 1.40768361\n",
      "Trained batch 407 batch loss 1.57527649 epoch total loss 1.40809536\n",
      "Trained batch 408 batch loss 1.52553535 epoch total loss 1.40838313\n",
      "Trained batch 409 batch loss 1.50967312 epoch total loss 1.40863073\n",
      "Trained batch 410 batch loss 1.32963049 epoch total loss 1.40843809\n",
      "Trained batch 411 batch loss 1.32937455 epoch total loss 1.40824556\n",
      "Trained batch 412 batch loss 1.2889452 epoch total loss 1.407956\n",
      "Trained batch 413 batch loss 1.3489579 epoch total loss 1.40781307\n",
      "Trained batch 414 batch loss 1.38185287 epoch total loss 1.40775037\n",
      "Trained batch 415 batch loss 1.34818077 epoch total loss 1.40760684\n",
      "Trained batch 416 batch loss 1.38375473 epoch total loss 1.4075495\n",
      "Trained batch 417 batch loss 1.40078545 epoch total loss 1.40753317\n",
      "Trained batch 418 batch loss 1.32713401 epoch total loss 1.40734088\n",
      "Trained batch 419 batch loss 1.28782952 epoch total loss 1.40705574\n",
      "Trained batch 420 batch loss 1.32189655 epoch total loss 1.40685296\n",
      "Trained batch 421 batch loss 1.33304238 epoch total loss 1.40667772\n",
      "Trained batch 422 batch loss 1.42576742 epoch total loss 1.4067229\n",
      "Trained batch 423 batch loss 1.3753612 epoch total loss 1.40664887\n",
      "Trained batch 424 batch loss 1.35680413 epoch total loss 1.40653133\n",
      "Trained batch 425 batch loss 1.30169821 epoch total loss 1.40628457\n",
      "Trained batch 426 batch loss 1.40715909 epoch total loss 1.40628672\n",
      "Trained batch 427 batch loss 1.42514336 epoch total loss 1.40633094\n",
      "Trained batch 428 batch loss 1.43869257 epoch total loss 1.40640652\n",
      "Trained batch 429 batch loss 1.43686008 epoch total loss 1.40647769\n",
      "Trained batch 430 batch loss 1.34213316 epoch total loss 1.40632808\n",
      "Trained batch 431 batch loss 1.28499973 epoch total loss 1.40604651\n",
      "Trained batch 432 batch loss 1.28455091 epoch total loss 1.4057653\n",
      "Trained batch 433 batch loss 1.36183536 epoch total loss 1.40566373\n",
      "Trained batch 434 batch loss 1.27833271 epoch total loss 1.40537035\n",
      "Trained batch 435 batch loss 1.3780371 epoch total loss 1.40530753\n",
      "Trained batch 436 batch loss 1.34840465 epoch total loss 1.405177\n",
      "Trained batch 437 batch loss 1.3678242 epoch total loss 1.4050914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 438 batch loss 1.44966447 epoch total loss 1.40519321\n",
      "Trained batch 439 batch loss 1.48927617 epoch total loss 1.40538466\n",
      "Trained batch 440 batch loss 1.41427529 epoch total loss 1.40540481\n",
      "Trained batch 441 batch loss 1.37113261 epoch total loss 1.40532708\n",
      "Trained batch 442 batch loss 1.30258894 epoch total loss 1.40509474\n",
      "Trained batch 443 batch loss 1.3453269 epoch total loss 1.4049598\n",
      "Trained batch 444 batch loss 1.30433333 epoch total loss 1.40473318\n",
      "Trained batch 445 batch loss 1.38945735 epoch total loss 1.40469885\n",
      "Trained batch 446 batch loss 1.46541262 epoch total loss 1.40483499\n",
      "Trained batch 447 batch loss 1.48963439 epoch total loss 1.40502465\n",
      "Trained batch 448 batch loss 1.58593869 epoch total loss 1.40542853\n",
      "Trained batch 449 batch loss 1.61381638 epoch total loss 1.40589261\n",
      "Trained batch 450 batch loss 1.50685239 epoch total loss 1.40611696\n",
      "Trained batch 451 batch loss 1.32762694 epoch total loss 1.40594292\n",
      "Trained batch 452 batch loss 1.40339184 epoch total loss 1.40593731\n",
      "Trained batch 453 batch loss 1.17182958 epoch total loss 1.40542042\n",
      "Trained batch 454 batch loss 1.13848615 epoch total loss 1.40483248\n",
      "Trained batch 455 batch loss 1.257846 epoch total loss 1.40450954\n",
      "Trained batch 456 batch loss 1.46633148 epoch total loss 1.40464497\n",
      "Trained batch 457 batch loss 1.34151769 epoch total loss 1.4045068\n",
      "Trained batch 458 batch loss 1.37877834 epoch total loss 1.40445065\n",
      "Trained batch 459 batch loss 1.43632364 epoch total loss 1.40452015\n",
      "Trained batch 460 batch loss 1.46309 epoch total loss 1.40464747\n",
      "Trained batch 461 batch loss 1.42569804 epoch total loss 1.40469313\n",
      "Trained batch 462 batch loss 1.39773738 epoch total loss 1.40467811\n",
      "Trained batch 463 batch loss 1.38007879 epoch total loss 1.40462494\n",
      "Trained batch 464 batch loss 1.46380544 epoch total loss 1.40475249\n",
      "Trained batch 465 batch loss 1.35920238 epoch total loss 1.4046545\n",
      "Trained batch 466 batch loss 1.35888028 epoch total loss 1.40455639\n",
      "Trained batch 467 batch loss 1.3878386 epoch total loss 1.40452051\n",
      "Trained batch 468 batch loss 1.49270821 epoch total loss 1.40470898\n",
      "Trained batch 469 batch loss 1.33998823 epoch total loss 1.40457094\n",
      "Trained batch 470 batch loss 1.32915306 epoch total loss 1.40441048\n",
      "Trained batch 471 batch loss 1.23774076 epoch total loss 1.40405667\n",
      "Trained batch 472 batch loss 1.25744808 epoch total loss 1.40374601\n",
      "Trained batch 473 batch loss 1.29173684 epoch total loss 1.40350926\n",
      "Trained batch 474 batch loss 1.36153626 epoch total loss 1.40342057\n",
      "Trained batch 475 batch loss 1.3263346 epoch total loss 1.40325832\n",
      "Trained batch 476 batch loss 1.19942045 epoch total loss 1.40283012\n",
      "Trained batch 477 batch loss 1.28115523 epoch total loss 1.40257502\n",
      "Trained batch 478 batch loss 1.40171993 epoch total loss 1.40257323\n",
      "Trained batch 479 batch loss 1.43057013 epoch total loss 1.40263164\n",
      "Trained batch 480 batch loss 1.40886688 epoch total loss 1.40264463\n",
      "Trained batch 481 batch loss 1.37828445 epoch total loss 1.40259397\n",
      "Trained batch 482 batch loss 1.29910612 epoch total loss 1.40237927\n",
      "Trained batch 483 batch loss 1.15456653 epoch total loss 1.4018662\n",
      "Trained batch 484 batch loss 1.30695 epoch total loss 1.4016701\n",
      "Trained batch 485 batch loss 1.41359949 epoch total loss 1.40169466\n",
      "Trained batch 486 batch loss 1.4263916 epoch total loss 1.40174544\n",
      "Trained batch 487 batch loss 1.49390125 epoch total loss 1.40193462\n",
      "Trained batch 488 batch loss 1.51132417 epoch total loss 1.40215886\n",
      "Trained batch 489 batch loss 1.50595593 epoch total loss 1.40237117\n",
      "Trained batch 490 batch loss 1.44052291 epoch total loss 1.40244913\n",
      "Trained batch 491 batch loss 1.4145844 epoch total loss 1.40247393\n",
      "Trained batch 492 batch loss 1.38731658 epoch total loss 1.40244317\n",
      "Trained batch 493 batch loss 1.37721848 epoch total loss 1.40239191\n",
      "Trained batch 494 batch loss 1.47493315 epoch total loss 1.40253866\n",
      "Trained batch 495 batch loss 1.45087934 epoch total loss 1.40263629\n",
      "Trained batch 496 batch loss 1.51560283 epoch total loss 1.4028641\n",
      "Trained batch 497 batch loss 1.33399475 epoch total loss 1.40272558\n",
      "Trained batch 498 batch loss 1.38581502 epoch total loss 1.4026916\n",
      "Trained batch 499 batch loss 1.38900614 epoch total loss 1.40266407\n",
      "Trained batch 500 batch loss 1.39038777 epoch total loss 1.40263951\n",
      "Trained batch 501 batch loss 1.30002189 epoch total loss 1.40243471\n",
      "Trained batch 502 batch loss 1.36265087 epoch total loss 1.40235555\n",
      "Trained batch 503 batch loss 1.31280947 epoch total loss 1.40217745\n",
      "Trained batch 504 batch loss 1.27815092 epoch total loss 1.40193141\n",
      "Trained batch 505 batch loss 1.3668555 epoch total loss 1.40186203\n",
      "Trained batch 506 batch loss 1.27770853 epoch total loss 1.40161669\n",
      "Trained batch 507 batch loss 1.27367806 epoch total loss 1.40136433\n",
      "Trained batch 508 batch loss 1.27754283 epoch total loss 1.40112054\n",
      "Trained batch 509 batch loss 1.21262515 epoch total loss 1.40075028\n",
      "Trained batch 510 batch loss 1.28456807 epoch total loss 1.40052235\n",
      "Trained batch 511 batch loss 1.17524242 epoch total loss 1.40008152\n",
      "Trained batch 512 batch loss 1.23253393 epoch total loss 1.39975429\n",
      "Trained batch 513 batch loss 1.35115743 epoch total loss 1.39965951\n",
      "Trained batch 514 batch loss 1.34137094 epoch total loss 1.39954615\n",
      "Trained batch 515 batch loss 1.3811847 epoch total loss 1.39951038\n",
      "Trained batch 516 batch loss 1.38406694 epoch total loss 1.39948058\n",
      "Trained batch 517 batch loss 1.44857538 epoch total loss 1.39957547\n",
      "Trained batch 518 batch loss 1.41127586 epoch total loss 1.399598\n",
      "Trained batch 519 batch loss 1.44410276 epoch total loss 1.39968371\n",
      "Trained batch 520 batch loss 1.42545092 epoch total loss 1.3997333\n",
      "Trained batch 521 batch loss 1.45711219 epoch total loss 1.39984345\n",
      "Trained batch 522 batch loss 1.22495437 epoch total loss 1.39950848\n",
      "Trained batch 523 batch loss 1.39234173 epoch total loss 1.39949465\n",
      "Trained batch 524 batch loss 1.35055625 epoch total loss 1.39940131\n",
      "Trained batch 525 batch loss 1.32199907 epoch total loss 1.39925396\n",
      "Trained batch 526 batch loss 1.38677025 epoch total loss 1.39923024\n",
      "Trained batch 527 batch loss 1.38944781 epoch total loss 1.39921176\n",
      "Trained batch 528 batch loss 1.33001363 epoch total loss 1.39908063\n",
      "Trained batch 529 batch loss 1.3978157 epoch total loss 1.39907837\n",
      "Trained batch 530 batch loss 1.21361923 epoch total loss 1.39872837\n",
      "Trained batch 531 batch loss 1.21236062 epoch total loss 1.39837742\n",
      "Trained batch 532 batch loss 1.31464911 epoch total loss 1.39822\n",
      "Trained batch 533 batch loss 1.2798903 epoch total loss 1.39799798\n",
      "Trained batch 534 batch loss 1.27913713 epoch total loss 1.39777541\n",
      "Trained batch 535 batch loss 1.2971735 epoch total loss 1.3975873\n",
      "Trained batch 536 batch loss 1.32392311 epoch total loss 1.39744985\n",
      "Trained batch 537 batch loss 1.34357929 epoch total loss 1.3973496\n",
      "Trained batch 538 batch loss 1.4551847 epoch total loss 1.39745712\n",
      "Trained batch 539 batch loss 1.40514922 epoch total loss 1.39747131\n",
      "Trained batch 540 batch loss 1.49698973 epoch total loss 1.39765573\n",
      "Trained batch 541 batch loss 1.61113191 epoch total loss 1.39805031\n",
      "Trained batch 542 batch loss 1.55026507 epoch total loss 1.39833117\n",
      "Trained batch 543 batch loss 1.51983082 epoch total loss 1.39855492\n",
      "Trained batch 544 batch loss 1.42103243 epoch total loss 1.39859629\n",
      "Trained batch 545 batch loss 1.35184646 epoch total loss 1.39851058\n",
      "Trained batch 546 batch loss 1.39378524 epoch total loss 1.39850187\n",
      "Trained batch 547 batch loss 1.36007082 epoch total loss 1.39843154\n",
      "Trained batch 548 batch loss 1.37246335 epoch total loss 1.39838409\n",
      "Trained batch 549 batch loss 1.44769716 epoch total loss 1.39847398\n",
      "Trained batch 550 batch loss 1.46060276 epoch total loss 1.39858699\n",
      "Trained batch 551 batch loss 1.40554333 epoch total loss 1.39859951\n",
      "Trained batch 552 batch loss 1.46112156 epoch total loss 1.39871287\n",
      "Trained batch 553 batch loss 1.3428669 epoch total loss 1.3986119\n",
      "Trained batch 554 batch loss 1.36884642 epoch total loss 1.39855814\n",
      "Trained batch 555 batch loss 1.37980139 epoch total loss 1.3985244\n",
      "Trained batch 556 batch loss 1.4220463 epoch total loss 1.39856672\n",
      "Trained batch 557 batch loss 1.41791034 epoch total loss 1.39860141\n",
      "Trained batch 558 batch loss 1.36637282 epoch total loss 1.39854372\n",
      "Trained batch 559 batch loss 1.41247785 epoch total loss 1.39856863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 560 batch loss 1.39613652 epoch total loss 1.39856422\n",
      "Trained batch 561 batch loss 1.35731792 epoch total loss 1.39849067\n",
      "Trained batch 562 batch loss 1.41449332 epoch total loss 1.39851916\n",
      "Trained batch 563 batch loss 1.40902245 epoch total loss 1.39853776\n",
      "Trained batch 564 batch loss 1.28628564 epoch total loss 1.39833879\n",
      "Trained batch 565 batch loss 1.38604641 epoch total loss 1.3983171\n",
      "Trained batch 566 batch loss 1.43851566 epoch total loss 1.39838815\n",
      "Trained batch 567 batch loss 1.42597175 epoch total loss 1.39843678\n",
      "Trained batch 568 batch loss 1.6351285 epoch total loss 1.39885342\n",
      "Trained batch 569 batch loss 1.55400825 epoch total loss 1.39912617\n",
      "Trained batch 570 batch loss 1.53494298 epoch total loss 1.39936447\n",
      "Trained batch 571 batch loss 1.53721082 epoch total loss 1.39960599\n",
      "Trained batch 572 batch loss 1.38568711 epoch total loss 1.39958155\n",
      "Trained batch 573 batch loss 1.20741558 epoch total loss 1.39924622\n",
      "Trained batch 574 batch loss 1.31989157 epoch total loss 1.39910793\n",
      "Trained batch 575 batch loss 1.38399148 epoch total loss 1.39908159\n",
      "Trained batch 576 batch loss 1.53154802 epoch total loss 1.39931154\n",
      "Trained batch 577 batch loss 1.43760085 epoch total loss 1.39937794\n",
      "Trained batch 578 batch loss 1.32302856 epoch total loss 1.39924598\n",
      "Trained batch 579 batch loss 1.28322279 epoch total loss 1.39904559\n",
      "Trained batch 580 batch loss 1.35708141 epoch total loss 1.39897311\n",
      "Trained batch 581 batch loss 1.28982019 epoch total loss 1.39878523\n",
      "Trained batch 582 batch loss 1.34967089 epoch total loss 1.39870083\n",
      "Trained batch 583 batch loss 1.2935462 epoch total loss 1.39852047\n",
      "Trained batch 584 batch loss 1.33297634 epoch total loss 1.39840817\n",
      "Trained batch 585 batch loss 1.30380857 epoch total loss 1.39824641\n",
      "Trained batch 586 batch loss 1.25715256 epoch total loss 1.39800572\n",
      "Trained batch 587 batch loss 1.20313466 epoch total loss 1.39767373\n",
      "Trained batch 588 batch loss 1.39689767 epoch total loss 1.39767241\n",
      "Trained batch 589 batch loss 1.47731304 epoch total loss 1.3978076\n",
      "Trained batch 590 batch loss 1.41298223 epoch total loss 1.39783323\n",
      "Trained batch 591 batch loss 1.55542874 epoch total loss 1.3980999\n",
      "Trained batch 592 batch loss 1.44670892 epoch total loss 1.39818203\n",
      "Trained batch 593 batch loss 1.45465457 epoch total loss 1.39827728\n",
      "Trained batch 594 batch loss 1.23714721 epoch total loss 1.39800596\n",
      "Trained batch 595 batch loss 1.31720889 epoch total loss 1.39787018\n",
      "Trained batch 596 batch loss 1.43190658 epoch total loss 1.39792717\n",
      "Trained batch 597 batch loss 1.39671755 epoch total loss 1.39792514\n",
      "Trained batch 598 batch loss 1.4543438 epoch total loss 1.39801955\n",
      "Trained batch 599 batch loss 1.43221188 epoch total loss 1.39807653\n",
      "Trained batch 600 batch loss 1.45132399 epoch total loss 1.39816523\n",
      "Trained batch 601 batch loss 1.44917119 epoch total loss 1.3982501\n",
      "Trained batch 602 batch loss 1.53489852 epoch total loss 1.3984772\n",
      "Trained batch 603 batch loss 1.50981939 epoch total loss 1.39866185\n",
      "Trained batch 604 batch loss 1.46387756 epoch total loss 1.39876974\n",
      "Trained batch 605 batch loss 1.51759982 epoch total loss 1.39896619\n",
      "Trained batch 606 batch loss 1.48063779 epoch total loss 1.3991009\n",
      "Trained batch 607 batch loss 1.45086193 epoch total loss 1.39918625\n",
      "Trained batch 608 batch loss 1.48583102 epoch total loss 1.39932871\n",
      "Trained batch 609 batch loss 1.40374148 epoch total loss 1.39933598\n",
      "Trained batch 610 batch loss 1.3165493 epoch total loss 1.3992002\n",
      "Trained batch 611 batch loss 1.2930491 epoch total loss 1.39902651\n",
      "Trained batch 612 batch loss 1.29550362 epoch total loss 1.39885736\n",
      "Trained batch 613 batch loss 1.30685675 epoch total loss 1.39870727\n",
      "Trained batch 614 batch loss 1.4141376 epoch total loss 1.39873242\n",
      "Trained batch 615 batch loss 1.28043747 epoch total loss 1.39854014\n",
      "Trained batch 616 batch loss 1.2641933 epoch total loss 1.39832211\n",
      "Trained batch 617 batch loss 1.42410147 epoch total loss 1.39836383\n",
      "Trained batch 618 batch loss 1.45581603 epoch total loss 1.39845681\n",
      "Trained batch 619 batch loss 1.4261179 epoch total loss 1.39850152\n",
      "Trained batch 620 batch loss 1.32791829 epoch total loss 1.39838767\n",
      "Trained batch 621 batch loss 1.41172123 epoch total loss 1.39840925\n",
      "Trained batch 622 batch loss 1.2621491 epoch total loss 1.39819014\n",
      "Trained batch 623 batch loss 1.29074264 epoch total loss 1.39801764\n",
      "Trained batch 624 batch loss 1.43582118 epoch total loss 1.3980782\n",
      "Trained batch 625 batch loss 1.52899992 epoch total loss 1.39828765\n",
      "Trained batch 626 batch loss 1.4387666 epoch total loss 1.39835238\n",
      "Trained batch 627 batch loss 1.29968882 epoch total loss 1.39819503\n",
      "Trained batch 628 batch loss 1.32884681 epoch total loss 1.39808464\n",
      "Trained batch 629 batch loss 1.18386328 epoch total loss 1.39774394\n",
      "Trained batch 630 batch loss 1.15124035 epoch total loss 1.3973527\n",
      "Trained batch 631 batch loss 1.14857721 epoch total loss 1.39695847\n",
      "Trained batch 632 batch loss 1.21609199 epoch total loss 1.39667225\n",
      "Trained batch 633 batch loss 1.5610044 epoch total loss 1.39693177\n",
      "Trained batch 634 batch loss 1.69083428 epoch total loss 1.39739537\n",
      "Trained batch 635 batch loss 1.35336781 epoch total loss 1.39732611\n",
      "Trained batch 636 batch loss 1.46367705 epoch total loss 1.39743042\n",
      "Trained batch 637 batch loss 1.36914206 epoch total loss 1.39738595\n",
      "Trained batch 638 batch loss 1.43917227 epoch total loss 1.39745152\n",
      "Trained batch 639 batch loss 1.45356238 epoch total loss 1.39753926\n",
      "Trained batch 640 batch loss 1.3994987 epoch total loss 1.39754224\n",
      "Trained batch 641 batch loss 1.37785101 epoch total loss 1.3975116\n",
      "Trained batch 642 batch loss 1.41167498 epoch total loss 1.39753366\n",
      "Trained batch 643 batch loss 1.48267829 epoch total loss 1.3976661\n",
      "Trained batch 644 batch loss 1.3967855 epoch total loss 1.39766467\n",
      "Trained batch 645 batch loss 1.36298466 epoch total loss 1.3976109\n",
      "Trained batch 646 batch loss 1.30059314 epoch total loss 1.3974607\n",
      "Trained batch 647 batch loss 1.37016284 epoch total loss 1.39741862\n",
      "Trained batch 648 batch loss 1.41186047 epoch total loss 1.39744091\n",
      "Trained batch 649 batch loss 1.35405421 epoch total loss 1.39737403\n",
      "Trained batch 650 batch loss 1.21263564 epoch total loss 1.39708984\n",
      "Trained batch 651 batch loss 1.2691927 epoch total loss 1.39689338\n",
      "Trained batch 652 batch loss 1.30250061 epoch total loss 1.39674854\n",
      "Trained batch 653 batch loss 1.22034061 epoch total loss 1.39647841\n",
      "Trained batch 654 batch loss 1.26020062 epoch total loss 1.39627\n",
      "Trained batch 655 batch loss 1.24462497 epoch total loss 1.39603853\n",
      "Trained batch 656 batch loss 1.25699759 epoch total loss 1.39582658\n",
      "Trained batch 657 batch loss 1.41154051 epoch total loss 1.39585054\n",
      "Trained batch 658 batch loss 1.63612533 epoch total loss 1.39621568\n",
      "Trained batch 659 batch loss 1.36954927 epoch total loss 1.39617527\n",
      "Trained batch 660 batch loss 1.38931131 epoch total loss 1.39616477\n",
      "Trained batch 661 batch loss 1.33938456 epoch total loss 1.39607882\n",
      "Trained batch 662 batch loss 1.30634856 epoch total loss 1.39594328\n",
      "Trained batch 663 batch loss 1.30450606 epoch total loss 1.39580536\n",
      "Trained batch 664 batch loss 1.32198024 epoch total loss 1.39569414\n",
      "Trained batch 665 batch loss 1.36926639 epoch total loss 1.39565444\n",
      "Trained batch 666 batch loss 1.39791965 epoch total loss 1.39565778\n",
      "Trained batch 667 batch loss 1.37493157 epoch total loss 1.39562678\n",
      "Trained batch 668 batch loss 1.41386175 epoch total loss 1.39565408\n",
      "Trained batch 669 batch loss 1.31601501 epoch total loss 1.39553511\n",
      "Trained batch 670 batch loss 1.32452345 epoch total loss 1.39542913\n",
      "Trained batch 671 batch loss 1.300331 epoch total loss 1.39528739\n",
      "Trained batch 672 batch loss 1.33979177 epoch total loss 1.39520478\n",
      "Trained batch 673 batch loss 1.38347483 epoch total loss 1.39518738\n",
      "Trained batch 674 batch loss 1.33949304 epoch total loss 1.39510477\n",
      "Trained batch 675 batch loss 1.31069589 epoch total loss 1.3949796\n",
      "Trained batch 676 batch loss 1.33228874 epoch total loss 1.39488685\n",
      "Trained batch 677 batch loss 1.29819465 epoch total loss 1.39474416\n",
      "Trained batch 678 batch loss 1.29601169 epoch total loss 1.39459848\n",
      "Trained batch 679 batch loss 1.3794229 epoch total loss 1.39457607\n",
      "Trained batch 680 batch loss 1.24347425 epoch total loss 1.39435387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 681 batch loss 1.16298509 epoch total loss 1.39401412\n",
      "Trained batch 682 batch loss 1.05541658 epoch total loss 1.39351761\n",
      "Trained batch 683 batch loss 1.13459635 epoch total loss 1.39313853\n",
      "Trained batch 684 batch loss 1.4845016 epoch total loss 1.39327204\n",
      "Trained batch 685 batch loss 1.4137361 epoch total loss 1.39330196\n",
      "Trained batch 686 batch loss 1.3709445 epoch total loss 1.39326942\n",
      "Trained batch 687 batch loss 1.38396263 epoch total loss 1.39325595\n",
      "Trained batch 688 batch loss 1.31635833 epoch total loss 1.39314413\n",
      "Trained batch 689 batch loss 1.33351684 epoch total loss 1.39305758\n",
      "Trained batch 690 batch loss 1.23229957 epoch total loss 1.39282453\n",
      "Trained batch 691 batch loss 1.3556205 epoch total loss 1.39277065\n",
      "Trained batch 692 batch loss 1.36012042 epoch total loss 1.39272344\n",
      "Trained batch 693 batch loss 1.32269347 epoch total loss 1.39262247\n",
      "Trained batch 694 batch loss 1.33727908 epoch total loss 1.39254272\n",
      "Trained batch 695 batch loss 1.39605653 epoch total loss 1.39254773\n",
      "Trained batch 696 batch loss 1.42341793 epoch total loss 1.39259207\n",
      "Trained batch 697 batch loss 1.32451844 epoch total loss 1.39249444\n",
      "Trained batch 698 batch loss 1.2396431 epoch total loss 1.39227545\n",
      "Trained batch 699 batch loss 1.25662756 epoch total loss 1.39208138\n",
      "Trained batch 700 batch loss 1.26828468 epoch total loss 1.39190459\n",
      "Trained batch 701 batch loss 1.3640976 epoch total loss 1.3918649\n",
      "Trained batch 702 batch loss 1.46889949 epoch total loss 1.39197457\n",
      "Trained batch 703 batch loss 1.61638284 epoch total loss 1.39229381\n",
      "Trained batch 704 batch loss 1.61295867 epoch total loss 1.39260721\n",
      "Trained batch 705 batch loss 1.46890879 epoch total loss 1.39271557\n",
      "Trained batch 706 batch loss 1.44788969 epoch total loss 1.39279366\n",
      "Trained batch 707 batch loss 1.36463356 epoch total loss 1.39275384\n",
      "Trained batch 708 batch loss 1.23899937 epoch total loss 1.39253664\n",
      "Trained batch 709 batch loss 1.28448176 epoch total loss 1.39238429\n",
      "Trained batch 710 batch loss 1.46339273 epoch total loss 1.39248431\n",
      "Trained batch 711 batch loss 1.34589291 epoch total loss 1.39241874\n",
      "Trained batch 712 batch loss 1.38035262 epoch total loss 1.39240181\n",
      "Trained batch 713 batch loss 1.44324899 epoch total loss 1.3924731\n",
      "Trained batch 714 batch loss 1.35523748 epoch total loss 1.39242089\n",
      "Trained batch 715 batch loss 1.33311248 epoch total loss 1.39233804\n",
      "Trained batch 716 batch loss 1.36837077 epoch total loss 1.39230454\n",
      "Trained batch 717 batch loss 1.33223295 epoch total loss 1.39222074\n",
      "Trained batch 718 batch loss 1.29697466 epoch total loss 1.39208806\n",
      "Trained batch 719 batch loss 1.3588208 epoch total loss 1.3920418\n",
      "Trained batch 720 batch loss 1.36021209 epoch total loss 1.39199758\n",
      "Trained batch 721 batch loss 1.32879329 epoch total loss 1.39191\n",
      "Trained batch 722 batch loss 1.29032934 epoch total loss 1.39176929\n",
      "Trained batch 723 batch loss 1.29590809 epoch total loss 1.39163673\n",
      "Trained batch 724 batch loss 1.39576089 epoch total loss 1.39164233\n",
      "Trained batch 725 batch loss 1.45503569 epoch total loss 1.39172983\n",
      "Trained batch 726 batch loss 1.40306067 epoch total loss 1.39174545\n",
      "Trained batch 727 batch loss 1.33699906 epoch total loss 1.39167011\n",
      "Trained batch 728 batch loss 1.20438266 epoch total loss 1.39141285\n",
      "Trained batch 729 batch loss 1.35608423 epoch total loss 1.39136434\n",
      "Trained batch 730 batch loss 1.43793046 epoch total loss 1.39142811\n",
      "Trained batch 731 batch loss 1.42214131 epoch total loss 1.39147019\n",
      "Trained batch 732 batch loss 1.41483283 epoch total loss 1.39150214\n",
      "Trained batch 733 batch loss 1.4728117 epoch total loss 1.39161301\n",
      "Trained batch 734 batch loss 1.47506249 epoch total loss 1.39172673\n",
      "Trained batch 735 batch loss 1.43644202 epoch total loss 1.39178753\n",
      "Trained batch 736 batch loss 1.39689279 epoch total loss 1.39179444\n",
      "Trained batch 737 batch loss 1.34036338 epoch total loss 1.39172459\n",
      "Trained batch 738 batch loss 1.42181683 epoch total loss 1.39176548\n",
      "Trained batch 739 batch loss 1.31014431 epoch total loss 1.39165509\n",
      "Trained batch 740 batch loss 1.32307518 epoch total loss 1.39156246\n",
      "Trained batch 741 batch loss 1.40418863 epoch total loss 1.39157951\n",
      "Trained batch 742 batch loss 1.34713399 epoch total loss 1.39151967\n",
      "Trained batch 743 batch loss 1.33019948 epoch total loss 1.39143705\n",
      "Trained batch 744 batch loss 1.36014652 epoch total loss 1.39139497\n",
      "Trained batch 745 batch loss 1.38226676 epoch total loss 1.39138281\n",
      "Trained batch 746 batch loss 1.45621598 epoch total loss 1.39146972\n",
      "Trained batch 747 batch loss 1.45045555 epoch total loss 1.39154863\n",
      "Trained batch 748 batch loss 1.43969631 epoch total loss 1.39161301\n",
      "Trained batch 749 batch loss 1.40704858 epoch total loss 1.39163363\n",
      "Trained batch 750 batch loss 1.41710556 epoch total loss 1.3916676\n",
      "Trained batch 751 batch loss 1.35685718 epoch total loss 1.39162123\n",
      "Trained batch 752 batch loss 1.2943809 epoch total loss 1.39149201\n",
      "Trained batch 753 batch loss 1.33880377 epoch total loss 1.39142191\n",
      "Trained batch 754 batch loss 1.39609277 epoch total loss 1.39142811\n",
      "Trained batch 755 batch loss 1.23154032 epoch total loss 1.3912164\n",
      "Trained batch 756 batch loss 1.44174469 epoch total loss 1.39128327\n",
      "Trained batch 757 batch loss 1.41550589 epoch total loss 1.39131534\n",
      "Trained batch 758 batch loss 1.36308479 epoch total loss 1.39127803\n",
      "Trained batch 759 batch loss 1.35634685 epoch total loss 1.39123201\n",
      "Trained batch 760 batch loss 1.3997153 epoch total loss 1.3912431\n",
      "Trained batch 761 batch loss 1.19805264 epoch total loss 1.39098918\n",
      "Trained batch 762 batch loss 1.38556421 epoch total loss 1.39098203\n",
      "Trained batch 763 batch loss 1.41362453 epoch total loss 1.39101171\n",
      "Trained batch 764 batch loss 1.32447016 epoch total loss 1.39092457\n",
      "Trained batch 765 batch loss 1.22682703 epoch total loss 1.39071\n",
      "Trained batch 766 batch loss 1.40664363 epoch total loss 1.39073086\n",
      "Trained batch 767 batch loss 1.4812479 epoch total loss 1.39084876\n",
      "Trained batch 768 batch loss 1.31987906 epoch total loss 1.39075625\n",
      "Trained batch 769 batch loss 1.28875494 epoch total loss 1.39062357\n",
      "Trained batch 770 batch loss 1.24736786 epoch total loss 1.39043748\n",
      "Trained batch 771 batch loss 1.25250781 epoch total loss 1.39025867\n",
      "Trained batch 772 batch loss 1.19989228 epoch total loss 1.39001215\n",
      "Trained batch 773 batch loss 1.16994286 epoch total loss 1.38972735\n",
      "Trained batch 774 batch loss 1.21569705 epoch total loss 1.38950253\n",
      "Trained batch 775 batch loss 1.27462482 epoch total loss 1.38935435\n",
      "Trained batch 776 batch loss 1.30040467 epoch total loss 1.38923979\n",
      "Trained batch 777 batch loss 1.31770992 epoch total loss 1.38914776\n",
      "Trained batch 778 batch loss 1.41188121 epoch total loss 1.38917696\n",
      "Trained batch 779 batch loss 1.35738492 epoch total loss 1.3891362\n",
      "Trained batch 780 batch loss 1.40057349 epoch total loss 1.38915074\n",
      "Trained batch 781 batch loss 1.31515169 epoch total loss 1.38905609\n",
      "Trained batch 782 batch loss 1.29817247 epoch total loss 1.38893986\n",
      "Trained batch 783 batch loss 1.1622889 epoch total loss 1.38865042\n",
      "Trained batch 784 batch loss 1.32863164 epoch total loss 1.38857377\n",
      "Trained batch 785 batch loss 1.4100014 epoch total loss 1.38860106\n",
      "Trained batch 786 batch loss 1.32981467 epoch total loss 1.38852632\n",
      "Trained batch 787 batch loss 1.48528886 epoch total loss 1.38864923\n",
      "Trained batch 788 batch loss 1.4714992 epoch total loss 1.38875449\n",
      "Trained batch 789 batch loss 1.4182353 epoch total loss 1.3887918\n",
      "Trained batch 790 batch loss 1.33462977 epoch total loss 1.38872313\n",
      "Trained batch 791 batch loss 1.48256707 epoch total loss 1.38884175\n",
      "Trained batch 792 batch loss 1.45166922 epoch total loss 1.38892114\n",
      "Trained batch 793 batch loss 1.34672284 epoch total loss 1.38886786\n",
      "Trained batch 794 batch loss 1.4535569 epoch total loss 1.38894939\n",
      "Trained batch 795 batch loss 1.44480968 epoch total loss 1.38901961\n",
      "Trained batch 796 batch loss 1.42218113 epoch total loss 1.38906145\n",
      "Trained batch 797 batch loss 1.33133078 epoch total loss 1.38898897\n",
      "Trained batch 798 batch loss 1.47080231 epoch total loss 1.38909149\n",
      "Trained batch 799 batch loss 1.51753664 epoch total loss 1.38925231\n",
      "Trained batch 800 batch loss 1.51212251 epoch total loss 1.38940585\n",
      "Trained batch 801 batch loss 1.41767168 epoch total loss 1.38944113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 802 batch loss 1.34413743 epoch total loss 1.38938463\n",
      "Trained batch 803 batch loss 1.38458931 epoch total loss 1.38937879\n",
      "Trained batch 804 batch loss 1.36554468 epoch total loss 1.38934922\n",
      "Trained batch 805 batch loss 1.40090156 epoch total loss 1.38936353\n",
      "Trained batch 806 batch loss 1.37117267 epoch total loss 1.389341\n",
      "Trained batch 807 batch loss 1.38838434 epoch total loss 1.3893398\n",
      "Trained batch 808 batch loss 1.20766509 epoch total loss 1.38911498\n",
      "Trained batch 809 batch loss 1.31405973 epoch total loss 1.38902223\n",
      "Trained batch 810 batch loss 1.34855747 epoch total loss 1.38897228\n",
      "Trained batch 811 batch loss 1.31069124 epoch total loss 1.38887572\n",
      "Trained batch 812 batch loss 1.25772703 epoch total loss 1.38871408\n",
      "Trained batch 813 batch loss 1.25428903 epoch total loss 1.38854873\n",
      "Trained batch 814 batch loss 1.26597559 epoch total loss 1.38839817\n",
      "Trained batch 815 batch loss 1.26375735 epoch total loss 1.38824534\n",
      "Trained batch 816 batch loss 1.2478199 epoch total loss 1.38807321\n",
      "Trained batch 817 batch loss 1.31797445 epoch total loss 1.38798738\n",
      "Trained batch 818 batch loss 1.2907747 epoch total loss 1.38786852\n",
      "Trained batch 819 batch loss 1.27713954 epoch total loss 1.38773334\n",
      "Trained batch 820 batch loss 1.47636425 epoch total loss 1.38784134\n",
      "Trained batch 821 batch loss 1.6378653 epoch total loss 1.3881458\n",
      "Trained batch 822 batch loss 1.42683268 epoch total loss 1.38819289\n",
      "Trained batch 823 batch loss 1.41609728 epoch total loss 1.38822687\n",
      "Trained batch 824 batch loss 1.37759066 epoch total loss 1.38821399\n",
      "Trained batch 825 batch loss 1.4443121 epoch total loss 1.38828194\n",
      "Trained batch 826 batch loss 1.44413352 epoch total loss 1.38834953\n",
      "Trained batch 827 batch loss 1.27564502 epoch total loss 1.38821328\n",
      "Trained batch 828 batch loss 1.1794486 epoch total loss 1.38796115\n",
      "Trained batch 829 batch loss 1.18007934 epoch total loss 1.38771033\n",
      "Trained batch 830 batch loss 1.33675051 epoch total loss 1.38764894\n",
      "Trained batch 831 batch loss 1.30546629 epoch total loss 1.38755\n",
      "Trained batch 832 batch loss 1.32246244 epoch total loss 1.38747191\n",
      "Trained batch 833 batch loss 1.40515757 epoch total loss 1.38749313\n",
      "Trained batch 834 batch loss 1.26459455 epoch total loss 1.38734579\n",
      "Trained batch 835 batch loss 1.39131188 epoch total loss 1.38735056\n",
      "Trained batch 836 batch loss 1.36584425 epoch total loss 1.38732481\n",
      "Trained batch 837 batch loss 1.45390928 epoch total loss 1.38740432\n",
      "Trained batch 838 batch loss 1.5335288 epoch total loss 1.38757873\n",
      "Trained batch 839 batch loss 1.58102703 epoch total loss 1.3878094\n",
      "Trained batch 840 batch loss 1.5274148 epoch total loss 1.38797569\n",
      "Trained batch 841 batch loss 1.28090811 epoch total loss 1.38784826\n",
      "Trained batch 842 batch loss 1.29542255 epoch total loss 1.38773847\n",
      "Trained batch 843 batch loss 1.46292615 epoch total loss 1.38782763\n",
      "Trained batch 844 batch loss 1.49237299 epoch total loss 1.38795161\n",
      "Trained batch 845 batch loss 1.53865695 epoch total loss 1.38813\n",
      "Trained batch 846 batch loss 1.47412252 epoch total loss 1.38823164\n",
      "Trained batch 847 batch loss 1.42596185 epoch total loss 1.3882761\n",
      "Trained batch 848 batch loss 1.37711966 epoch total loss 1.38826287\n",
      "Trained batch 849 batch loss 1.3089 epoch total loss 1.38816953\n",
      "Trained batch 850 batch loss 1.42344189 epoch total loss 1.38821101\n",
      "Trained batch 851 batch loss 1.39907324 epoch total loss 1.38822377\n",
      "Trained batch 852 batch loss 1.40846789 epoch total loss 1.38824749\n",
      "Trained batch 853 batch loss 1.27784228 epoch total loss 1.38811803\n",
      "Trained batch 854 batch loss 1.29197598 epoch total loss 1.3880055\n",
      "Trained batch 855 batch loss 1.29560471 epoch total loss 1.38789749\n",
      "Trained batch 856 batch loss 1.38660276 epoch total loss 1.38789594\n",
      "Trained batch 857 batch loss 1.4471122 epoch total loss 1.38796508\n",
      "Trained batch 858 batch loss 1.4232049 epoch total loss 1.38800621\n",
      "Trained batch 859 batch loss 1.45196056 epoch total loss 1.3880806\n",
      "Trained batch 860 batch loss 1.42223096 epoch total loss 1.38812029\n",
      "Trained batch 861 batch loss 1.44925475 epoch total loss 1.38819122\n",
      "Trained batch 862 batch loss 1.42391837 epoch total loss 1.38823271\n",
      "Trained batch 863 batch loss 1.38049233 epoch total loss 1.38822377\n",
      "Trained batch 864 batch loss 1.35640919 epoch total loss 1.38818693\n",
      "Trained batch 865 batch loss 1.37286115 epoch total loss 1.38816917\n",
      "Trained batch 866 batch loss 1.26145327 epoch total loss 1.3880229\n",
      "Trained batch 867 batch loss 1.35436893 epoch total loss 1.38798404\n",
      "Trained batch 868 batch loss 1.35617936 epoch total loss 1.38794744\n",
      "Trained batch 869 batch loss 1.29700983 epoch total loss 1.38784277\n",
      "Trained batch 870 batch loss 1.57234108 epoch total loss 1.38805497\n",
      "Trained batch 871 batch loss 1.37469172 epoch total loss 1.38803947\n",
      "Trained batch 872 batch loss 1.29956865 epoch total loss 1.38793802\n",
      "Trained batch 873 batch loss 1.26359844 epoch total loss 1.38779557\n",
      "Trained batch 874 batch loss 1.28378558 epoch total loss 1.3876766\n",
      "Trained batch 875 batch loss 1.3201102 epoch total loss 1.38759935\n",
      "Trained batch 876 batch loss 1.43870032 epoch total loss 1.38765764\n",
      "Trained batch 877 batch loss 1.35587263 epoch total loss 1.3876214\n",
      "Trained batch 878 batch loss 1.36230421 epoch total loss 1.38759255\n",
      "Trained batch 879 batch loss 1.38814223 epoch total loss 1.38759327\n",
      "Trained batch 880 batch loss 1.33356643 epoch total loss 1.38753188\n",
      "Trained batch 881 batch loss 1.30890834 epoch total loss 1.38744271\n",
      "Trained batch 882 batch loss 1.44413757 epoch total loss 1.38750696\n",
      "Trained batch 883 batch loss 1.42090905 epoch total loss 1.38754475\n",
      "Trained batch 884 batch loss 1.49859846 epoch total loss 1.3876704\n",
      "Trained batch 885 batch loss 1.36460531 epoch total loss 1.38764441\n",
      "Trained batch 886 batch loss 1.30851936 epoch total loss 1.387555\n",
      "Trained batch 887 batch loss 1.33198464 epoch total loss 1.38749242\n",
      "Trained batch 888 batch loss 1.27054751 epoch total loss 1.38736069\n",
      "Trained batch 889 batch loss 1.46130311 epoch total loss 1.3874439\n",
      "Trained batch 890 batch loss 1.41228175 epoch total loss 1.3874718\n",
      "Trained batch 891 batch loss 1.40996599 epoch total loss 1.38749695\n",
      "Trained batch 892 batch loss 1.36101818 epoch total loss 1.38746715\n",
      "Trained batch 893 batch loss 1.37691534 epoch total loss 1.38745546\n",
      "Trained batch 894 batch loss 1.31319404 epoch total loss 1.38737237\n",
      "Trained batch 895 batch loss 1.44196534 epoch total loss 1.38743341\n",
      "Trained batch 896 batch loss 1.38768303 epoch total loss 1.38743377\n",
      "Trained batch 897 batch loss 1.29898047 epoch total loss 1.38733506\n",
      "Trained batch 898 batch loss 1.29439306 epoch total loss 1.38723159\n",
      "Trained batch 899 batch loss 1.39963245 epoch total loss 1.38724542\n",
      "Trained batch 900 batch loss 1.43071342 epoch total loss 1.3872937\n",
      "Trained batch 901 batch loss 1.46387208 epoch total loss 1.38737869\n",
      "Trained batch 902 batch loss 1.38290787 epoch total loss 1.38737381\n",
      "Trained batch 903 batch loss 1.32503295 epoch total loss 1.38730478\n",
      "Trained batch 904 batch loss 1.34239769 epoch total loss 1.38725507\n",
      "Trained batch 905 batch loss 1.33960772 epoch total loss 1.3872025\n",
      "Trained batch 906 batch loss 1.23960805 epoch total loss 1.38703954\n",
      "Trained batch 907 batch loss 1.21741271 epoch total loss 1.3868525\n",
      "Trained batch 908 batch loss 1.388304 epoch total loss 1.38685417\n",
      "Trained batch 909 batch loss 1.44678891 epoch total loss 1.38692009\n",
      "Trained batch 910 batch loss 1.36795509 epoch total loss 1.38689911\n",
      "Trained batch 911 batch loss 1.30396199 epoch total loss 1.38680816\n",
      "Trained batch 912 batch loss 1.32925773 epoch total loss 1.38674498\n",
      "Trained batch 913 batch loss 1.2486124 epoch total loss 1.3865937\n",
      "Trained batch 914 batch loss 1.18156755 epoch total loss 1.38636935\n",
      "Trained batch 915 batch loss 1.22521162 epoch total loss 1.38619328\n",
      "Trained batch 916 batch loss 1.24285448 epoch total loss 1.38603675\n",
      "Trained batch 917 batch loss 1.35934746 epoch total loss 1.38600767\n",
      "Trained batch 918 batch loss 1.26979399 epoch total loss 1.38588107\n",
      "Trained batch 919 batch loss 1.3175559 epoch total loss 1.38580656\n",
      "Trained batch 920 batch loss 1.4591769 epoch total loss 1.38588643\n",
      "Trained batch 921 batch loss 1.25121486 epoch total loss 1.38574016\n",
      "Trained batch 922 batch loss 1.35006821 epoch total loss 1.38570154\n",
      "Trained batch 923 batch loss 1.32987869 epoch total loss 1.38564098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 924 batch loss 1.30687547 epoch total loss 1.38555574\n",
      "Trained batch 925 batch loss 1.28816032 epoch total loss 1.38545048\n",
      "Trained batch 926 batch loss 1.17850626 epoch total loss 1.38522696\n",
      "Trained batch 927 batch loss 0.984409034 epoch total loss 1.38479459\n",
      "Trained batch 928 batch loss 1.14100397 epoch total loss 1.38453186\n",
      "Trained batch 929 batch loss 1.36672187 epoch total loss 1.38451266\n",
      "Trained batch 930 batch loss 1.45225728 epoch total loss 1.3845855\n",
      "Trained batch 931 batch loss 1.55635214 epoch total loss 1.38477\n",
      "Trained batch 932 batch loss 1.44924164 epoch total loss 1.38483918\n",
      "Trained batch 933 batch loss 1.41192043 epoch total loss 1.38486814\n",
      "Trained batch 934 batch loss 1.47586811 epoch total loss 1.38496554\n",
      "Trained batch 935 batch loss 1.28508544 epoch total loss 1.38485873\n",
      "Trained batch 936 batch loss 1.35146177 epoch total loss 1.38482296\n",
      "Trained batch 937 batch loss 1.33959544 epoch total loss 1.38477468\n",
      "Trained batch 938 batch loss 1.23241854 epoch total loss 1.38461232\n",
      "Trained batch 939 batch loss 1.21299398 epoch total loss 1.38442957\n",
      "Trained batch 940 batch loss 1.27168894 epoch total loss 1.38430965\n",
      "Trained batch 941 batch loss 1.23094928 epoch total loss 1.38414669\n",
      "Trained batch 942 batch loss 1.34730542 epoch total loss 1.38410759\n",
      "Trained batch 943 batch loss 1.26787615 epoch total loss 1.38398433\n",
      "Trained batch 944 batch loss 1.23961329 epoch total loss 1.38383138\n",
      "Trained batch 945 batch loss 1.25228953 epoch total loss 1.38369215\n",
      "Trained batch 946 batch loss 1.36380351 epoch total loss 1.38367116\n",
      "Trained batch 947 batch loss 1.43827617 epoch total loss 1.38372874\n",
      "Trained batch 948 batch loss 1.28404331 epoch total loss 1.3836236\n",
      "Trained batch 949 batch loss 1.31893313 epoch total loss 1.38355541\n",
      "Trained batch 950 batch loss 1.42195499 epoch total loss 1.38359594\n",
      "Trained batch 951 batch loss 1.33993948 epoch total loss 1.38355\n",
      "Trained batch 952 batch loss 1.28962374 epoch total loss 1.38345146\n",
      "Trained batch 953 batch loss 1.36842215 epoch total loss 1.38343561\n",
      "Trained batch 954 batch loss 1.41169596 epoch total loss 1.38346529\n",
      "Trained batch 955 batch loss 1.32757235 epoch total loss 1.38340676\n",
      "Trained batch 956 batch loss 1.25448942 epoch total loss 1.38327193\n",
      "Trained batch 957 batch loss 1.27457464 epoch total loss 1.38315833\n",
      "Trained batch 958 batch loss 1.28230286 epoch total loss 1.38305306\n",
      "Trained batch 959 batch loss 1.31925941 epoch total loss 1.38298655\n",
      "Trained batch 960 batch loss 1.35392702 epoch total loss 1.38295615\n",
      "Trained batch 961 batch loss 1.32557833 epoch total loss 1.38289642\n",
      "Trained batch 962 batch loss 1.35408092 epoch total loss 1.3828665\n",
      "Trained batch 963 batch loss 1.44834888 epoch total loss 1.38293457\n",
      "Trained batch 964 batch loss 1.34812713 epoch total loss 1.38289845\n",
      "Trained batch 965 batch loss 1.48155785 epoch total loss 1.38300073\n",
      "Trained batch 966 batch loss 1.34944558 epoch total loss 1.38296604\n",
      "Trained batch 967 batch loss 1.21743631 epoch total loss 1.38279486\n",
      "Trained batch 968 batch loss 1.29762363 epoch total loss 1.38270688\n",
      "Trained batch 969 batch loss 1.33853233 epoch total loss 1.38266122\n",
      "Trained batch 970 batch loss 1.31067109 epoch total loss 1.38258696\n",
      "Trained batch 971 batch loss 1.41694903 epoch total loss 1.38262236\n",
      "Trained batch 972 batch loss 1.42690241 epoch total loss 1.3826679\n",
      "Trained batch 973 batch loss 1.43849051 epoch total loss 1.38272536\n",
      "Trained batch 974 batch loss 1.30326092 epoch total loss 1.3826437\n",
      "Trained batch 975 batch loss 1.34484041 epoch total loss 1.38260496\n",
      "Trained batch 976 batch loss 1.34767246 epoch total loss 1.38256907\n",
      "Trained batch 977 batch loss 1.43668127 epoch total loss 1.38262451\n",
      "Trained batch 978 batch loss 1.33238328 epoch total loss 1.38257313\n",
      "Trained batch 979 batch loss 1.30674636 epoch total loss 1.38249564\n",
      "Trained batch 980 batch loss 1.24937963 epoch total loss 1.38235986\n",
      "Trained batch 981 batch loss 1.29448462 epoch total loss 1.38227022\n",
      "Trained batch 982 batch loss 1.31908751 epoch total loss 1.38220584\n",
      "Trained batch 983 batch loss 1.34943771 epoch total loss 1.38217258\n",
      "Trained batch 984 batch loss 1.38285553 epoch total loss 1.3821733\n",
      "Trained batch 985 batch loss 1.32909477 epoch total loss 1.38211942\n",
      "Trained batch 986 batch loss 1.36577475 epoch total loss 1.38210273\n",
      "Trained batch 987 batch loss 1.33299053 epoch total loss 1.38205302\n",
      "Trained batch 988 batch loss 1.25327921 epoch total loss 1.38192272\n",
      "Trained batch 989 batch loss 1.34930015 epoch total loss 1.38188958\n",
      "Trained batch 990 batch loss 1.32173073 epoch total loss 1.3818289\n",
      "Trained batch 991 batch loss 1.24005556 epoch total loss 1.38168585\n",
      "Trained batch 992 batch loss 1.33633816 epoch total loss 1.3816402\n",
      "Trained batch 993 batch loss 1.29110765 epoch total loss 1.381549\n",
      "Trained batch 994 batch loss 1.3309921 epoch total loss 1.3814981\n",
      "Trained batch 995 batch loss 1.33409846 epoch total loss 1.38145041\n",
      "Trained batch 996 batch loss 1.24992943 epoch total loss 1.38131833\n",
      "Trained batch 997 batch loss 1.20540071 epoch total loss 1.38114202\n",
      "Trained batch 998 batch loss 1.21702087 epoch total loss 1.38097751\n",
      "Trained batch 999 batch loss 1.40506542 epoch total loss 1.38100159\n",
      "Trained batch 1000 batch loss 1.28890979 epoch total loss 1.38090956\n",
      "Trained batch 1001 batch loss 1.40114665 epoch total loss 1.38092971\n",
      "Trained batch 1002 batch loss 1.37986648 epoch total loss 1.38092864\n",
      "Trained batch 1003 batch loss 1.41261101 epoch total loss 1.38096023\n",
      "Trained batch 1004 batch loss 1.30439615 epoch total loss 1.38088405\n",
      "Trained batch 1005 batch loss 1.29658818 epoch total loss 1.38080025\n",
      "Trained batch 1006 batch loss 1.1860981 epoch total loss 1.38060677\n",
      "Trained batch 1007 batch loss 1.24183393 epoch total loss 1.38046896\n",
      "Trained batch 1008 batch loss 1.2302053 epoch total loss 1.38031983\n",
      "Trained batch 1009 batch loss 1.2886529 epoch total loss 1.38022912\n",
      "Trained batch 1010 batch loss 1.27967811 epoch total loss 1.38012946\n",
      "Trained batch 1011 batch loss 1.33764148 epoch total loss 1.38008749\n",
      "Trained batch 1012 batch loss 1.1768043 epoch total loss 1.37988651\n",
      "Trained batch 1013 batch loss 1.31048036 epoch total loss 1.37981796\n",
      "Trained batch 1014 batch loss 1.25867391 epoch total loss 1.37969851\n",
      "Trained batch 1015 batch loss 1.23890805 epoch total loss 1.37955976\n",
      "Trained batch 1016 batch loss 1.44456077 epoch total loss 1.37962377\n",
      "Trained batch 1017 batch loss 1.39180088 epoch total loss 1.37963581\n",
      "Trained batch 1018 batch loss 1.32242727 epoch total loss 1.37957954\n",
      "Trained batch 1019 batch loss 1.3641994 epoch total loss 1.37956452\n",
      "Trained batch 1020 batch loss 1.31664717 epoch total loss 1.37950289\n",
      "Trained batch 1021 batch loss 1.32131696 epoch total loss 1.37944579\n",
      "Trained batch 1022 batch loss 1.22873 epoch total loss 1.37929833\n",
      "Trained batch 1023 batch loss 1.31467474 epoch total loss 1.37923527\n",
      "Trained batch 1024 batch loss 1.26816535 epoch total loss 1.37912679\n",
      "Trained batch 1025 batch loss 1.3699069 epoch total loss 1.37911773\n",
      "Trained batch 1026 batch loss 1.51973808 epoch total loss 1.37925482\n",
      "Trained batch 1027 batch loss 1.39597678 epoch total loss 1.37927115\n",
      "Trained batch 1028 batch loss 1.2251339 epoch total loss 1.37912118\n",
      "Trained batch 1029 batch loss 1.3336246 epoch total loss 1.37907696\n",
      "Trained batch 1030 batch loss 1.29757762 epoch total loss 1.3789978\n",
      "Trained batch 1031 batch loss 1.34411204 epoch total loss 1.37896407\n",
      "Trained batch 1032 batch loss 1.25224078 epoch total loss 1.37884116\n",
      "Trained batch 1033 batch loss 1.29518104 epoch total loss 1.37876022\n",
      "Trained batch 1034 batch loss 1.35222232 epoch total loss 1.37873447\n",
      "Trained batch 1035 batch loss 1.35126197 epoch total loss 1.378708\n",
      "Trained batch 1036 batch loss 1.39201868 epoch total loss 1.37872076\n",
      "Trained batch 1037 batch loss 1.34784508 epoch total loss 1.37869108\n",
      "Trained batch 1038 batch loss 1.43937838 epoch total loss 1.37874949\n",
      "Trained batch 1039 batch loss 1.4547509 epoch total loss 1.37882257\n",
      "Trained batch 1040 batch loss 1.38288152 epoch total loss 1.3788265\n",
      "Trained batch 1041 batch loss 1.25265133 epoch total loss 1.37870538\n",
      "Trained batch 1042 batch loss 1.33307946 epoch total loss 1.37866163\n",
      "Trained batch 1043 batch loss 1.46418166 epoch total loss 1.37874365\n",
      "Trained batch 1044 batch loss 1.47018206 epoch total loss 1.37883127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1045 batch loss 1.47071838 epoch total loss 1.37891924\n",
      "Trained batch 1046 batch loss 1.42243195 epoch total loss 1.37896085\n",
      "Trained batch 1047 batch loss 1.37164974 epoch total loss 1.37895393\n",
      "Trained batch 1048 batch loss 1.40144849 epoch total loss 1.37897539\n",
      "Trained batch 1049 batch loss 1.24443805 epoch total loss 1.37884712\n",
      "Trained batch 1050 batch loss 1.33321238 epoch total loss 1.37880373\n",
      "Trained batch 1051 batch loss 1.431301 epoch total loss 1.37885368\n",
      "Trained batch 1052 batch loss 1.37163568 epoch total loss 1.37884676\n",
      "Trained batch 1053 batch loss 1.22460639 epoch total loss 1.37870026\n",
      "Trained batch 1054 batch loss 1.23499382 epoch total loss 1.37856388\n",
      "Trained batch 1055 batch loss 1.24764287 epoch total loss 1.37843978\n",
      "Trained batch 1056 batch loss 1.30911183 epoch total loss 1.3783741\n",
      "Trained batch 1057 batch loss 1.53320301 epoch total loss 1.37852061\n",
      "Trained batch 1058 batch loss 1.46587431 epoch total loss 1.3786031\n",
      "Trained batch 1059 batch loss 1.63975549 epoch total loss 1.37884974\n",
      "Trained batch 1060 batch loss 1.46599352 epoch total loss 1.37893188\n",
      "Trained batch 1061 batch loss 1.46579778 epoch total loss 1.37901378\n",
      "Trained batch 1062 batch loss 1.5372901 epoch total loss 1.37916279\n",
      "Trained batch 1063 batch loss 1.44577813 epoch total loss 1.37922549\n",
      "Trained batch 1064 batch loss 1.41607451 epoch total loss 1.37926006\n",
      "Trained batch 1065 batch loss 1.42174625 epoch total loss 1.3793\n",
      "Trained batch 1066 batch loss 1.31757808 epoch total loss 1.37924206\n",
      "Trained batch 1067 batch loss 1.32107973 epoch total loss 1.37918758\n",
      "Trained batch 1068 batch loss 1.31378651 epoch total loss 1.37912643\n",
      "Trained batch 1069 batch loss 1.36159897 epoch total loss 1.37911\n",
      "Trained batch 1070 batch loss 1.30994403 epoch total loss 1.37904537\n",
      "Trained batch 1071 batch loss 1.34980237 epoch total loss 1.37901807\n",
      "Trained batch 1072 batch loss 1.42936695 epoch total loss 1.37906504\n",
      "Trained batch 1073 batch loss 1.40294743 epoch total loss 1.37908721\n",
      "Trained batch 1074 batch loss 1.3661902 epoch total loss 1.37907529\n",
      "Trained batch 1075 batch loss 1.40721166 epoch total loss 1.3791014\n",
      "Trained batch 1076 batch loss 1.40564513 epoch total loss 1.37912607\n",
      "Trained batch 1077 batch loss 1.37601399 epoch total loss 1.37912321\n",
      "Trained batch 1078 batch loss 1.47523534 epoch total loss 1.37921238\n",
      "Trained batch 1079 batch loss 1.38009715 epoch total loss 1.37921321\n",
      "Trained batch 1080 batch loss 1.38842177 epoch total loss 1.37922168\n",
      "Trained batch 1081 batch loss 1.30110943 epoch total loss 1.37914944\n",
      "Trained batch 1082 batch loss 1.31852818 epoch total loss 1.37909341\n",
      "Trained batch 1083 batch loss 1.2360568 epoch total loss 1.37896132\n",
      "Trained batch 1084 batch loss 1.23967123 epoch total loss 1.37883282\n",
      "Trained batch 1085 batch loss 1.19216466 epoch total loss 1.3786608\n",
      "Trained batch 1086 batch loss 1.35104632 epoch total loss 1.37863541\n",
      "Trained batch 1087 batch loss 1.48811865 epoch total loss 1.37873614\n",
      "Trained batch 1088 batch loss 1.36119187 epoch total loss 1.37872\n",
      "Trained batch 1089 batch loss 1.39726639 epoch total loss 1.37873697\n",
      "Trained batch 1090 batch loss 1.42250061 epoch total loss 1.37877715\n",
      "Trained batch 1091 batch loss 1.33813286 epoch total loss 1.37873983\n",
      "Trained batch 1092 batch loss 1.31617689 epoch total loss 1.37868261\n",
      "Trained batch 1093 batch loss 1.41415215 epoch total loss 1.37871504\n",
      "Trained batch 1094 batch loss 1.33619809 epoch total loss 1.37867618\n",
      "Trained batch 1095 batch loss 1.30762362 epoch total loss 1.37861133\n",
      "Trained batch 1096 batch loss 1.33141756 epoch total loss 1.37856817\n",
      "Trained batch 1097 batch loss 1.33447671 epoch total loss 1.378528\n",
      "Trained batch 1098 batch loss 1.33515728 epoch total loss 1.37848854\n",
      "Trained batch 1099 batch loss 1.20543885 epoch total loss 1.37833107\n",
      "Trained batch 1100 batch loss 1.20429826 epoch total loss 1.37817299\n",
      "Trained batch 1101 batch loss 1.11985302 epoch total loss 1.37793839\n",
      "Trained batch 1102 batch loss 1.2661432 epoch total loss 1.37783682\n",
      "Trained batch 1103 batch loss 1.40611577 epoch total loss 1.37786245\n",
      "Trained batch 1104 batch loss 1.35528052 epoch total loss 1.37784195\n",
      "Trained batch 1105 batch loss 1.51451361 epoch total loss 1.37796569\n",
      "Trained batch 1106 batch loss 1.44817269 epoch total loss 1.37802911\n",
      "Trained batch 1107 batch loss 1.42145634 epoch total loss 1.37806845\n",
      "Trained batch 1108 batch loss 1.28302348 epoch total loss 1.37798274\n",
      "Trained batch 1109 batch loss 1.18399704 epoch total loss 1.37780774\n",
      "Trained batch 1110 batch loss 1.16615224 epoch total loss 1.377617\n",
      "Trained batch 1111 batch loss 1.16652155 epoch total loss 1.37742698\n",
      "Trained batch 1112 batch loss 1.25231647 epoch total loss 1.37731445\n",
      "Trained batch 1113 batch loss 1.47894645 epoch total loss 1.37740588\n",
      "Trained batch 1114 batch loss 1.44744837 epoch total loss 1.37746871\n",
      "Trained batch 1115 batch loss 1.47382259 epoch total loss 1.37755513\n",
      "Trained batch 1116 batch loss 1.3299613 epoch total loss 1.37751245\n",
      "Trained batch 1117 batch loss 1.27826118 epoch total loss 1.37742364\n",
      "Trained batch 1118 batch loss 1.3701719 epoch total loss 1.37741721\n",
      "Trained batch 1119 batch loss 1.32266724 epoch total loss 1.37736821\n",
      "Trained batch 1120 batch loss 1.46381462 epoch total loss 1.37744546\n",
      "Trained batch 1121 batch loss 1.47179747 epoch total loss 1.37752962\n",
      "Trained batch 1122 batch loss 1.47577786 epoch total loss 1.37761724\n",
      "Trained batch 1123 batch loss 1.51063144 epoch total loss 1.37773561\n",
      "Trained batch 1124 batch loss 1.42599618 epoch total loss 1.37777865\n",
      "Trained batch 1125 batch loss 1.37945807 epoch total loss 1.3777802\n",
      "Trained batch 1126 batch loss 1.30668807 epoch total loss 1.37771702\n",
      "Trained batch 1127 batch loss 1.22376454 epoch total loss 1.3775804\n",
      "Trained batch 1128 batch loss 1.27437854 epoch total loss 1.37748897\n",
      "Trained batch 1129 batch loss 1.23356605 epoch total loss 1.37736142\n",
      "Trained batch 1130 batch loss 1.25484216 epoch total loss 1.37725306\n",
      "Trained batch 1131 batch loss 1.28353143 epoch total loss 1.37717021\n",
      "Trained batch 1132 batch loss 1.21188629 epoch total loss 1.37702417\n",
      "Trained batch 1133 batch loss 1.26579332 epoch total loss 1.37692595\n",
      "Trained batch 1134 batch loss 1.39764941 epoch total loss 1.3769443\n",
      "Trained batch 1135 batch loss 1.43331158 epoch total loss 1.37699401\n",
      "Trained batch 1136 batch loss 1.39570606 epoch total loss 1.37701046\n",
      "Trained batch 1137 batch loss 1.44973874 epoch total loss 1.37707448\n",
      "Trained batch 1138 batch loss 1.40426803 epoch total loss 1.37709832\n",
      "Trained batch 1139 batch loss 1.23764014 epoch total loss 1.37697589\n",
      "Trained batch 1140 batch loss 1.25767899 epoch total loss 1.37687135\n",
      "Trained batch 1141 batch loss 1.18316901 epoch total loss 1.37670159\n",
      "Trained batch 1142 batch loss 1.26330686 epoch total loss 1.37660229\n",
      "Trained batch 1143 batch loss 1.44431531 epoch total loss 1.37666154\n",
      "Trained batch 1144 batch loss 1.4536432 epoch total loss 1.37672877\n",
      "Trained batch 1145 batch loss 1.30760276 epoch total loss 1.37666845\n",
      "Trained batch 1146 batch loss 1.3363148 epoch total loss 1.37663329\n",
      "Trained batch 1147 batch loss 1.27756643 epoch total loss 1.37654686\n",
      "Trained batch 1148 batch loss 1.36578858 epoch total loss 1.37653756\n",
      "Trained batch 1149 batch loss 1.18943119 epoch total loss 1.37637472\n",
      "Trained batch 1150 batch loss 1.20016801 epoch total loss 1.37622154\n",
      "Trained batch 1151 batch loss 1.38695419 epoch total loss 1.37623084\n",
      "Trained batch 1152 batch loss 1.31187105 epoch total loss 1.37617505\n",
      "Trained batch 1153 batch loss 1.16911817 epoch total loss 1.3759954\n",
      "Trained batch 1154 batch loss 1.32992196 epoch total loss 1.37595546\n",
      "Trained batch 1155 batch loss 1.31246614 epoch total loss 1.37590063\n",
      "Trained batch 1156 batch loss 1.24889553 epoch total loss 1.37579072\n",
      "Trained batch 1157 batch loss 1.25078166 epoch total loss 1.37568259\n",
      "Trained batch 1158 batch loss 1.22826743 epoch total loss 1.37555528\n",
      "Trained batch 1159 batch loss 1.22145772 epoch total loss 1.37542236\n",
      "Trained batch 1160 batch loss 1.30893624 epoch total loss 1.37536502\n",
      "Trained batch 1161 batch loss 1.29590535 epoch total loss 1.37529659\n",
      "Trained batch 1162 batch loss 1.19610119 epoch total loss 1.37514234\n",
      "Trained batch 1163 batch loss 1.37904012 epoch total loss 1.37514567\n",
      "Trained batch 1164 batch loss 1.39660811 epoch total loss 1.37516415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1165 batch loss 1.42862022 epoch total loss 1.37520993\n",
      "Trained batch 1166 batch loss 1.43225288 epoch total loss 1.37525892\n",
      "Trained batch 1167 batch loss 1.32334459 epoch total loss 1.37521446\n",
      "Trained batch 1168 batch loss 1.41502237 epoch total loss 1.37524855\n",
      "Trained batch 1169 batch loss 1.34419394 epoch total loss 1.37522197\n",
      "Trained batch 1170 batch loss 1.37194037 epoch total loss 1.37521923\n",
      "Trained batch 1171 batch loss 1.22335148 epoch total loss 1.37508953\n",
      "Trained batch 1172 batch loss 1.3418231 epoch total loss 1.37506115\n",
      "Trained batch 1173 batch loss 1.33173597 epoch total loss 1.3750242\n",
      "Trained batch 1174 batch loss 1.34238076 epoch total loss 1.37499642\n",
      "Trained batch 1175 batch loss 1.26291382 epoch total loss 1.37490106\n",
      "Trained batch 1176 batch loss 1.27274835 epoch total loss 1.37481415\n",
      "Trained batch 1177 batch loss 1.34491456 epoch total loss 1.37478888\n",
      "Trained batch 1178 batch loss 1.29090643 epoch total loss 1.37471759\n",
      "Trained batch 1179 batch loss 1.33253717 epoch total loss 1.37468183\n",
      "Trained batch 1180 batch loss 1.3710773 epoch total loss 1.37467873\n",
      "Trained batch 1181 batch loss 1.37904 epoch total loss 1.37468243\n",
      "Trained batch 1182 batch loss 1.43490601 epoch total loss 1.37473345\n",
      "Trained batch 1183 batch loss 1.40775561 epoch total loss 1.37476134\n",
      "Trained batch 1184 batch loss 1.40556288 epoch total loss 1.37478733\n",
      "Trained batch 1185 batch loss 1.4371196 epoch total loss 1.3748399\n",
      "Trained batch 1186 batch loss 1.49095321 epoch total loss 1.37493789\n",
      "Trained batch 1187 batch loss 1.23767138 epoch total loss 1.37482214\n",
      "Trained batch 1188 batch loss 1.27612567 epoch total loss 1.37473917\n",
      "Trained batch 1189 batch loss 1.29413736 epoch total loss 1.37467134\n",
      "Trained batch 1190 batch loss 1.21650028 epoch total loss 1.37453854\n",
      "Trained batch 1191 batch loss 1.27701581 epoch total loss 1.37445652\n",
      "Trained batch 1192 batch loss 1.25563216 epoch total loss 1.37435687\n",
      "Trained batch 1193 batch loss 1.32045317 epoch total loss 1.37431169\n",
      "Trained batch 1194 batch loss 1.29842687 epoch total loss 1.37424815\n",
      "Trained batch 1195 batch loss 1.17005837 epoch total loss 1.37407732\n",
      "Trained batch 1196 batch loss 1.26237869 epoch total loss 1.37398386\n",
      "Trained batch 1197 batch loss 1.36382592 epoch total loss 1.37397528\n",
      "Trained batch 1198 batch loss 1.36730123 epoch total loss 1.37396979\n",
      "Trained batch 1199 batch loss 1.44011569 epoch total loss 1.37402487\n",
      "Trained batch 1200 batch loss 1.30729198 epoch total loss 1.3739692\n",
      "Trained batch 1201 batch loss 1.36521339 epoch total loss 1.37396193\n",
      "Trained batch 1202 batch loss 1.34773362 epoch total loss 1.37394011\n",
      "Trained batch 1203 batch loss 1.42949986 epoch total loss 1.37398624\n",
      "Trained batch 1204 batch loss 1.43229795 epoch total loss 1.37403464\n",
      "Trained batch 1205 batch loss 1.34166622 epoch total loss 1.37400782\n",
      "Trained batch 1206 batch loss 1.3142333 epoch total loss 1.37395823\n",
      "Trained batch 1207 batch loss 1.31016743 epoch total loss 1.37390542\n",
      "Trained batch 1208 batch loss 1.2594943 epoch total loss 1.37381077\n",
      "Trained batch 1209 batch loss 1.36175823 epoch total loss 1.37380075\n",
      "Trained batch 1210 batch loss 1.2162466 epoch total loss 1.37367058\n",
      "Trained batch 1211 batch loss 1.24521959 epoch total loss 1.37356448\n",
      "Trained batch 1212 batch loss 1.26214075 epoch total loss 1.37347245\n",
      "Trained batch 1213 batch loss 1.19899416 epoch total loss 1.37332869\n",
      "Trained batch 1214 batch loss 1.32903242 epoch total loss 1.37329209\n",
      "Trained batch 1215 batch loss 1.29867542 epoch total loss 1.3732307\n",
      "Trained batch 1216 batch loss 1.31089973 epoch total loss 1.37317944\n",
      "Trained batch 1217 batch loss 1.28501141 epoch total loss 1.37310708\n",
      "Trained batch 1218 batch loss 1.42799091 epoch total loss 1.37315214\n",
      "Trained batch 1219 batch loss 1.42478204 epoch total loss 1.37319446\n",
      "Trained batch 1220 batch loss 1.28457713 epoch total loss 1.37312186\n",
      "Trained batch 1221 batch loss 1.16320252 epoch total loss 1.37295\n",
      "Trained batch 1222 batch loss 1.22214663 epoch total loss 1.37282646\n",
      "Trained batch 1223 batch loss 1.26744699 epoch total loss 1.37274039\n",
      "Trained batch 1224 batch loss 1.33905387 epoch total loss 1.37271285\n",
      "Trained batch 1225 batch loss 1.3082968 epoch total loss 1.37266028\n",
      "Trained batch 1226 batch loss 1.33057165 epoch total loss 1.37262595\n",
      "Trained batch 1227 batch loss 1.22237754 epoch total loss 1.37250352\n",
      "Trained batch 1228 batch loss 1.21309018 epoch total loss 1.37237382\n",
      "Trained batch 1229 batch loss 1.33277178 epoch total loss 1.37234151\n",
      "Trained batch 1230 batch loss 1.45453489 epoch total loss 1.37240839\n",
      "Trained batch 1231 batch loss 1.38503325 epoch total loss 1.37241864\n",
      "Trained batch 1232 batch loss 1.34165418 epoch total loss 1.37239373\n",
      "Trained batch 1233 batch loss 1.17075312 epoch total loss 1.37223017\n",
      "Trained batch 1234 batch loss 1.16459608 epoch total loss 1.37206185\n",
      "Trained batch 1235 batch loss 1.10700512 epoch total loss 1.37184727\n",
      "Trained batch 1236 batch loss 1.1694026 epoch total loss 1.3716836\n",
      "Trained batch 1237 batch loss 1.31653845 epoch total loss 1.37163901\n",
      "Trained batch 1238 batch loss 1.27250981 epoch total loss 1.3715589\n",
      "Trained batch 1239 batch loss 1.36455631 epoch total loss 1.37155318\n",
      "Trained batch 1240 batch loss 1.38079262 epoch total loss 1.37156057\n",
      "Trained batch 1241 batch loss 1.41369212 epoch total loss 1.37159455\n",
      "Trained batch 1242 batch loss 1.41768265 epoch total loss 1.37163162\n",
      "Trained batch 1243 batch loss 1.36021638 epoch total loss 1.37162244\n",
      "Trained batch 1244 batch loss 1.40218234 epoch total loss 1.37164712\n",
      "Trained batch 1245 batch loss 1.30971158 epoch total loss 1.37159729\n",
      "Trained batch 1246 batch loss 1.41229773 epoch total loss 1.37163007\n",
      "Trained batch 1247 batch loss 1.46261799 epoch total loss 1.37170303\n",
      "Trained batch 1248 batch loss 1.38994622 epoch total loss 1.37171757\n",
      "Trained batch 1249 batch loss 1.40124035 epoch total loss 1.37174129\n",
      "Trained batch 1250 batch loss 1.34289956 epoch total loss 1.37171817\n",
      "Trained batch 1251 batch loss 1.3340441 epoch total loss 1.37168801\n",
      "Trained batch 1252 batch loss 1.32741141 epoch total loss 1.3716526\n",
      "Trained batch 1253 batch loss 1.33885503 epoch total loss 1.3716265\n",
      "Trained batch 1254 batch loss 1.33749938 epoch total loss 1.37159932\n",
      "Trained batch 1255 batch loss 1.29337263 epoch total loss 1.37153685\n",
      "Trained batch 1256 batch loss 1.33824706 epoch total loss 1.37151039\n",
      "Trained batch 1257 batch loss 1.23739934 epoch total loss 1.37140369\n",
      "Trained batch 1258 batch loss 1.17514527 epoch total loss 1.37124777\n",
      "Trained batch 1259 batch loss 1.0918113 epoch total loss 1.3710258\n",
      "Trained batch 1260 batch loss 1.10362601 epoch total loss 1.37081361\n",
      "Trained batch 1261 batch loss 1.42673063 epoch total loss 1.37085795\n",
      "Trained batch 1262 batch loss 1.37777579 epoch total loss 1.37086344\n",
      "Trained batch 1263 batch loss 1.42160118 epoch total loss 1.37090361\n",
      "Trained batch 1264 batch loss 1.44236541 epoch total loss 1.37096024\n",
      "Trained batch 1265 batch loss 1.41009915 epoch total loss 1.37099123\n",
      "Trained batch 1266 batch loss 1.41496968 epoch total loss 1.37102592\n",
      "Trained batch 1267 batch loss 1.39970386 epoch total loss 1.37104845\n",
      "Trained batch 1268 batch loss 1.41147351 epoch total loss 1.3710804\n",
      "Trained batch 1269 batch loss 1.40959549 epoch total loss 1.37111068\n",
      "Trained batch 1270 batch loss 1.2874229 epoch total loss 1.37104487\n",
      "Trained batch 1271 batch loss 1.34234691 epoch total loss 1.37102234\n",
      "Trained batch 1272 batch loss 1.24824417 epoch total loss 1.37092578\n",
      "Trained batch 1273 batch loss 1.20934415 epoch total loss 1.37079883\n",
      "Trained batch 1274 batch loss 1.35373712 epoch total loss 1.37078547\n",
      "Trained batch 1275 batch loss 1.29829717 epoch total loss 1.37072873\n",
      "Trained batch 1276 batch loss 1.29988861 epoch total loss 1.37067318\n",
      "Trained batch 1277 batch loss 1.48128939 epoch total loss 1.37075984\n",
      "Trained batch 1278 batch loss 1.41747153 epoch total loss 1.37079644\n",
      "Trained batch 1279 batch loss 1.25647116 epoch total loss 1.37070704\n",
      "Trained batch 1280 batch loss 1.33835793 epoch total loss 1.37068176\n",
      "Trained batch 1281 batch loss 1.33963656 epoch total loss 1.37065744\n",
      "Trained batch 1282 batch loss 1.25795901 epoch total loss 1.37056959\n",
      "Trained batch 1283 batch loss 1.30697751 epoch total loss 1.37052\n",
      "Trained batch 1284 batch loss 1.29107821 epoch total loss 1.37045825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1285 batch loss 1.31857443 epoch total loss 1.37041783\n",
      "Trained batch 1286 batch loss 1.62684941 epoch total loss 1.37061727\n",
      "Trained batch 1287 batch loss 1.45377183 epoch total loss 1.37068176\n",
      "Trained batch 1288 batch loss 1.53724861 epoch total loss 1.3708111\n",
      "Trained batch 1289 batch loss 1.4446559 epoch total loss 1.37086844\n",
      "Trained batch 1290 batch loss 1.43578792 epoch total loss 1.37091875\n",
      "Trained batch 1291 batch loss 1.46163976 epoch total loss 1.37098908\n",
      "Trained batch 1292 batch loss 1.31762695 epoch total loss 1.37094772\n",
      "Trained batch 1293 batch loss 1.32579696 epoch total loss 1.37091291\n",
      "Trained batch 1294 batch loss 1.36070621 epoch total loss 1.37090504\n",
      "Trained batch 1295 batch loss 1.33826709 epoch total loss 1.37087977\n",
      "Trained batch 1296 batch loss 1.39490438 epoch total loss 1.37089825\n",
      "Trained batch 1297 batch loss 1.36562276 epoch total loss 1.37089419\n",
      "Trained batch 1298 batch loss 1.37945437 epoch total loss 1.37090075\n",
      "Trained batch 1299 batch loss 1.26256394 epoch total loss 1.37081742\n",
      "Trained batch 1300 batch loss 1.30385816 epoch total loss 1.37076581\n",
      "Trained batch 1301 batch loss 1.29670954 epoch total loss 1.37070894\n",
      "Trained batch 1302 batch loss 1.4339447 epoch total loss 1.37075758\n",
      "Trained batch 1303 batch loss 1.43247819 epoch total loss 1.37080491\n",
      "Trained batch 1304 batch loss 1.40027118 epoch total loss 1.37082756\n",
      "Trained batch 1305 batch loss 1.36763334 epoch total loss 1.37082505\n",
      "Trained batch 1306 batch loss 1.39672756 epoch total loss 1.37084496\n",
      "Trained batch 1307 batch loss 1.37484622 epoch total loss 1.37084806\n",
      "Trained batch 1308 batch loss 1.34460044 epoch total loss 1.37082791\n",
      "Trained batch 1309 batch loss 1.23746872 epoch total loss 1.37072599\n",
      "Trained batch 1310 batch loss 1.24127555 epoch total loss 1.37062728\n",
      "Trained batch 1311 batch loss 1.15984571 epoch total loss 1.37046647\n",
      "Trained batch 1312 batch loss 1.11124408 epoch total loss 1.37026882\n",
      "Trained batch 1313 batch loss 1.2453841 epoch total loss 1.37017369\n",
      "Trained batch 1314 batch loss 1.17235529 epoch total loss 1.37002313\n",
      "Trained batch 1315 batch loss 1.08093631 epoch total loss 1.36980331\n",
      "Trained batch 1316 batch loss 1.04397988 epoch total loss 1.36955571\n",
      "Trained batch 1317 batch loss 0.978895545 epoch total loss 1.36925912\n",
      "Trained batch 1318 batch loss 1.32701659 epoch total loss 1.36922705\n",
      "Trained batch 1319 batch loss 1.3057313 epoch total loss 1.36917889\n",
      "Trained batch 1320 batch loss 1.26037037 epoch total loss 1.36909652\n",
      "Trained batch 1321 batch loss 1.28088343 epoch total loss 1.36902976\n",
      "Trained batch 1322 batch loss 1.28676629 epoch total loss 1.36896753\n",
      "Trained batch 1323 batch loss 1.38921046 epoch total loss 1.36898279\n",
      "Trained batch 1324 batch loss 1.27973127 epoch total loss 1.36891544\n",
      "Trained batch 1325 batch loss 1.2626543 epoch total loss 1.36883521\n",
      "Trained batch 1326 batch loss 1.24289966 epoch total loss 1.3687402\n",
      "Trained batch 1327 batch loss 1.27889395 epoch total loss 1.36867261\n",
      "Trained batch 1328 batch loss 1.46184719 epoch total loss 1.3687427\n",
      "Trained batch 1329 batch loss 1.41523576 epoch total loss 1.36877775\n",
      "Trained batch 1330 batch loss 1.244807 epoch total loss 1.36868441\n",
      "Trained batch 1331 batch loss 1.34393275 epoch total loss 1.36866581\n",
      "Trained batch 1332 batch loss 1.16569865 epoch total loss 1.36851346\n",
      "Trained batch 1333 batch loss 1.17653024 epoch total loss 1.36836934\n",
      "Trained batch 1334 batch loss 1.25315046 epoch total loss 1.36828303\n",
      "Trained batch 1335 batch loss 1.35197818 epoch total loss 1.36827075\n",
      "Trained batch 1336 batch loss 1.38724852 epoch total loss 1.36828494\n",
      "Trained batch 1337 batch loss 1.36990678 epoch total loss 1.36828613\n",
      "Trained batch 1338 batch loss 1.35601211 epoch total loss 1.36827695\n",
      "Trained batch 1339 batch loss 1.37262142 epoch total loss 1.36828017\n",
      "Trained batch 1340 batch loss 1.24991703 epoch total loss 1.36819184\n",
      "Trained batch 1341 batch loss 1.27515936 epoch total loss 1.36812246\n",
      "Trained batch 1342 batch loss 1.36564159 epoch total loss 1.36812055\n",
      "Trained batch 1343 batch loss 1.25503492 epoch total loss 1.36803639\n",
      "Trained batch 1344 batch loss 1.40907741 epoch total loss 1.36806691\n",
      "Trained batch 1345 batch loss 1.44799387 epoch total loss 1.36812627\n",
      "Trained batch 1346 batch loss 1.44933736 epoch total loss 1.36818659\n",
      "Trained batch 1347 batch loss 1.28507853 epoch total loss 1.36812496\n",
      "Trained batch 1348 batch loss 1.34006977 epoch total loss 1.3681041\n",
      "Trained batch 1349 batch loss 1.21902502 epoch total loss 1.36799359\n",
      "Trained batch 1350 batch loss 1.31146324 epoch total loss 1.36795175\n",
      "Trained batch 1351 batch loss 1.34253895 epoch total loss 1.36793292\n",
      "Trained batch 1352 batch loss 1.21465135 epoch total loss 1.36781955\n",
      "Trained batch 1353 batch loss 1.15509987 epoch total loss 1.36766231\n",
      "Trained batch 1354 batch loss 1.14400542 epoch total loss 1.36749721\n",
      "Trained batch 1355 batch loss 1.17345834 epoch total loss 1.36735404\n",
      "Trained batch 1356 batch loss 1.23096168 epoch total loss 1.36725342\n",
      "Trained batch 1357 batch loss 1.1374886 epoch total loss 1.36708403\n",
      "Trained batch 1358 batch loss 1.2378273 epoch total loss 1.3669889\n",
      "Trained batch 1359 batch loss 1.29545856 epoch total loss 1.36693621\n",
      "Trained batch 1360 batch loss 1.34843421 epoch total loss 1.3669225\n",
      "Trained batch 1361 batch loss 1.2041961 epoch total loss 1.36680305\n",
      "Trained batch 1362 batch loss 1.21420813 epoch total loss 1.36669099\n",
      "Trained batch 1363 batch loss 1.22241235 epoch total loss 1.36658514\n",
      "Trained batch 1364 batch loss 1.39426327 epoch total loss 1.3666054\n",
      "Trained batch 1365 batch loss 1.398983 epoch total loss 1.36662912\n",
      "Trained batch 1366 batch loss 1.33182418 epoch total loss 1.36660361\n",
      "Trained batch 1367 batch loss 1.27637815 epoch total loss 1.36653757\n",
      "Trained batch 1368 batch loss 1.3244276 epoch total loss 1.36650681\n",
      "Trained batch 1369 batch loss 1.33652568 epoch total loss 1.366485\n",
      "Trained batch 1370 batch loss 1.29862785 epoch total loss 1.36643541\n",
      "Trained batch 1371 batch loss 1.49502587 epoch total loss 1.36652911\n",
      "Trained batch 1372 batch loss 1.43845618 epoch total loss 1.36658156\n",
      "Trained batch 1373 batch loss 1.39161253 epoch total loss 1.3665998\n",
      "Trained batch 1374 batch loss 1.3222425 epoch total loss 1.36656761\n",
      "Trained batch 1375 batch loss 1.31560397 epoch total loss 1.36653042\n",
      "Trained batch 1376 batch loss 1.28333116 epoch total loss 1.36647\n",
      "Trained batch 1377 batch loss 1.34377122 epoch total loss 1.36645353\n",
      "Trained batch 1378 batch loss 1.45782387 epoch total loss 1.36651969\n",
      "Trained batch 1379 batch loss 1.48022008 epoch total loss 1.36660218\n",
      "Trained batch 1380 batch loss 1.63557231 epoch total loss 1.36679709\n",
      "Trained batch 1381 batch loss 1.39195859 epoch total loss 1.36681533\n",
      "Trained batch 1382 batch loss 1.34023464 epoch total loss 1.36679614\n",
      "Trained batch 1383 batch loss 1.42841184 epoch total loss 1.36684072\n",
      "Trained batch 1384 batch loss 1.45102143 epoch total loss 1.36690152\n",
      "Trained batch 1385 batch loss 1.47207642 epoch total loss 1.36697745\n",
      "Trained batch 1386 batch loss 1.40324831 epoch total loss 1.36700356\n",
      "Trained batch 1387 batch loss 1.2670486 epoch total loss 1.36693156\n",
      "Trained batch 1388 batch loss 1.29622638 epoch total loss 1.36688066\n",
      "Epoch 2 train loss 1.3668806552886963\n",
      "Validated batch 1 batch loss 1.2456727\n",
      "Validated batch 2 batch loss 1.36939597\n",
      "Validated batch 3 batch loss 1.39825344\n",
      "Validated batch 4 batch loss 1.29428184\n",
      "Validated batch 5 batch loss 1.42642438\n",
      "Validated batch 6 batch loss 1.44234872\n",
      "Validated batch 7 batch loss 1.22091413\n",
      "Validated batch 8 batch loss 1.35024023\n",
      "Validated batch 9 batch loss 1.31740975\n",
      "Validated batch 10 batch loss 1.41005719\n",
      "Validated batch 11 batch loss 1.34264421\n",
      "Validated batch 12 batch loss 1.17983663\n",
      "Validated batch 13 batch loss 1.22293282\n",
      "Validated batch 14 batch loss 1.35142624\n",
      "Validated batch 15 batch loss 1.33246553\n",
      "Validated batch 16 batch loss 1.30830419\n",
      "Validated batch 17 batch loss 1.30405295\n",
      "Validated batch 18 batch loss 1.3149296\n",
      "Validated batch 19 batch loss 1.35493267\n",
      "Validated batch 20 batch loss 1.41845429\n",
      "Validated batch 21 batch loss 1.38437128\n",
      "Validated batch 22 batch loss 1.37035036\n",
      "Validated batch 23 batch loss 1.2410816\n",
      "Validated batch 24 batch loss 1.29628491\n",
      "Validated batch 25 batch loss 1.27794194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 26 batch loss 1.28619695\n",
      "Validated batch 27 batch loss 1.28145206\n",
      "Validated batch 28 batch loss 1.35442328\n",
      "Validated batch 29 batch loss 1.3331672\n",
      "Validated batch 30 batch loss 1.37190616\n",
      "Validated batch 31 batch loss 1.27037907\n",
      "Validated batch 32 batch loss 1.36616075\n",
      "Validated batch 33 batch loss 1.35133958\n",
      "Validated batch 34 batch loss 1.37470019\n",
      "Validated batch 35 batch loss 1.3525008\n",
      "Validated batch 36 batch loss 1.2730732\n",
      "Validated batch 37 batch loss 1.26415658\n",
      "Validated batch 38 batch loss 1.35189855\n",
      "Validated batch 39 batch loss 1.31909\n",
      "Validated batch 40 batch loss 1.43497503\n",
      "Validated batch 41 batch loss 1.40041268\n",
      "Validated batch 42 batch loss 1.23324645\n",
      "Validated batch 43 batch loss 1.42992866\n",
      "Validated batch 44 batch loss 1.30629385\n",
      "Validated batch 45 batch loss 1.28808308\n",
      "Validated batch 46 batch loss 1.37218654\n",
      "Validated batch 47 batch loss 1.34702277\n",
      "Validated batch 48 batch loss 1.32539952\n",
      "Validated batch 49 batch loss 1.25205386\n",
      "Validated batch 50 batch loss 1.24578691\n",
      "Validated batch 51 batch loss 1.34734845\n",
      "Validated batch 52 batch loss 1.34035492\n",
      "Validated batch 53 batch loss 1.29285121\n",
      "Validated batch 54 batch loss 1.28551054\n",
      "Validated batch 55 batch loss 1.31928778\n",
      "Validated batch 56 batch loss 1.39654386\n",
      "Validated batch 57 batch loss 1.24369574\n",
      "Validated batch 58 batch loss 1.21005905\n",
      "Validated batch 59 batch loss 1.38061059\n",
      "Validated batch 60 batch loss 1.37538862\n",
      "Validated batch 61 batch loss 1.4644165\n",
      "Validated batch 62 batch loss 1.46409369\n",
      "Validated batch 63 batch loss 1.26795602\n",
      "Validated batch 64 batch loss 1.49714947\n",
      "Validated batch 65 batch loss 1.30343449\n",
      "Validated batch 66 batch loss 1.34879649\n",
      "Validated batch 67 batch loss 1.38346195\n",
      "Validated batch 68 batch loss 1.11486065\n",
      "Validated batch 69 batch loss 1.36654913\n",
      "Validated batch 70 batch loss 1.41598773\n",
      "Validated batch 71 batch loss 1.33153605\n",
      "Validated batch 72 batch loss 1.32349586\n",
      "Validated batch 73 batch loss 1.26566529\n",
      "Validated batch 74 batch loss 1.36455464\n",
      "Validated batch 75 batch loss 1.4868387\n",
      "Validated batch 76 batch loss 1.27016246\n",
      "Validated batch 77 batch loss 1.36862922\n",
      "Validated batch 78 batch loss 1.31261563\n",
      "Validated batch 79 batch loss 1.37453151\n",
      "Validated batch 80 batch loss 1.34460914\n",
      "Validated batch 81 batch loss 1.22858167\n",
      "Validated batch 82 batch loss 1.46259189\n",
      "Validated batch 83 batch loss 1.34454894\n",
      "Validated batch 84 batch loss 1.39615512\n",
      "Validated batch 85 batch loss 1.31856024\n",
      "Validated batch 86 batch loss 1.38344717\n",
      "Validated batch 87 batch loss 1.22719443\n",
      "Validated batch 88 batch loss 1.31676245\n",
      "Validated batch 89 batch loss 1.27275348\n",
      "Validated batch 90 batch loss 1.27865672\n",
      "Validated batch 91 batch loss 1.33890879\n",
      "Validated batch 92 batch loss 1.33476925\n",
      "Validated batch 93 batch loss 1.3733685\n",
      "Validated batch 94 batch loss 1.2621516\n",
      "Validated batch 95 batch loss 1.3165704\n",
      "Validated batch 96 batch loss 1.27758181\n",
      "Validated batch 97 batch loss 1.29264379\n",
      "Validated batch 98 batch loss 1.41872537\n",
      "Validated batch 99 batch loss 1.35390067\n",
      "Validated batch 100 batch loss 1.39946425\n",
      "Validated batch 101 batch loss 1.40463567\n",
      "Validated batch 102 batch loss 1.37510192\n",
      "Validated batch 103 batch loss 1.33060324\n",
      "Validated batch 104 batch loss 1.44677615\n",
      "Validated batch 105 batch loss 1.37266707\n",
      "Validated batch 106 batch loss 1.44290972\n",
      "Validated batch 107 batch loss 1.40539169\n",
      "Validated batch 108 batch loss 1.434183\n",
      "Validated batch 109 batch loss 1.40021646\n",
      "Validated batch 110 batch loss 1.24896526\n",
      "Validated batch 111 batch loss 1.3444947\n",
      "Validated batch 112 batch loss 1.36685979\n",
      "Validated batch 113 batch loss 1.40561295\n",
      "Validated batch 114 batch loss 1.3304255\n",
      "Validated batch 115 batch loss 1.30598545\n",
      "Validated batch 116 batch loss 1.31085622\n",
      "Validated batch 117 batch loss 1.41048563\n",
      "Validated batch 118 batch loss 1.23400068\n",
      "Validated batch 119 batch loss 1.29377007\n",
      "Validated batch 120 batch loss 1.31621981\n",
      "Validated batch 121 batch loss 1.33998454\n",
      "Validated batch 122 batch loss 1.4027667\n",
      "Validated batch 123 batch loss 1.42943311\n",
      "Validated batch 124 batch loss 1.39005697\n",
      "Validated batch 125 batch loss 1.30368042\n",
      "Validated batch 126 batch loss 1.41614819\n",
      "Validated batch 127 batch loss 1.342098\n",
      "Validated batch 128 batch loss 1.32160091\n",
      "Validated batch 129 batch loss 1.43591428\n",
      "Validated batch 130 batch loss 1.39009666\n",
      "Validated batch 131 batch loss 1.37668371\n",
      "Validated batch 132 batch loss 1.4232825\n",
      "Validated batch 133 batch loss 1.25627112\n",
      "Validated batch 134 batch loss 1.27629578\n",
      "Validated batch 135 batch loss 1.30195856\n",
      "Validated batch 136 batch loss 1.23532021\n",
      "Validated batch 137 batch loss 1.46044922\n",
      "Validated batch 138 batch loss 1.30070019\n",
      "Validated batch 139 batch loss 1.31066036\n",
      "Validated batch 140 batch loss 1.33733487\n",
      "Validated batch 141 batch loss 1.35127175\n",
      "Validated batch 142 batch loss 1.16246665\n",
      "Validated batch 143 batch loss 1.25986958\n",
      "Validated batch 144 batch loss 1.39178753\n",
      "Validated batch 145 batch loss 1.2232821\n",
      "Validated batch 146 batch loss 1.21167123\n",
      "Validated batch 147 batch loss 1.28351665\n",
      "Validated batch 148 batch loss 1.34369659\n",
      "Validated batch 149 batch loss 1.25973547\n",
      "Validated batch 150 batch loss 1.33525062\n",
      "Validated batch 151 batch loss 1.24502063\n",
      "Validated batch 152 batch loss 1.31637859\n",
      "Validated batch 153 batch loss 1.41921902\n",
      "Validated batch 154 batch loss 1.42767382\n",
      "Validated batch 155 batch loss 1.26275098\n",
      "Validated batch 156 batch loss 1.45248711\n",
      "Validated batch 157 batch loss 1.19695961\n",
      "Validated batch 158 batch loss 1.20021009\n",
      "Validated batch 159 batch loss 1.35204482\n",
      "Validated batch 160 batch loss 1.31740403\n",
      "Validated batch 161 batch loss 1.42239666\n",
      "Validated batch 162 batch loss 1.47685838\n",
      "Validated batch 163 batch loss 1.32288098\n",
      "Validated batch 164 batch loss 1.33582282\n",
      "Validated batch 165 batch loss 1.32005048\n",
      "Validated batch 166 batch loss 1.23465061\n",
      "Validated batch 167 batch loss 1.38898754\n",
      "Validated batch 168 batch loss 1.35597801\n",
      "Validated batch 169 batch loss 1.27049875\n",
      "Validated batch 170 batch loss 1.25853074\n",
      "Validated batch 171 batch loss 1.36642981\n",
      "Validated batch 172 batch loss 1.33969092\n",
      "Validated batch 173 batch loss 1.41743028\n",
      "Validated batch 174 batch loss 1.3737222\n",
      "Validated batch 175 batch loss 1.26169252\n",
      "Validated batch 176 batch loss 1.34565449\n",
      "Validated batch 177 batch loss 1.33549905\n",
      "Validated batch 178 batch loss 1.33458614\n",
      "Validated batch 179 batch loss 1.33226895\n",
      "Validated batch 180 batch loss 1.39036405\n",
      "Validated batch 181 batch loss 1.49989164\n",
      "Validated batch 182 batch loss 1.50896525\n",
      "Validated batch 183 batch loss 1.35859752\n",
      "Validated batch 184 batch loss 1.2699163\n",
      "Validated batch 185 batch loss 1.18664598\n",
      "Epoch 2 val loss 1.3360581398010254\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-2-loss-1.3361.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.26854253 epoch total loss 1.26854253\n",
      "Trained batch 2 batch loss 1.36461377 epoch total loss 1.31657815\n",
      "Trained batch 3 batch loss 1.42388916 epoch total loss 1.35234845\n",
      "Trained batch 4 batch loss 1.56375515 epoch total loss 1.40520012\n",
      "Trained batch 5 batch loss 1.50982511 epoch total loss 1.42612517\n",
      "Trained batch 6 batch loss 1.44405603 epoch total loss 1.42911375\n",
      "Trained batch 7 batch loss 1.39875555 epoch total loss 1.42477691\n",
      "Trained batch 8 batch loss 1.34251022 epoch total loss 1.41449356\n",
      "Trained batch 9 batch loss 1.25028038 epoch total loss 1.39624763\n",
      "Trained batch 10 batch loss 1.25967193 epoch total loss 1.38259\n",
      "Trained batch 11 batch loss 1.32827544 epoch total loss 1.37765241\n",
      "Trained batch 12 batch loss 1.40627134 epoch total loss 1.38003731\n",
      "Trained batch 13 batch loss 1.34788787 epoch total loss 1.37756419\n",
      "Trained batch 14 batch loss 1.29750168 epoch total loss 1.37184548\n",
      "Trained batch 15 batch loss 1.22510099 epoch total loss 1.36206257\n",
      "Trained batch 16 batch loss 1.33367789 epoch total loss 1.3602885\n",
      "Trained batch 17 batch loss 1.26978326 epoch total loss 1.35496461\n",
      "Trained batch 18 batch loss 1.20376384 epoch total loss 1.34656465\n",
      "Trained batch 19 batch loss 1.2723155 epoch total loss 1.34265673\n",
      "Trained batch 20 batch loss 1.24979746 epoch total loss 1.33801389\n",
      "Trained batch 21 batch loss 1.27075195 epoch total loss 1.33481085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 22 batch loss 1.379251 epoch total loss 1.33683097\n",
      "Trained batch 23 batch loss 1.50798976 epoch total loss 1.34427261\n",
      "Trained batch 24 batch loss 1.327636 epoch total loss 1.34357941\n",
      "Trained batch 25 batch loss 1.23154783 epoch total loss 1.33909822\n",
      "Trained batch 26 batch loss 1.23637497 epoch total loss 1.33514726\n",
      "Trained batch 27 batch loss 1.32813239 epoch total loss 1.3348875\n",
      "Trained batch 28 batch loss 1.19963825 epoch total loss 1.33005714\n",
      "Trained batch 29 batch loss 1.15239346 epoch total loss 1.32393086\n",
      "Trained batch 30 batch loss 1.13988328 epoch total loss 1.31779599\n",
      "Trained batch 31 batch loss 1.08830619 epoch total loss 1.3103931\n",
      "Trained batch 32 batch loss 1.12892604 epoch total loss 1.30472219\n",
      "Trained batch 33 batch loss 1.14899576 epoch total loss 1.30000317\n",
      "Trained batch 34 batch loss 1.13427818 epoch total loss 1.29512882\n",
      "Trained batch 35 batch loss 1.21522343 epoch total loss 1.29284596\n",
      "Trained batch 36 batch loss 1.21544147 epoch total loss 1.29069579\n",
      "Trained batch 37 batch loss 1.27088606 epoch total loss 1.29016042\n",
      "Trained batch 38 batch loss 1.10530329 epoch total loss 1.28529584\n",
      "Trained batch 39 batch loss 1.16275811 epoch total loss 1.28215384\n",
      "Trained batch 40 batch loss 1.25555456 epoch total loss 1.28148878\n",
      "Trained batch 41 batch loss 1.37082422 epoch total loss 1.28366768\n",
      "Trained batch 42 batch loss 1.36015677 epoch total loss 1.28548884\n",
      "Trained batch 43 batch loss 1.29972255 epoch total loss 1.28581989\n",
      "Trained batch 44 batch loss 1.25915551 epoch total loss 1.28521383\n",
      "Trained batch 45 batch loss 1.29351401 epoch total loss 1.28539824\n",
      "Trained batch 46 batch loss 1.33157408 epoch total loss 1.28640211\n",
      "Trained batch 47 batch loss 1.29781723 epoch total loss 1.28664494\n",
      "Trained batch 48 batch loss 1.4067359 epoch total loss 1.28914678\n",
      "Trained batch 49 batch loss 1.3007139 epoch total loss 1.28938282\n",
      "Trained batch 50 batch loss 1.23834157 epoch total loss 1.28836203\n",
      "Trained batch 51 batch loss 1.20562482 epoch total loss 1.28673971\n",
      "Trained batch 52 batch loss 1.18621743 epoch total loss 1.28480661\n",
      "Trained batch 53 batch loss 1.23504019 epoch total loss 1.2838676\n",
      "Trained batch 54 batch loss 1.23559642 epoch total loss 1.28297365\n",
      "Trained batch 55 batch loss 1.22052479 epoch total loss 1.2818383\n",
      "Trained batch 56 batch loss 1.16901469 epoch total loss 1.27982354\n",
      "Trained batch 57 batch loss 1.24875319 epoch total loss 1.27927852\n",
      "Trained batch 58 batch loss 1.26313949 epoch total loss 1.27900028\n",
      "Trained batch 59 batch loss 1.25597024 epoch total loss 1.27861\n",
      "Trained batch 60 batch loss 1.19747472 epoch total loss 1.27725768\n",
      "Trained batch 61 batch loss 1.30768025 epoch total loss 1.27775633\n",
      "Trained batch 62 batch loss 1.29761541 epoch total loss 1.27807665\n",
      "Trained batch 63 batch loss 1.30540645 epoch total loss 1.27851045\n",
      "Trained batch 64 batch loss 1.2979269 epoch total loss 1.27881384\n",
      "Trained batch 65 batch loss 1.32893145 epoch total loss 1.27958488\n",
      "Trained batch 66 batch loss 1.38490033 epoch total loss 1.28118062\n",
      "Trained batch 67 batch loss 1.3448987 epoch total loss 1.28213167\n",
      "Trained batch 68 batch loss 1.28111792 epoch total loss 1.28211689\n",
      "Trained batch 69 batch loss 1.17297816 epoch total loss 1.2805351\n",
      "Trained batch 70 batch loss 1.11952245 epoch total loss 1.27823496\n",
      "Trained batch 71 batch loss 1.16221035 epoch total loss 1.27660084\n",
      "Trained batch 72 batch loss 1.34501123 epoch total loss 1.27755094\n",
      "Trained batch 73 batch loss 1.30186903 epoch total loss 1.27788413\n",
      "Trained batch 74 batch loss 1.51377153 epoch total loss 1.28107178\n",
      "Trained batch 75 batch loss 1.55633593 epoch total loss 1.28474188\n",
      "Trained batch 76 batch loss 1.44852853 epoch total loss 1.28689706\n",
      "Trained batch 77 batch loss 1.35128689 epoch total loss 1.28773332\n",
      "Trained batch 78 batch loss 1.32571554 epoch total loss 1.28822029\n",
      "Trained batch 79 batch loss 1.167907 epoch total loss 1.28669727\n",
      "Trained batch 80 batch loss 1.19146419 epoch total loss 1.28550696\n",
      "Trained batch 81 batch loss 1.3850354 epoch total loss 1.28673565\n",
      "Trained batch 82 batch loss 1.35905707 epoch total loss 1.28761756\n",
      "Trained batch 83 batch loss 1.29392076 epoch total loss 1.2876935\n",
      "Trained batch 84 batch loss 1.35577953 epoch total loss 1.28850412\n",
      "Trained batch 85 batch loss 1.32699251 epoch total loss 1.288957\n",
      "Trained batch 86 batch loss 1.27510297 epoch total loss 1.28879583\n",
      "Trained batch 87 batch loss 1.36944938 epoch total loss 1.2897228\n",
      "Trained batch 88 batch loss 1.30476701 epoch total loss 1.28989375\n",
      "Trained batch 89 batch loss 1.41007924 epoch total loss 1.29124415\n",
      "Trained batch 90 batch loss 1.24664485 epoch total loss 1.2907486\n",
      "Trained batch 91 batch loss 1.26346493 epoch total loss 1.29044878\n",
      "Trained batch 92 batch loss 1.28934968 epoch total loss 1.29043686\n",
      "Trained batch 93 batch loss 1.25535631 epoch total loss 1.29005969\n",
      "Trained batch 94 batch loss 1.23989224 epoch total loss 1.28952599\n",
      "Trained batch 95 batch loss 1.33855987 epoch total loss 1.29004216\n",
      "Trained batch 96 batch loss 1.30966234 epoch total loss 1.29024649\n",
      "Trained batch 97 batch loss 1.29392183 epoch total loss 1.2902844\n",
      "Trained batch 98 batch loss 1.26081574 epoch total loss 1.28998375\n",
      "Trained batch 99 batch loss 1.20978582 epoch total loss 1.2891736\n",
      "Trained batch 100 batch loss 1.29636765 epoch total loss 1.28924561\n",
      "Trained batch 101 batch loss 1.29757071 epoch total loss 1.2893281\n",
      "Trained batch 102 batch loss 1.17913127 epoch total loss 1.28824782\n",
      "Trained batch 103 batch loss 1.22643352 epoch total loss 1.28764772\n",
      "Trained batch 104 batch loss 1.37128818 epoch total loss 1.28845203\n",
      "Trained batch 105 batch loss 1.32034588 epoch total loss 1.28875577\n",
      "Trained batch 106 batch loss 1.25667739 epoch total loss 1.2884531\n",
      "Trained batch 107 batch loss 1.27703381 epoch total loss 1.28834653\n",
      "Trained batch 108 batch loss 1.15919733 epoch total loss 1.28715062\n",
      "Trained batch 109 batch loss 1.29324305 epoch total loss 1.28720653\n",
      "Trained batch 110 batch loss 1.28245068 epoch total loss 1.28716338\n",
      "Trained batch 111 batch loss 1.24784732 epoch total loss 1.28680921\n",
      "Trained batch 112 batch loss 1.13298547 epoch total loss 1.28543568\n",
      "Trained batch 113 batch loss 1.10629797 epoch total loss 1.28385031\n",
      "Trained batch 114 batch loss 1.26099324 epoch total loss 1.2836498\n",
      "Trained batch 115 batch loss 1.46358597 epoch total loss 1.28521454\n",
      "Trained batch 116 batch loss 1.4933629 epoch total loss 1.28700888\n",
      "Trained batch 117 batch loss 1.30557942 epoch total loss 1.28716755\n",
      "Trained batch 118 batch loss 1.26018643 epoch total loss 1.28693891\n",
      "Trained batch 119 batch loss 1.35471165 epoch total loss 1.28750837\n",
      "Trained batch 120 batch loss 1.32284284 epoch total loss 1.28780293\n",
      "Trained batch 121 batch loss 1.2580961 epoch total loss 1.28755748\n",
      "Trained batch 122 batch loss 1.23970926 epoch total loss 1.28716528\n",
      "Trained batch 123 batch loss 1.33881664 epoch total loss 1.28758526\n",
      "Trained batch 124 batch loss 1.2381382 epoch total loss 1.2871865\n",
      "Trained batch 125 batch loss 1.30742204 epoch total loss 1.28734839\n",
      "Trained batch 126 batch loss 1.29716372 epoch total loss 1.28742635\n",
      "Trained batch 127 batch loss 1.20615101 epoch total loss 1.28678632\n",
      "Trained batch 128 batch loss 1.32577968 epoch total loss 1.2870909\n",
      "Trained batch 129 batch loss 1.21328902 epoch total loss 1.28651881\n",
      "Trained batch 130 batch loss 1.26862848 epoch total loss 1.28638113\n",
      "Trained batch 131 batch loss 1.38379633 epoch total loss 1.28712475\n",
      "Trained batch 132 batch loss 1.25085461 epoch total loss 1.28685\n",
      "Trained batch 133 batch loss 1.35397208 epoch total loss 1.28735471\n",
      "Trained batch 134 batch loss 1.19039357 epoch total loss 1.28663111\n",
      "Trained batch 135 batch loss 1.21280146 epoch total loss 1.28608418\n",
      "Trained batch 136 batch loss 1.33279049 epoch total loss 1.28642762\n",
      "Trained batch 137 batch loss 1.30215013 epoch total loss 1.28654242\n",
      "Trained batch 138 batch loss 1.3277061 epoch total loss 1.2868408\n",
      "Trained batch 139 batch loss 1.35556269 epoch total loss 1.28733516\n",
      "Trained batch 140 batch loss 1.35183537 epoch total loss 1.2877959\n",
      "Trained batch 141 batch loss 1.40399909 epoch total loss 1.28862\n",
      "Trained batch 142 batch loss 1.3416512 epoch total loss 1.28899336\n",
      "Trained batch 143 batch loss 1.39164066 epoch total loss 1.28971124\n",
      "Trained batch 144 batch loss 1.44771218 epoch total loss 1.29080844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 145 batch loss 1.26333976 epoch total loss 1.29061902\n",
      "Trained batch 146 batch loss 1.30810344 epoch total loss 1.29073882\n",
      "Trained batch 147 batch loss 1.29314876 epoch total loss 1.29075515\n",
      "Trained batch 148 batch loss 1.21053362 epoch total loss 1.29021323\n",
      "Trained batch 149 batch loss 1.20628333 epoch total loss 1.28965\n",
      "Trained batch 150 batch loss 1.11024165 epoch total loss 1.28845394\n",
      "Trained batch 151 batch loss 1.26695418 epoch total loss 1.28831148\n",
      "Trained batch 152 batch loss 1.36177683 epoch total loss 1.28879476\n",
      "Trained batch 153 batch loss 1.34079885 epoch total loss 1.28913474\n",
      "Trained batch 154 batch loss 1.43478119 epoch total loss 1.29008043\n",
      "Trained batch 155 batch loss 1.33739817 epoch total loss 1.29038572\n",
      "Trained batch 156 batch loss 1.40398741 epoch total loss 1.29111397\n",
      "Trained batch 157 batch loss 1.33834052 epoch total loss 1.29141474\n",
      "Trained batch 158 batch loss 1.419245 epoch total loss 1.29222381\n",
      "Trained batch 159 batch loss 1.27714372 epoch total loss 1.29212904\n",
      "Trained batch 160 batch loss 1.2993927 epoch total loss 1.29217446\n",
      "Trained batch 161 batch loss 1.43430734 epoch total loss 1.29305732\n",
      "Trained batch 162 batch loss 1.39900136 epoch total loss 1.2937113\n",
      "Trained batch 163 batch loss 1.44013488 epoch total loss 1.29460955\n",
      "Trained batch 164 batch loss 1.32465196 epoch total loss 1.29479277\n",
      "Trained batch 165 batch loss 1.24534631 epoch total loss 1.29449308\n",
      "Trained batch 166 batch loss 1.40202165 epoch total loss 1.29514086\n",
      "Trained batch 167 batch loss 1.25111151 epoch total loss 1.29487717\n",
      "Trained batch 168 batch loss 1.42724955 epoch total loss 1.29566514\n",
      "Trained batch 169 batch loss 1.44363904 epoch total loss 1.29654062\n",
      "Trained batch 170 batch loss 1.39791298 epoch total loss 1.29713702\n",
      "Trained batch 171 batch loss 1.35455251 epoch total loss 1.29747272\n",
      "Trained batch 172 batch loss 1.1980921 epoch total loss 1.29689491\n",
      "Trained batch 173 batch loss 1.18523836 epoch total loss 1.29624951\n",
      "Trained batch 174 batch loss 1.18094504 epoch total loss 1.29558682\n",
      "Trained batch 175 batch loss 1.19390154 epoch total loss 1.2950058\n",
      "Trained batch 176 batch loss 1.22922039 epoch total loss 1.29463208\n",
      "Trained batch 177 batch loss 1.23089886 epoch total loss 1.29427195\n",
      "Trained batch 178 batch loss 1.20354879 epoch total loss 1.29376233\n",
      "Trained batch 179 batch loss 1.29152918 epoch total loss 1.29374981\n",
      "Trained batch 180 batch loss 1.36266148 epoch total loss 1.29413271\n",
      "Trained batch 181 batch loss 1.41360366 epoch total loss 1.29479277\n",
      "Trained batch 182 batch loss 1.32731843 epoch total loss 1.29497147\n",
      "Trained batch 183 batch loss 1.34070706 epoch total loss 1.29522133\n",
      "Trained batch 184 batch loss 1.26106942 epoch total loss 1.29503572\n",
      "Trained batch 185 batch loss 1.20754075 epoch total loss 1.29456282\n",
      "Trained batch 186 batch loss 1.18945575 epoch total loss 1.29399765\n",
      "Trained batch 187 batch loss 1.23802686 epoch total loss 1.29369831\n",
      "Trained batch 188 batch loss 1.21935391 epoch total loss 1.29330289\n",
      "Trained batch 189 batch loss 1.35477448 epoch total loss 1.29362822\n",
      "Trained batch 190 batch loss 1.36235571 epoch total loss 1.2939899\n",
      "Trained batch 191 batch loss 1.28764582 epoch total loss 1.29395664\n",
      "Trained batch 192 batch loss 1.20302367 epoch total loss 1.29348302\n",
      "Trained batch 193 batch loss 1.26595783 epoch total loss 1.29334044\n",
      "Trained batch 194 batch loss 1.28412318 epoch total loss 1.29329288\n",
      "Trained batch 195 batch loss 1.05577981 epoch total loss 1.29207492\n",
      "Trained batch 196 batch loss 1.07900906 epoch total loss 1.29098785\n",
      "Trained batch 197 batch loss 1.11382413 epoch total loss 1.29008853\n",
      "Trained batch 198 batch loss 1.26923788 epoch total loss 1.28998327\n",
      "Trained batch 199 batch loss 1.37347186 epoch total loss 1.29040277\n",
      "Trained batch 200 batch loss 1.47811091 epoch total loss 1.29134142\n",
      "Trained batch 201 batch loss 1.39101708 epoch total loss 1.29183733\n",
      "Trained batch 202 batch loss 1.48424089 epoch total loss 1.29278982\n",
      "Trained batch 203 batch loss 1.3035301 epoch total loss 1.29284275\n",
      "Trained batch 204 batch loss 1.29818428 epoch total loss 1.29286897\n",
      "Trained batch 205 batch loss 1.40226388 epoch total loss 1.29340255\n",
      "Trained batch 206 batch loss 1.40595984 epoch total loss 1.29394889\n",
      "Trained batch 207 batch loss 1.31036854 epoch total loss 1.29402816\n",
      "Trained batch 208 batch loss 1.25875127 epoch total loss 1.29385865\n",
      "Trained batch 209 batch loss 1.22378135 epoch total loss 1.29352331\n",
      "Trained batch 210 batch loss 1.09498763 epoch total loss 1.29257798\n",
      "Trained batch 211 batch loss 1.18921304 epoch total loss 1.29208803\n",
      "Trained batch 212 batch loss 1.36600256 epoch total loss 1.29243672\n",
      "Trained batch 213 batch loss 1.28813303 epoch total loss 1.29241657\n",
      "Trained batch 214 batch loss 1.35867214 epoch total loss 1.29272616\n",
      "Trained batch 215 batch loss 1.40562248 epoch total loss 1.29325116\n",
      "Trained batch 216 batch loss 1.34310317 epoch total loss 1.29348207\n",
      "Trained batch 217 batch loss 1.37234116 epoch total loss 1.29384542\n",
      "Trained batch 218 batch loss 1.30962729 epoch total loss 1.29391789\n",
      "Trained batch 219 batch loss 1.28730381 epoch total loss 1.29388762\n",
      "Trained batch 220 batch loss 1.33136654 epoch total loss 1.29405797\n",
      "Trained batch 221 batch loss 1.38463712 epoch total loss 1.29446781\n",
      "Trained batch 222 batch loss 1.17011535 epoch total loss 1.29390764\n",
      "Trained batch 223 batch loss 1.25607371 epoch total loss 1.29373801\n",
      "Trained batch 224 batch loss 1.18772876 epoch total loss 1.29326463\n",
      "Trained batch 225 batch loss 1.27185917 epoch total loss 1.2931695\n",
      "Trained batch 226 batch loss 1.12199187 epoch total loss 1.29241204\n",
      "Trained batch 227 batch loss 1.09948874 epoch total loss 1.29156208\n",
      "Trained batch 228 batch loss 1.18533611 epoch total loss 1.29109621\n",
      "Trained batch 229 batch loss 1.33113217 epoch total loss 1.29127109\n",
      "Trained batch 230 batch loss 1.17576241 epoch total loss 1.29076886\n",
      "Trained batch 231 batch loss 1.14654362 epoch total loss 1.29014444\n",
      "Trained batch 232 batch loss 1.30017805 epoch total loss 1.29018772\n",
      "Trained batch 233 batch loss 1.18446267 epoch total loss 1.28973389\n",
      "Trained batch 234 batch loss 1.25726879 epoch total loss 1.28959513\n",
      "Trained batch 235 batch loss 1.4066962 epoch total loss 1.29009342\n",
      "Trained batch 236 batch loss 1.36713386 epoch total loss 1.29041994\n",
      "Trained batch 237 batch loss 1.37786627 epoch total loss 1.29078889\n",
      "Trained batch 238 batch loss 1.295506 epoch total loss 1.29080868\n",
      "Trained batch 239 batch loss 1.35165823 epoch total loss 1.29106331\n",
      "Trained batch 240 batch loss 1.23770416 epoch total loss 1.29084086\n",
      "Trained batch 241 batch loss 1.40359402 epoch total loss 1.29130876\n",
      "Trained batch 242 batch loss 1.40419662 epoch total loss 1.29177523\n",
      "Trained batch 243 batch loss 1.27031076 epoch total loss 1.29168701\n",
      "Trained batch 244 batch loss 1.29804468 epoch total loss 1.291713\n",
      "Trained batch 245 batch loss 1.29257488 epoch total loss 1.29171658\n",
      "Trained batch 246 batch loss 1.34062409 epoch total loss 1.29191542\n",
      "Trained batch 247 batch loss 1.3926599 epoch total loss 1.29232335\n",
      "Trained batch 248 batch loss 1.11844385 epoch total loss 1.29162216\n",
      "Trained batch 249 batch loss 1.09420753 epoch total loss 1.2908293\n",
      "Trained batch 250 batch loss 1.2326076 epoch total loss 1.29059649\n",
      "Trained batch 251 batch loss 1.23814821 epoch total loss 1.29038751\n",
      "Trained batch 252 batch loss 1.17922759 epoch total loss 1.28994644\n",
      "Trained batch 253 batch loss 1.23335063 epoch total loss 1.28972268\n",
      "Trained batch 254 batch loss 1.27005792 epoch total loss 1.2896452\n",
      "Trained batch 255 batch loss 1.34526968 epoch total loss 1.28986335\n",
      "Trained batch 256 batch loss 1.3802253 epoch total loss 1.29021633\n",
      "Trained batch 257 batch loss 1.29963291 epoch total loss 1.29025292\n",
      "Trained batch 258 batch loss 1.34322584 epoch total loss 1.29045832\n",
      "Trained batch 259 batch loss 1.54890752 epoch total loss 1.29145622\n",
      "Trained batch 260 batch loss 1.46461689 epoch total loss 1.29212224\n",
      "Trained batch 261 batch loss 1.20725191 epoch total loss 1.29179704\n",
      "Trained batch 262 batch loss 1.10496116 epoch total loss 1.29108393\n",
      "Trained batch 263 batch loss 1.24526668 epoch total loss 1.29090965\n",
      "Trained batch 264 batch loss 1.18422568 epoch total loss 1.29050565\n",
      "Trained batch 265 batch loss 1.25220823 epoch total loss 1.29036105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 266 batch loss 1.2150532 epoch total loss 1.29007792\n",
      "Trained batch 267 batch loss 1.2737869 epoch total loss 1.29001689\n",
      "Trained batch 268 batch loss 1.24978542 epoch total loss 1.28986681\n",
      "Trained batch 269 batch loss 1.24783528 epoch total loss 1.28971052\n",
      "Trained batch 270 batch loss 1.15752232 epoch total loss 1.28922093\n",
      "Trained batch 271 batch loss 1.27439392 epoch total loss 1.28916621\n",
      "Trained batch 272 batch loss 1.22530103 epoch total loss 1.28893149\n",
      "Trained batch 273 batch loss 1.23023081 epoch total loss 1.28871644\n",
      "Trained batch 274 batch loss 1.19720268 epoch total loss 1.28838241\n",
      "Trained batch 275 batch loss 1.28762686 epoch total loss 1.28837967\n",
      "Trained batch 276 batch loss 1.34757066 epoch total loss 1.28859413\n",
      "Trained batch 277 batch loss 1.56638122 epoch total loss 1.28959692\n",
      "Trained batch 278 batch loss 1.51773524 epoch total loss 1.29041755\n",
      "Trained batch 279 batch loss 1.32849085 epoch total loss 1.29055405\n",
      "Trained batch 280 batch loss 1.1917038 epoch total loss 1.29020107\n",
      "Trained batch 281 batch loss 1.28470349 epoch total loss 1.2901814\n",
      "Trained batch 282 batch loss 1.26035678 epoch total loss 1.29007566\n",
      "Trained batch 283 batch loss 1.24821401 epoch total loss 1.28992772\n",
      "Trained batch 284 batch loss 1.24327254 epoch total loss 1.28976345\n",
      "Trained batch 285 batch loss 1.22840118 epoch total loss 1.28954816\n",
      "Trained batch 286 batch loss 1.30541098 epoch total loss 1.28960359\n",
      "Trained batch 287 batch loss 1.27225685 epoch total loss 1.28954315\n",
      "Trained batch 288 batch loss 1.26481414 epoch total loss 1.2894572\n",
      "Trained batch 289 batch loss 1.19579303 epoch total loss 1.28913319\n",
      "Trained batch 290 batch loss 1.24186659 epoch total loss 1.28897011\n",
      "Trained batch 291 batch loss 1.26157141 epoch total loss 1.28887594\n",
      "Trained batch 292 batch loss 1.38592505 epoch total loss 1.28920829\n",
      "Trained batch 293 batch loss 1.25603008 epoch total loss 1.28909504\n",
      "Trained batch 294 batch loss 1.19883776 epoch total loss 1.2887882\n",
      "Trained batch 295 batch loss 1.32975769 epoch total loss 1.28892696\n",
      "Trained batch 296 batch loss 1.27326226 epoch total loss 1.28887403\n",
      "Trained batch 297 batch loss 1.29177284 epoch total loss 1.28888381\n",
      "Trained batch 298 batch loss 1.35269153 epoch total loss 1.28909791\n",
      "Trained batch 299 batch loss 1.30775034 epoch total loss 1.28916025\n",
      "Trained batch 300 batch loss 1.20800292 epoch total loss 1.28888977\n",
      "Trained batch 301 batch loss 1.20900893 epoch total loss 1.28862441\n",
      "Trained batch 302 batch loss 1.23229194 epoch total loss 1.28843784\n",
      "Trained batch 303 batch loss 1.21913767 epoch total loss 1.2882092\n",
      "Trained batch 304 batch loss 1.25996852 epoch total loss 1.28811634\n",
      "Trained batch 305 batch loss 1.36929893 epoch total loss 1.28838253\n",
      "Trained batch 306 batch loss 1.2132268 epoch total loss 1.28813696\n",
      "Trained batch 307 batch loss 1.30366254 epoch total loss 1.28818738\n",
      "Trained batch 308 batch loss 1.26382828 epoch total loss 1.28810835\n",
      "Trained batch 309 batch loss 1.17975354 epoch total loss 1.28775764\n",
      "Trained batch 310 batch loss 1.12149072 epoch total loss 1.28722131\n",
      "Trained batch 311 batch loss 1.12301803 epoch total loss 1.28669333\n",
      "Trained batch 312 batch loss 1.2037456 epoch total loss 1.28642738\n",
      "Trained batch 313 batch loss 1.23983717 epoch total loss 1.28627861\n",
      "Trained batch 314 batch loss 1.15572643 epoch total loss 1.2858628\n",
      "Trained batch 315 batch loss 1.112391 epoch total loss 1.28531218\n",
      "Trained batch 316 batch loss 1.17508888 epoch total loss 1.28496325\n",
      "Trained batch 317 batch loss 1.35198522 epoch total loss 1.28517473\n",
      "Trained batch 318 batch loss 1.30441666 epoch total loss 1.28523529\n",
      "Trained batch 319 batch loss 1.34643984 epoch total loss 1.28542709\n",
      "Trained batch 320 batch loss 1.29275417 epoch total loss 1.28545\n",
      "Trained batch 321 batch loss 1.33627534 epoch total loss 1.28560829\n",
      "Trained batch 322 batch loss 1.14047623 epoch total loss 1.28515756\n",
      "Trained batch 323 batch loss 1.27551341 epoch total loss 1.28512776\n",
      "Trained batch 324 batch loss 1.31688595 epoch total loss 1.28522575\n",
      "Trained batch 325 batch loss 1.36548781 epoch total loss 1.28547275\n",
      "Trained batch 326 batch loss 1.50076807 epoch total loss 1.28613305\n",
      "Trained batch 327 batch loss 1.30782533 epoch total loss 1.28619945\n",
      "Trained batch 328 batch loss 1.18948352 epoch total loss 1.28590453\n",
      "Trained batch 329 batch loss 1.35151374 epoch total loss 1.28610396\n",
      "Trained batch 330 batch loss 1.3391571 epoch total loss 1.28626466\n",
      "Trained batch 331 batch loss 1.22352099 epoch total loss 1.28607512\n",
      "Trained batch 332 batch loss 1.32873893 epoch total loss 1.28620362\n",
      "Trained batch 333 batch loss 1.31738114 epoch total loss 1.2862972\n",
      "Trained batch 334 batch loss 1.37494063 epoch total loss 1.28656256\n",
      "Trained batch 335 batch loss 1.34264278 epoch total loss 1.28673\n",
      "Trained batch 336 batch loss 1.48693609 epoch total loss 1.28732586\n",
      "Trained batch 337 batch loss 1.36414742 epoch total loss 1.28755379\n",
      "Trained batch 338 batch loss 1.3626883 epoch total loss 1.28777611\n",
      "Trained batch 339 batch loss 1.2438947 epoch total loss 1.28764677\n",
      "Trained batch 340 batch loss 1.22316861 epoch total loss 1.28745711\n",
      "Trained batch 341 batch loss 1.30244958 epoch total loss 1.2875011\n",
      "Trained batch 342 batch loss 1.27926707 epoch total loss 1.28747702\n",
      "Trained batch 343 batch loss 1.259606 epoch total loss 1.28739583\n",
      "Trained batch 344 batch loss 1.32027912 epoch total loss 1.28749132\n",
      "Trained batch 345 batch loss 1.32219088 epoch total loss 1.28759205\n",
      "Trained batch 346 batch loss 1.3263576 epoch total loss 1.28770399\n",
      "Trained batch 347 batch loss 1.2787677 epoch total loss 1.28767836\n",
      "Trained batch 348 batch loss 1.27319741 epoch total loss 1.28763664\n",
      "Trained batch 349 batch loss 1.33610749 epoch total loss 1.28777564\n",
      "Trained batch 350 batch loss 1.26917768 epoch total loss 1.28772247\n",
      "Trained batch 351 batch loss 1.42201066 epoch total loss 1.28810501\n",
      "Trained batch 352 batch loss 1.44798875 epoch total loss 1.2885592\n",
      "Trained batch 353 batch loss 1.58045 epoch total loss 1.28938603\n",
      "Trained batch 354 batch loss 1.56736541 epoch total loss 1.29017127\n",
      "Trained batch 355 batch loss 1.29480553 epoch total loss 1.29018438\n",
      "Trained batch 356 batch loss 1.17666459 epoch total loss 1.28986549\n",
      "Trained batch 357 batch loss 1.35918212 epoch total loss 1.29005969\n",
      "Trained batch 358 batch loss 1.26204908 epoch total loss 1.28998148\n",
      "Trained batch 359 batch loss 1.21309745 epoch total loss 1.28976727\n",
      "Trained batch 360 batch loss 1.17171288 epoch total loss 1.28943944\n",
      "Trained batch 361 batch loss 1.20142484 epoch total loss 1.28919554\n",
      "Trained batch 362 batch loss 1.22570181 epoch total loss 1.28902018\n",
      "Trained batch 363 batch loss 1.12826633 epoch total loss 1.28857732\n",
      "Trained batch 364 batch loss 1.17235541 epoch total loss 1.28825808\n",
      "Trained batch 365 batch loss 1.21917331 epoch total loss 1.28806877\n",
      "Trained batch 366 batch loss 1.29290903 epoch total loss 1.288082\n",
      "Trained batch 367 batch loss 1.2036705 epoch total loss 1.28785205\n",
      "Trained batch 368 batch loss 1.17072821 epoch total loss 1.28753376\n",
      "Trained batch 369 batch loss 1.19118881 epoch total loss 1.28727257\n",
      "Trained batch 370 batch loss 1.29479432 epoch total loss 1.28729296\n",
      "Trained batch 371 batch loss 1.25867295 epoch total loss 1.28721583\n",
      "Trained batch 372 batch loss 1.2712456 epoch total loss 1.28717291\n",
      "Trained batch 373 batch loss 1.29369783 epoch total loss 1.28719032\n",
      "Trained batch 374 batch loss 1.29712737 epoch total loss 1.2872169\n",
      "Trained batch 375 batch loss 1.70683992 epoch total loss 1.28833592\n",
      "Trained batch 376 batch loss 1.32119823 epoch total loss 1.2884233\n",
      "Trained batch 377 batch loss 1.39821744 epoch total loss 1.28871453\n",
      "Trained batch 378 batch loss 1.42455268 epoch total loss 1.28907394\n",
      "Trained batch 379 batch loss 1.4366827 epoch total loss 1.2894634\n",
      "Trained batch 380 batch loss 1.35257137 epoch total loss 1.28962946\n",
      "Trained batch 381 batch loss 1.35323334 epoch total loss 1.28979647\n",
      "Trained batch 382 batch loss 1.29618073 epoch total loss 1.28981316\n",
      "Trained batch 383 batch loss 1.25626159 epoch total loss 1.28972554\n",
      "Trained batch 384 batch loss 1.23360324 epoch total loss 1.28957939\n",
      "Trained batch 385 batch loss 1.38073516 epoch total loss 1.28981614\n",
      "Trained batch 386 batch loss 1.3402288 epoch total loss 1.28994679\n",
      "Trained batch 387 batch loss 1.28513384 epoch total loss 1.2899344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 388 batch loss 1.23209012 epoch total loss 1.28978527\n",
      "Trained batch 389 batch loss 1.3279525 epoch total loss 1.28988338\n",
      "Trained batch 390 batch loss 1.20053303 epoch total loss 1.28965425\n",
      "Trained batch 391 batch loss 1.19037092 epoch total loss 1.28940034\n",
      "Trained batch 392 batch loss 1.21608698 epoch total loss 1.2892133\n",
      "Trained batch 393 batch loss 1.19372439 epoch total loss 1.28897035\n",
      "Trained batch 394 batch loss 1.30161786 epoch total loss 1.28900242\n",
      "Trained batch 395 batch loss 1.3136158 epoch total loss 1.28906476\n",
      "Trained batch 396 batch loss 1.34487188 epoch total loss 1.28920567\n",
      "Trained batch 397 batch loss 1.29616094 epoch total loss 1.28922319\n",
      "Trained batch 398 batch loss 1.30985045 epoch total loss 1.28927505\n",
      "Trained batch 399 batch loss 1.17634559 epoch total loss 1.28899193\n",
      "Trained batch 400 batch loss 1.19395542 epoch total loss 1.28875446\n",
      "Trained batch 401 batch loss 1.18068123 epoch total loss 1.28848493\n",
      "Trained batch 402 batch loss 1.23197842 epoch total loss 1.28834438\n",
      "Trained batch 403 batch loss 1.39996529 epoch total loss 1.28862131\n",
      "Trained batch 404 batch loss 1.41236889 epoch total loss 1.28892756\n",
      "Trained batch 405 batch loss 1.28110397 epoch total loss 1.28890836\n",
      "Trained batch 406 batch loss 1.24194801 epoch total loss 1.28879261\n",
      "Trained batch 407 batch loss 1.31403255 epoch total loss 1.2888546\n",
      "Trained batch 408 batch loss 1.32082033 epoch total loss 1.28893292\n",
      "Trained batch 409 batch loss 1.24303579 epoch total loss 1.28882074\n",
      "Trained batch 410 batch loss 1.32693887 epoch total loss 1.28891373\n",
      "Trained batch 411 batch loss 1.2989428 epoch total loss 1.28893816\n",
      "Trained batch 412 batch loss 1.26445723 epoch total loss 1.2888788\n",
      "Trained batch 413 batch loss 1.32285261 epoch total loss 1.28896117\n",
      "Trained batch 414 batch loss 1.43476892 epoch total loss 1.28931332\n",
      "Trained batch 415 batch loss 1.31450796 epoch total loss 1.28937399\n",
      "Trained batch 416 batch loss 1.44518971 epoch total loss 1.28974855\n",
      "Trained batch 417 batch loss 1.37704515 epoch total loss 1.289958\n",
      "Trained batch 418 batch loss 1.3404876 epoch total loss 1.29007888\n",
      "Trained batch 419 batch loss 1.31715345 epoch total loss 1.29014349\n",
      "Trained batch 420 batch loss 1.32737684 epoch total loss 1.29023218\n",
      "Trained batch 421 batch loss 1.29415941 epoch total loss 1.2902416\n",
      "Trained batch 422 batch loss 1.3006022 epoch total loss 1.29026616\n",
      "Trained batch 423 batch loss 1.27738714 epoch total loss 1.29023576\n",
      "Trained batch 424 batch loss 1.26270473 epoch total loss 1.29017079\n",
      "Trained batch 425 batch loss 1.29940164 epoch total loss 1.29019248\n",
      "Trained batch 426 batch loss 1.22933936 epoch total loss 1.29004955\n",
      "Trained batch 427 batch loss 1.1611836 epoch total loss 1.28974771\n",
      "Trained batch 428 batch loss 1.23650014 epoch total loss 1.28962338\n",
      "Trained batch 429 batch loss 1.19883573 epoch total loss 1.28941178\n",
      "Trained batch 430 batch loss 1.31556785 epoch total loss 1.28947258\n",
      "Trained batch 431 batch loss 1.33007288 epoch total loss 1.28956676\n",
      "Trained batch 432 batch loss 1.26312304 epoch total loss 1.2895056\n",
      "Trained batch 433 batch loss 1.33860862 epoch total loss 1.28961897\n",
      "Trained batch 434 batch loss 1.3045125 epoch total loss 1.2896533\n",
      "Trained batch 435 batch loss 1.21530747 epoch total loss 1.28948247\n",
      "Trained batch 436 batch loss 1.34742951 epoch total loss 1.28961527\n",
      "Trained batch 437 batch loss 1.23473048 epoch total loss 1.28948975\n",
      "Trained batch 438 batch loss 1.23715198 epoch total loss 1.28937018\n",
      "Trained batch 439 batch loss 1.26694036 epoch total loss 1.28931916\n",
      "Trained batch 440 batch loss 1.26156819 epoch total loss 1.2892561\n",
      "Trained batch 441 batch loss 1.19147527 epoch total loss 1.28903437\n",
      "Trained batch 442 batch loss 1.26764274 epoch total loss 1.28898597\n",
      "Trained batch 443 batch loss 1.16936696 epoch total loss 1.28871596\n",
      "Trained batch 444 batch loss 1.20569813 epoch total loss 1.28852904\n",
      "Trained batch 445 batch loss 1.15902627 epoch total loss 1.28823793\n",
      "Trained batch 446 batch loss 1.11939454 epoch total loss 1.28785932\n",
      "Trained batch 447 batch loss 1.18690634 epoch total loss 1.28763342\n",
      "Trained batch 448 batch loss 1.23632526 epoch total loss 1.28751886\n",
      "Trained batch 449 batch loss 1.26918328 epoch total loss 1.28747809\n",
      "Trained batch 450 batch loss 1.21353686 epoch total loss 1.28731382\n",
      "Trained batch 451 batch loss 1.31376147 epoch total loss 1.28737247\n",
      "Trained batch 452 batch loss 1.33396828 epoch total loss 1.28747559\n",
      "Trained batch 453 batch loss 1.42561221 epoch total loss 1.28778052\n",
      "Trained batch 454 batch loss 1.29420888 epoch total loss 1.28779459\n",
      "Trained batch 455 batch loss 1.36422801 epoch total loss 1.28796268\n",
      "Trained batch 456 batch loss 1.26579535 epoch total loss 1.28791404\n",
      "Trained batch 457 batch loss 1.24447894 epoch total loss 1.28781915\n",
      "Trained batch 458 batch loss 1.28720307 epoch total loss 1.28781784\n",
      "Trained batch 459 batch loss 1.36120379 epoch total loss 1.2879777\n",
      "Trained batch 460 batch loss 1.28217316 epoch total loss 1.28796506\n",
      "Trained batch 461 batch loss 1.281721 epoch total loss 1.28795159\n",
      "Trained batch 462 batch loss 1.19630718 epoch total loss 1.28775311\n",
      "Trained batch 463 batch loss 1.2292757 epoch total loss 1.28762674\n",
      "Trained batch 464 batch loss 1.18630528 epoch total loss 1.28740835\n",
      "Trained batch 465 batch loss 1.2218014 epoch total loss 1.28726733\n",
      "Trained batch 466 batch loss 1.24536788 epoch total loss 1.28717732\n",
      "Trained batch 467 batch loss 1.34967852 epoch total loss 1.2873112\n",
      "Trained batch 468 batch loss 1.11794078 epoch total loss 1.28694928\n",
      "Trained batch 469 batch loss 1.22995949 epoch total loss 1.2868278\n",
      "Trained batch 470 batch loss 1.37576103 epoch total loss 1.28701687\n",
      "Trained batch 471 batch loss 1.35488319 epoch total loss 1.28716099\n",
      "Trained batch 472 batch loss 1.30916274 epoch total loss 1.28720748\n",
      "Trained batch 473 batch loss 1.22303963 epoch total loss 1.28707182\n",
      "Trained batch 474 batch loss 1.12090993 epoch total loss 1.28672123\n",
      "Trained batch 475 batch loss 1.06658363 epoch total loss 1.28625786\n",
      "Trained batch 476 batch loss 1.0143472 epoch total loss 1.28568661\n",
      "Trained batch 477 batch loss 1.10903251 epoch total loss 1.28531623\n",
      "Trained batch 478 batch loss 1.16404712 epoch total loss 1.28506255\n",
      "Trained batch 479 batch loss 1.20229363 epoch total loss 1.2848897\n",
      "Trained batch 480 batch loss 1.18860865 epoch total loss 1.28468907\n",
      "Trained batch 481 batch loss 1.14918637 epoch total loss 1.28440738\n",
      "Trained batch 482 batch loss 1.17651582 epoch total loss 1.2841835\n",
      "Trained batch 483 batch loss 1.1709708 epoch total loss 1.28394902\n",
      "Trained batch 484 batch loss 1.09662557 epoch total loss 1.28356206\n",
      "Trained batch 485 batch loss 1.2198416 epoch total loss 1.2834307\n",
      "Trained batch 486 batch loss 1.21967304 epoch total loss 1.28329945\n",
      "Trained batch 487 batch loss 1.15860844 epoch total loss 1.2830435\n",
      "Trained batch 488 batch loss 1.17789567 epoch total loss 1.28282809\n",
      "Trained batch 489 batch loss 1.23532796 epoch total loss 1.28273094\n",
      "Trained batch 490 batch loss 1.38896143 epoch total loss 1.28294778\n",
      "Trained batch 491 batch loss 1.49661577 epoch total loss 1.28338301\n",
      "Trained batch 492 batch loss 1.40097547 epoch total loss 1.28362203\n",
      "Trained batch 493 batch loss 1.28257537 epoch total loss 1.28362\n",
      "Trained batch 494 batch loss 1.35186303 epoch total loss 1.28375816\n",
      "Trained batch 495 batch loss 1.52364397 epoch total loss 1.28424275\n",
      "Trained batch 496 batch loss 1.35723746 epoch total loss 1.28438985\n",
      "Trained batch 497 batch loss 1.30058658 epoch total loss 1.28442252\n",
      "Trained batch 498 batch loss 1.24139571 epoch total loss 1.28433609\n",
      "Trained batch 499 batch loss 1.17288244 epoch total loss 1.28411281\n",
      "Trained batch 500 batch loss 1.38551545 epoch total loss 1.28431559\n",
      "Trained batch 501 batch loss 1.4362824 epoch total loss 1.28461885\n",
      "Trained batch 502 batch loss 1.4098289 epoch total loss 1.28486836\n",
      "Trained batch 503 batch loss 1.45679188 epoch total loss 1.28521013\n",
      "Trained batch 504 batch loss 1.43758368 epoch total loss 1.28551245\n",
      "Trained batch 505 batch loss 1.29885781 epoch total loss 1.28553879\n",
      "Trained batch 506 batch loss 1.3878715 epoch total loss 1.28574097\n",
      "Trained batch 507 batch loss 1.33301651 epoch total loss 1.28583431\n",
      "Trained batch 508 batch loss 1.37106133 epoch total loss 1.28600192\n",
      "Trained batch 509 batch loss 1.31763458 epoch total loss 1.28606415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 510 batch loss 1.27091444 epoch total loss 1.28603446\n",
      "Trained batch 511 batch loss 1.28644645 epoch total loss 1.28603518\n",
      "Trained batch 512 batch loss 1.25704587 epoch total loss 1.28597856\n",
      "Trained batch 513 batch loss 1.16676807 epoch total loss 1.2857461\n",
      "Trained batch 514 batch loss 1.21841168 epoch total loss 1.28561509\n",
      "Trained batch 515 batch loss 1.13159394 epoch total loss 1.28531599\n",
      "Trained batch 516 batch loss 1.21287382 epoch total loss 1.28517568\n",
      "Trained batch 517 batch loss 1.22862768 epoch total loss 1.28506625\n",
      "Trained batch 518 batch loss 1.33278 epoch total loss 1.2851584\n",
      "Trained batch 519 batch loss 1.55739808 epoch total loss 1.28568292\n",
      "Trained batch 520 batch loss 1.3181231 epoch total loss 1.28574526\n",
      "Trained batch 521 batch loss 1.36291027 epoch total loss 1.28589332\n",
      "Trained batch 522 batch loss 1.32567763 epoch total loss 1.28596961\n",
      "Trained batch 523 batch loss 1.38570082 epoch total loss 1.28616023\n",
      "Trained batch 524 batch loss 1.30326557 epoch total loss 1.28619289\n",
      "Trained batch 525 batch loss 1.43607807 epoch total loss 1.2864784\n",
      "Trained batch 526 batch loss 1.53993344 epoch total loss 1.28696024\n",
      "Trained batch 527 batch loss 1.45855427 epoch total loss 1.28728592\n",
      "Trained batch 528 batch loss 1.30790043 epoch total loss 1.28732491\n",
      "Trained batch 529 batch loss 1.17573297 epoch total loss 1.28711402\n",
      "Trained batch 530 batch loss 1.05370069 epoch total loss 1.28667367\n",
      "Trained batch 531 batch loss 1.2460587 epoch total loss 1.28659701\n",
      "Trained batch 532 batch loss 1.35753548 epoch total loss 1.28673041\n",
      "Trained batch 533 batch loss 1.06666553 epoch total loss 1.28631747\n",
      "Trained batch 534 batch loss 1.04667389 epoch total loss 1.28586876\n",
      "Trained batch 535 batch loss 1.10722327 epoch total loss 1.28553486\n",
      "Trained batch 536 batch loss 1.15476215 epoch total loss 1.28529096\n",
      "Trained batch 537 batch loss 1.16760325 epoch total loss 1.28507185\n",
      "Trained batch 538 batch loss 1.26379251 epoch total loss 1.28503227\n",
      "Trained batch 539 batch loss 1.27401805 epoch total loss 1.28501189\n",
      "Trained batch 540 batch loss 1.34416723 epoch total loss 1.28512144\n",
      "Trained batch 541 batch loss 1.28290486 epoch total loss 1.28511727\n",
      "Trained batch 542 batch loss 1.36326385 epoch total loss 1.28526151\n",
      "Trained batch 543 batch loss 1.43665338 epoch total loss 1.28554034\n",
      "Trained batch 544 batch loss 1.29098606 epoch total loss 1.28555036\n",
      "Trained batch 545 batch loss 1.30134773 epoch total loss 1.28557932\n",
      "Trained batch 546 batch loss 1.24111176 epoch total loss 1.2854979\n",
      "Trained batch 547 batch loss 1.38424361 epoch total loss 1.28567839\n",
      "Trained batch 548 batch loss 1.32712579 epoch total loss 1.28575397\n",
      "Trained batch 549 batch loss 1.28643453 epoch total loss 1.28575528\n",
      "Trained batch 550 batch loss 1.31379414 epoch total loss 1.28580618\n",
      "Trained batch 551 batch loss 1.30172777 epoch total loss 1.28583515\n",
      "Trained batch 552 batch loss 1.22960186 epoch total loss 1.28573334\n",
      "Trained batch 553 batch loss 1.31223214 epoch total loss 1.28578126\n",
      "Trained batch 554 batch loss 1.40312493 epoch total loss 1.2859931\n",
      "Trained batch 555 batch loss 1.32462358 epoch total loss 1.28606272\n",
      "Trained batch 556 batch loss 1.26714289 epoch total loss 1.28602874\n",
      "Trained batch 557 batch loss 1.2981267 epoch total loss 1.28605056\n",
      "Trained batch 558 batch loss 1.36083794 epoch total loss 1.28618455\n",
      "Trained batch 559 batch loss 1.20267534 epoch total loss 1.28603518\n",
      "Trained batch 560 batch loss 1.18844867 epoch total loss 1.28586102\n",
      "Trained batch 561 batch loss 1.31141329 epoch total loss 1.28590655\n",
      "Trained batch 562 batch loss 1.35732102 epoch total loss 1.28603351\n",
      "Trained batch 563 batch loss 1.32671213 epoch total loss 1.28610587\n",
      "Trained batch 564 batch loss 1.22124398 epoch total loss 1.28599083\n",
      "Trained batch 565 batch loss 1.32411325 epoch total loss 1.28605831\n",
      "Trained batch 566 batch loss 1.27181 epoch total loss 1.28603303\n",
      "Trained batch 567 batch loss 1.32310116 epoch total loss 1.28609848\n",
      "Trained batch 568 batch loss 1.29507399 epoch total loss 1.28611422\n",
      "Trained batch 569 batch loss 1.39897537 epoch total loss 1.28631258\n",
      "Trained batch 570 batch loss 1.32629204 epoch total loss 1.28638279\n",
      "Trained batch 571 batch loss 1.2871 epoch total loss 1.28638399\n",
      "Trained batch 572 batch loss 1.41748953 epoch total loss 1.28661323\n",
      "Trained batch 573 batch loss 1.38929749 epoch total loss 1.2867924\n",
      "Trained batch 574 batch loss 1.40265441 epoch total loss 1.28699422\n",
      "Trained batch 575 batch loss 1.17205119 epoch total loss 1.2867943\n",
      "Trained batch 576 batch loss 1.38732576 epoch total loss 1.28696883\n",
      "Trained batch 577 batch loss 1.32417369 epoch total loss 1.28703332\n",
      "Trained batch 578 batch loss 1.32066751 epoch total loss 1.28709149\n",
      "Trained batch 579 batch loss 1.35546589 epoch total loss 1.28720963\n",
      "Trained batch 580 batch loss 1.25006652 epoch total loss 1.28714561\n",
      "Trained batch 581 batch loss 1.3440851 epoch total loss 1.28724349\n",
      "Trained batch 582 batch loss 1.20805585 epoch total loss 1.28710747\n",
      "Trained batch 583 batch loss 1.35287702 epoch total loss 1.28722036\n",
      "Trained batch 584 batch loss 1.34127688 epoch total loss 1.28731287\n",
      "Trained batch 585 batch loss 1.27585328 epoch total loss 1.28729331\n",
      "Trained batch 586 batch loss 1.23187709 epoch total loss 1.28719878\n",
      "Trained batch 587 batch loss 1.19749522 epoch total loss 1.28704596\n",
      "Trained batch 588 batch loss 1.15358329 epoch total loss 1.28681898\n",
      "Trained batch 589 batch loss 1.31292093 epoch total loss 1.28686321\n",
      "Trained batch 590 batch loss 1.24300277 epoch total loss 1.28678894\n",
      "Trained batch 591 batch loss 1.32566404 epoch total loss 1.28685474\n",
      "Trained batch 592 batch loss 1.33854532 epoch total loss 1.28694201\n",
      "Trained batch 593 batch loss 1.33990633 epoch total loss 1.28703141\n",
      "Trained batch 594 batch loss 1.32066524 epoch total loss 1.28708804\n",
      "Trained batch 595 batch loss 1.33198953 epoch total loss 1.28716338\n",
      "Trained batch 596 batch loss 1.37225485 epoch total loss 1.28730619\n",
      "Trained batch 597 batch loss 1.27625179 epoch total loss 1.28728771\n",
      "Trained batch 598 batch loss 1.2147181 epoch total loss 1.28716636\n",
      "Trained batch 599 batch loss 1.31115842 epoch total loss 1.28720641\n",
      "Trained batch 600 batch loss 1.21496141 epoch total loss 1.28708601\n",
      "Trained batch 601 batch loss 1.14300561 epoch total loss 1.28684628\n",
      "Trained batch 602 batch loss 1.24315381 epoch total loss 1.28677368\n",
      "Trained batch 603 batch loss 1.2624104 epoch total loss 1.28673327\n",
      "Trained batch 604 batch loss 1.28613329 epoch total loss 1.2867322\n",
      "Trained batch 605 batch loss 1.24456668 epoch total loss 1.28666258\n",
      "Trained batch 606 batch loss 1.19802833 epoch total loss 1.28651619\n",
      "Trained batch 607 batch loss 1.15183794 epoch total loss 1.28629434\n",
      "Trained batch 608 batch loss 1.04200506 epoch total loss 1.28589261\n",
      "Trained batch 609 batch loss 1.20886326 epoch total loss 1.28576612\n",
      "Trained batch 610 batch loss 1.4287833 epoch total loss 1.28600049\n",
      "Trained batch 611 batch loss 1.30823779 epoch total loss 1.28603697\n",
      "Trained batch 612 batch loss 1.30175745 epoch total loss 1.2860626\n",
      "Trained batch 613 batch loss 1.39994836 epoch total loss 1.28624845\n",
      "Trained batch 614 batch loss 1.34702587 epoch total loss 1.28634739\n",
      "Trained batch 615 batch loss 1.18831682 epoch total loss 1.28618801\n",
      "Trained batch 616 batch loss 1.12128282 epoch total loss 1.28592026\n",
      "Trained batch 617 batch loss 1.28015208 epoch total loss 1.28591096\n",
      "Trained batch 618 batch loss 1.31550968 epoch total loss 1.28595877\n",
      "Trained batch 619 batch loss 1.24496782 epoch total loss 1.28589261\n",
      "Trained batch 620 batch loss 1.43355536 epoch total loss 1.28613079\n",
      "Trained batch 621 batch loss 1.28024137 epoch total loss 1.28612125\n",
      "Trained batch 622 batch loss 1.26372504 epoch total loss 1.28608525\n",
      "Trained batch 623 batch loss 1.3648181 epoch total loss 1.28621161\n",
      "Trained batch 624 batch loss 1.46702302 epoch total loss 1.28650141\n",
      "Trained batch 625 batch loss 1.47930849 epoch total loss 1.28680992\n",
      "Trained batch 626 batch loss 1.32284105 epoch total loss 1.28686738\n",
      "Trained batch 627 batch loss 1.42821932 epoch total loss 1.2870928\n",
      "Trained batch 628 batch loss 1.28680182 epoch total loss 1.28709233\n",
      "Trained batch 629 batch loss 1.34303737 epoch total loss 1.28718126\n",
      "Trained batch 630 batch loss 1.37384331 epoch total loss 1.28731883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 631 batch loss 1.24517834 epoch total loss 1.28725207\n",
      "Trained batch 632 batch loss 1.26466632 epoch total loss 1.28721631\n",
      "Trained batch 633 batch loss 1.25452089 epoch total loss 1.28716457\n",
      "Trained batch 634 batch loss 1.27313137 epoch total loss 1.28714252\n",
      "Trained batch 635 batch loss 1.20997286 epoch total loss 1.28702092\n",
      "Trained batch 636 batch loss 1.21907592 epoch total loss 1.28691411\n",
      "Trained batch 637 batch loss 1.37386441 epoch total loss 1.28705049\n",
      "Trained batch 638 batch loss 1.3840456 epoch total loss 1.2872026\n",
      "Trained batch 639 batch loss 1.3610884 epoch total loss 1.28731823\n",
      "Trained batch 640 batch loss 1.31985712 epoch total loss 1.28736901\n",
      "Trained batch 641 batch loss 1.20026588 epoch total loss 1.28723311\n",
      "Trained batch 642 batch loss 1.28975797 epoch total loss 1.28723705\n",
      "Trained batch 643 batch loss 1.31191933 epoch total loss 1.28727543\n",
      "Trained batch 644 batch loss 1.37947989 epoch total loss 1.28741848\n",
      "Trained batch 645 batch loss 1.36299276 epoch total loss 1.28753567\n",
      "Trained batch 646 batch loss 1.36824203 epoch total loss 1.2876606\n",
      "Trained batch 647 batch loss 1.41178167 epoch total loss 1.28785241\n",
      "Trained batch 648 batch loss 1.38763416 epoch total loss 1.28800642\n",
      "Trained batch 649 batch loss 1.33714533 epoch total loss 1.28808212\n",
      "Trained batch 650 batch loss 1.32384849 epoch total loss 1.2881372\n",
      "Trained batch 651 batch loss 1.1937499 epoch total loss 1.28799224\n",
      "Trained batch 652 batch loss 1.2275362 epoch total loss 1.28789949\n",
      "Trained batch 653 batch loss 1.21994662 epoch total loss 1.28779542\n",
      "Trained batch 654 batch loss 1.31936038 epoch total loss 1.2878437\n",
      "Trained batch 655 batch loss 1.21834075 epoch total loss 1.28773749\n",
      "Trained batch 656 batch loss 1.23465276 epoch total loss 1.28765666\n",
      "Trained batch 657 batch loss 1.207528 epoch total loss 1.28753471\n",
      "Trained batch 658 batch loss 1.28996789 epoch total loss 1.28753841\n",
      "Trained batch 659 batch loss 1.29042125 epoch total loss 1.2875427\n",
      "Trained batch 660 batch loss 1.21833944 epoch total loss 1.2874378\n",
      "Trained batch 661 batch loss 1.36411536 epoch total loss 1.28755391\n",
      "Trained batch 662 batch loss 1.21304035 epoch total loss 1.28744125\n",
      "Trained batch 663 batch loss 1.37866902 epoch total loss 1.28757882\n",
      "Trained batch 664 batch loss 1.30026138 epoch total loss 1.28759789\n",
      "Trained batch 665 batch loss 1.2301569 epoch total loss 1.28751159\n",
      "Trained batch 666 batch loss 1.3305825 epoch total loss 1.2875762\n",
      "Trained batch 667 batch loss 1.46841466 epoch total loss 1.2878474\n",
      "Trained batch 668 batch loss 1.21323729 epoch total loss 1.2877357\n",
      "Trained batch 669 batch loss 1.31059694 epoch total loss 1.28776991\n",
      "Trained batch 670 batch loss 1.32487714 epoch total loss 1.28782535\n",
      "Trained batch 671 batch loss 1.2653302 epoch total loss 1.28779173\n",
      "Trained batch 672 batch loss 1.25641894 epoch total loss 1.28774512\n",
      "Trained batch 673 batch loss 1.24098861 epoch total loss 1.28767562\n",
      "Trained batch 674 batch loss 1.37170982 epoch total loss 1.28780019\n",
      "Trained batch 675 batch loss 1.30876517 epoch total loss 1.28783131\n",
      "Trained batch 676 batch loss 1.2215625 epoch total loss 1.28773332\n",
      "Trained batch 677 batch loss 1.37890685 epoch total loss 1.2878679\n",
      "Trained batch 678 batch loss 1.30090547 epoch total loss 1.28788722\n",
      "Trained batch 679 batch loss 1.28030884 epoch total loss 1.28787601\n",
      "Trained batch 680 batch loss 1.27917182 epoch total loss 1.28786325\n",
      "Trained batch 681 batch loss 1.25137377 epoch total loss 1.28780973\n",
      "Trained batch 682 batch loss 1.22359157 epoch total loss 1.28771555\n",
      "Trained batch 683 batch loss 1.20931125 epoch total loss 1.28760064\n",
      "Trained batch 684 batch loss 1.20564866 epoch total loss 1.28748083\n",
      "Trained batch 685 batch loss 1.34001756 epoch total loss 1.2875576\n",
      "Trained batch 686 batch loss 1.48576069 epoch total loss 1.28784657\n",
      "Trained batch 687 batch loss 1.41231716 epoch total loss 1.28802764\n",
      "Trained batch 688 batch loss 1.49970257 epoch total loss 1.28833532\n",
      "Trained batch 689 batch loss 1.37033308 epoch total loss 1.28845441\n",
      "Trained batch 690 batch loss 1.29215169 epoch total loss 1.28845978\n",
      "Trained batch 691 batch loss 1.082407 epoch total loss 1.28816152\n",
      "Trained batch 692 batch loss 1.05052066 epoch total loss 1.28781819\n",
      "Trained batch 693 batch loss 1.06483376 epoch total loss 1.28749633\n",
      "Trained batch 694 batch loss 1.29078984 epoch total loss 1.2875011\n",
      "Trained batch 695 batch loss 1.35600913 epoch total loss 1.28759968\n",
      "Trained batch 696 batch loss 1.47430348 epoch total loss 1.2878679\n",
      "Trained batch 697 batch loss 1.23095942 epoch total loss 1.28778625\n",
      "Trained batch 698 batch loss 1.20145464 epoch total loss 1.28766263\n",
      "Trained batch 699 batch loss 1.22288442 epoch total loss 1.28757\n",
      "Trained batch 700 batch loss 1.21210754 epoch total loss 1.28746212\n",
      "Trained batch 701 batch loss 1.24295545 epoch total loss 1.2873987\n",
      "Trained batch 702 batch loss 1.27653563 epoch total loss 1.2873832\n",
      "Trained batch 703 batch loss 1.39409161 epoch total loss 1.28753507\n",
      "Trained batch 704 batch loss 1.37021506 epoch total loss 1.28765249\n",
      "Trained batch 705 batch loss 1.44691622 epoch total loss 1.28787839\n",
      "Trained batch 706 batch loss 1.41976953 epoch total loss 1.28806531\n",
      "Trained batch 707 batch loss 1.28235853 epoch total loss 1.28805721\n",
      "Trained batch 708 batch loss 1.27229106 epoch total loss 1.28803492\n",
      "Trained batch 709 batch loss 1.25269806 epoch total loss 1.28798509\n",
      "Trained batch 710 batch loss 1.31993449 epoch total loss 1.28803\n",
      "Trained batch 711 batch loss 1.29902947 epoch total loss 1.28804553\n",
      "Trained batch 712 batch loss 1.22071874 epoch total loss 1.28795087\n",
      "Trained batch 713 batch loss 1.19537258 epoch total loss 1.28782105\n",
      "Trained batch 714 batch loss 1.15622258 epoch total loss 1.28763676\n",
      "Trained batch 715 batch loss 1.18654799 epoch total loss 1.28749537\n",
      "Trained batch 716 batch loss 1.29314458 epoch total loss 1.28750324\n",
      "Trained batch 717 batch loss 1.26976514 epoch total loss 1.28747857\n",
      "Trained batch 718 batch loss 1.30017579 epoch total loss 1.28749621\n",
      "Trained batch 719 batch loss 1.26170421 epoch total loss 1.28746045\n",
      "Trained batch 720 batch loss 1.19138849 epoch total loss 1.28732693\n",
      "Trained batch 721 batch loss 1.26539373 epoch total loss 1.28729653\n",
      "Trained batch 722 batch loss 1.22820258 epoch total loss 1.28721476\n",
      "Trained batch 723 batch loss 1.1887213 epoch total loss 1.2870785\n",
      "Trained batch 724 batch loss 1.29850292 epoch total loss 1.28709424\n",
      "Trained batch 725 batch loss 1.25307238 epoch total loss 1.28704739\n",
      "Trained batch 726 batch loss 1.26967168 epoch total loss 1.28702343\n",
      "Trained batch 727 batch loss 1.23790979 epoch total loss 1.28695583\n",
      "Trained batch 728 batch loss 1.25476122 epoch total loss 1.28691161\n",
      "Trained batch 729 batch loss 1.22425056 epoch total loss 1.28682566\n",
      "Trained batch 730 batch loss 1.2343998 epoch total loss 1.28675377\n",
      "Trained batch 731 batch loss 1.24596977 epoch total loss 1.28669798\n",
      "Trained batch 732 batch loss 1.17745316 epoch total loss 1.28654873\n",
      "Trained batch 733 batch loss 1.27489674 epoch total loss 1.28653288\n",
      "Trained batch 734 batch loss 1.21718335 epoch total loss 1.28643835\n",
      "Trained batch 735 batch loss 1.15196061 epoch total loss 1.28625536\n",
      "Trained batch 736 batch loss 1.24223077 epoch total loss 1.28619564\n",
      "Trained batch 737 batch loss 1.30649507 epoch total loss 1.28622317\n",
      "Trained batch 738 batch loss 1.25673985 epoch total loss 1.28618324\n",
      "Trained batch 739 batch loss 1.26568794 epoch total loss 1.28615546\n",
      "Trained batch 740 batch loss 1.55499899 epoch total loss 1.28651869\n",
      "Trained batch 741 batch loss 1.25532055 epoch total loss 1.28647661\n",
      "Trained batch 742 batch loss 1.08702457 epoch total loss 1.2862078\n",
      "Trained batch 743 batch loss 1.10940802 epoch total loss 1.28597\n",
      "Trained batch 744 batch loss 1.19654489 epoch total loss 1.28584969\n",
      "Trained batch 745 batch loss 1.25963902 epoch total loss 1.28581452\n",
      "Trained batch 746 batch loss 1.44254375 epoch total loss 1.28602469\n",
      "Trained batch 747 batch loss 1.30602646 epoch total loss 1.28605139\n",
      "Trained batch 748 batch loss 1.32509732 epoch total loss 1.28610361\n",
      "Trained batch 749 batch loss 1.24148524 epoch total loss 1.286044\n",
      "Trained batch 750 batch loss 1.25666714 epoch total loss 1.28600478\n",
      "Trained batch 751 batch loss 1.20013881 epoch total loss 1.28589046\n",
      "Trained batch 752 batch loss 1.30807853 epoch total loss 1.28592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 753 batch loss 1.39200163 epoch total loss 1.28606093\n",
      "Trained batch 754 batch loss 1.36212933 epoch total loss 1.28616178\n",
      "Trained batch 755 batch loss 1.35644734 epoch total loss 1.28625488\n",
      "Trained batch 756 batch loss 1.25755525 epoch total loss 1.28621697\n",
      "Trained batch 757 batch loss 1.17849243 epoch total loss 1.28607464\n",
      "Trained batch 758 batch loss 1.22190809 epoch total loss 1.28599\n",
      "Trained batch 759 batch loss 1.36946225 epoch total loss 1.28609991\n",
      "Trained batch 760 batch loss 1.33153713 epoch total loss 1.28615975\n",
      "Trained batch 761 batch loss 1.28288448 epoch total loss 1.28615546\n",
      "Trained batch 762 batch loss 1.34569848 epoch total loss 1.28623354\n",
      "Trained batch 763 batch loss 1.36137152 epoch total loss 1.28633213\n",
      "Trained batch 764 batch loss 1.20653641 epoch total loss 1.28622758\n",
      "Trained batch 765 batch loss 1.11405969 epoch total loss 1.28600264\n",
      "Trained batch 766 batch loss 1.10086608 epoch total loss 1.285761\n",
      "Trained batch 767 batch loss 1.27721906 epoch total loss 1.28574979\n",
      "Trained batch 768 batch loss 1.37538326 epoch total loss 1.2858665\n",
      "Trained batch 769 batch loss 1.35164177 epoch total loss 1.28595197\n",
      "Trained batch 770 batch loss 1.51614547 epoch total loss 1.28625095\n",
      "Trained batch 771 batch loss 1.3986305 epoch total loss 1.28639674\n",
      "Trained batch 772 batch loss 1.29213667 epoch total loss 1.28640413\n",
      "Trained batch 773 batch loss 1.32061517 epoch total loss 1.28644836\n",
      "Trained batch 774 batch loss 1.35705841 epoch total loss 1.28653967\n",
      "Trained batch 775 batch loss 1.345469 epoch total loss 1.28661561\n",
      "Trained batch 776 batch loss 1.29685724 epoch total loss 1.28662884\n",
      "Trained batch 777 batch loss 1.32533622 epoch total loss 1.28667867\n",
      "Trained batch 778 batch loss 1.38427353 epoch total loss 1.28680408\n",
      "Trained batch 779 batch loss 1.33270979 epoch total loss 1.28686309\n",
      "Trained batch 780 batch loss 1.33400393 epoch total loss 1.28692341\n",
      "Trained batch 781 batch loss 1.31571364 epoch total loss 1.28696036\n",
      "Trained batch 782 batch loss 1.30086184 epoch total loss 1.28697813\n",
      "Trained batch 783 batch loss 1.26712811 epoch total loss 1.28695273\n",
      "Trained batch 784 batch loss 1.15576386 epoch total loss 1.28678548\n",
      "Trained batch 785 batch loss 1.12385714 epoch total loss 1.28657782\n",
      "Trained batch 786 batch loss 1.28531 epoch total loss 1.28657627\n",
      "Trained batch 787 batch loss 1.19839096 epoch total loss 1.28646421\n",
      "Trained batch 788 batch loss 1.2850982 epoch total loss 1.28646243\n",
      "Trained batch 789 batch loss 1.30811918 epoch total loss 1.28648984\n",
      "Trained batch 790 batch loss 1.24809325 epoch total loss 1.28644133\n",
      "Trained batch 791 batch loss 1.2429539 epoch total loss 1.28638637\n",
      "Trained batch 792 batch loss 1.24657273 epoch total loss 1.28633606\n",
      "Trained batch 793 batch loss 1.31822729 epoch total loss 1.28637636\n",
      "Trained batch 794 batch loss 1.29247952 epoch total loss 1.28638399\n",
      "Trained batch 795 batch loss 1.24295473 epoch total loss 1.28632939\n",
      "Trained batch 796 batch loss 1.26958346 epoch total loss 1.28630841\n",
      "Trained batch 797 batch loss 1.27065933 epoch total loss 1.28628874\n",
      "Trained batch 798 batch loss 1.18579316 epoch total loss 1.28616285\n",
      "Trained batch 799 batch loss 1.13934851 epoch total loss 1.28597915\n",
      "Trained batch 800 batch loss 1.35360324 epoch total loss 1.28606367\n",
      "Trained batch 801 batch loss 1.24996543 epoch total loss 1.28601861\n",
      "Trained batch 802 batch loss 1.24431145 epoch total loss 1.28596663\n",
      "Trained batch 803 batch loss 1.06941295 epoch total loss 1.28569698\n",
      "Trained batch 804 batch loss 1.00416434 epoch total loss 1.28534675\n",
      "Trained batch 805 batch loss 1.07719123 epoch total loss 1.28508818\n",
      "Trained batch 806 batch loss 1.30669737 epoch total loss 1.28511488\n",
      "Trained batch 807 batch loss 1.46140361 epoch total loss 1.2853334\n",
      "Trained batch 808 batch loss 1.70587325 epoch total loss 1.28585398\n",
      "Trained batch 809 batch loss 1.18925321 epoch total loss 1.28573442\n",
      "Trained batch 810 batch loss 1.32023668 epoch total loss 1.28577697\n",
      "Trained batch 811 batch loss 1.29279876 epoch total loss 1.28578568\n",
      "Trained batch 812 batch loss 1.26029265 epoch total loss 1.28575432\n",
      "Trained batch 813 batch loss 1.3873142 epoch total loss 1.28587925\n",
      "Trained batch 814 batch loss 1.3254354 epoch total loss 1.28592777\n",
      "Trained batch 815 batch loss 1.27071607 epoch total loss 1.28590918\n",
      "Trained batch 816 batch loss 1.37954 epoch total loss 1.28602386\n",
      "Trained batch 817 batch loss 1.23575616 epoch total loss 1.28596234\n",
      "Trained batch 818 batch loss 1.29663038 epoch total loss 1.28597534\n",
      "Trained batch 819 batch loss 1.23499537 epoch total loss 1.28591311\n",
      "Trained batch 820 batch loss 1.19760871 epoch total loss 1.28580546\n",
      "Trained batch 821 batch loss 1.28041863 epoch total loss 1.28579891\n",
      "Trained batch 822 batch loss 1.21702123 epoch total loss 1.28571522\n",
      "Trained batch 823 batch loss 1.20955431 epoch total loss 1.28562272\n",
      "Trained batch 824 batch loss 1.2029177 epoch total loss 1.28552234\n",
      "Trained batch 825 batch loss 1.29707992 epoch total loss 1.28553641\n",
      "Trained batch 826 batch loss 1.24876714 epoch total loss 1.28549182\n",
      "Trained batch 827 batch loss 1.27792454 epoch total loss 1.28548276\n",
      "Trained batch 828 batch loss 1.27250159 epoch total loss 1.28546703\n",
      "Trained batch 829 batch loss 1.36046934 epoch total loss 1.28555751\n",
      "Trained batch 830 batch loss 1.20465362 epoch total loss 1.28546011\n",
      "Trained batch 831 batch loss 1.28158855 epoch total loss 1.28545547\n",
      "Trained batch 832 batch loss 1.26450801 epoch total loss 1.28543031\n",
      "Trained batch 833 batch loss 1.18780398 epoch total loss 1.28531301\n",
      "Trained batch 834 batch loss 1.18520069 epoch total loss 1.28519297\n",
      "Trained batch 835 batch loss 1.37076116 epoch total loss 1.28529537\n",
      "Trained batch 836 batch loss 1.3403722 epoch total loss 1.28536129\n",
      "Trained batch 837 batch loss 1.41008258 epoch total loss 1.28551018\n",
      "Trained batch 838 batch loss 1.26646829 epoch total loss 1.28548753\n",
      "Trained batch 839 batch loss 1.22356582 epoch total loss 1.28541362\n",
      "Trained batch 840 batch loss 1.34944618 epoch total loss 1.28548992\n",
      "Trained batch 841 batch loss 1.21837 epoch total loss 1.28541017\n",
      "Trained batch 842 batch loss 1.23288345 epoch total loss 1.28534782\n",
      "Trained batch 843 batch loss 1.34600174 epoch total loss 1.2854197\n",
      "Trained batch 844 batch loss 1.13650548 epoch total loss 1.28524315\n",
      "Trained batch 845 batch loss 1.22268295 epoch total loss 1.28516912\n",
      "Trained batch 846 batch loss 1.48347473 epoch total loss 1.28540361\n",
      "Trained batch 847 batch loss 1.43008542 epoch total loss 1.28557432\n",
      "Trained batch 848 batch loss 1.23290777 epoch total loss 1.28551221\n",
      "Trained batch 849 batch loss 1.16615629 epoch total loss 1.28537166\n",
      "Trained batch 850 batch loss 1.26514554 epoch total loss 1.28534782\n",
      "Trained batch 851 batch loss 1.31144655 epoch total loss 1.28537846\n",
      "Trained batch 852 batch loss 1.3852365 epoch total loss 1.28549564\n",
      "Trained batch 853 batch loss 1.35436261 epoch total loss 1.28557646\n",
      "Trained batch 854 batch loss 1.27549207 epoch total loss 1.28556466\n",
      "Trained batch 855 batch loss 1.35945249 epoch total loss 1.28565109\n",
      "Trained batch 856 batch loss 1.31953263 epoch total loss 1.28569078\n",
      "Trained batch 857 batch loss 1.28958809 epoch total loss 1.2856952\n",
      "Trained batch 858 batch loss 1.24294949 epoch total loss 1.28564537\n",
      "Trained batch 859 batch loss 1.22162175 epoch total loss 1.28557098\n",
      "Trained batch 860 batch loss 1.33136034 epoch total loss 1.28562427\n",
      "Trained batch 861 batch loss 1.23263621 epoch total loss 1.28556275\n",
      "Trained batch 862 batch loss 1.28903794 epoch total loss 1.28556681\n",
      "Trained batch 863 batch loss 1.27481031 epoch total loss 1.28555429\n",
      "Trained batch 864 batch loss 1.27593207 epoch total loss 1.28554308\n",
      "Trained batch 865 batch loss 1.3684361 epoch total loss 1.28563893\n",
      "Trained batch 866 batch loss 1.3086381 epoch total loss 1.28566539\n",
      "Trained batch 867 batch loss 1.27829492 epoch total loss 1.28565693\n",
      "Trained batch 868 batch loss 1.2985487 epoch total loss 1.28567183\n",
      "Trained batch 869 batch loss 1.33902967 epoch total loss 1.28573322\n",
      "Trained batch 870 batch loss 1.46715784 epoch total loss 1.28594172\n",
      "Trained batch 871 batch loss 1.47346675 epoch total loss 1.28615701\n",
      "Trained batch 872 batch loss 1.51015067 epoch total loss 1.28641391\n",
      "Trained batch 873 batch loss 1.34769583 epoch total loss 1.286484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 874 batch loss 1.26813471 epoch total loss 1.28646314\n",
      "Trained batch 875 batch loss 1.28582764 epoch total loss 1.28646231\n",
      "Trained batch 876 batch loss 1.11197126 epoch total loss 1.28626311\n",
      "Trained batch 877 batch loss 1.34874034 epoch total loss 1.2863344\n",
      "Trained batch 878 batch loss 1.26652741 epoch total loss 1.28631175\n",
      "Trained batch 879 batch loss 1.22425508 epoch total loss 1.28624117\n",
      "Trained batch 880 batch loss 1.12908864 epoch total loss 1.28606248\n",
      "Trained batch 881 batch loss 1.15412438 epoch total loss 1.28591275\n",
      "Trained batch 882 batch loss 1.14245105 epoch total loss 1.28575015\n",
      "Trained batch 883 batch loss 1.29448032 epoch total loss 1.28575993\n",
      "Trained batch 884 batch loss 1.49357224 epoch total loss 1.28599501\n",
      "Trained batch 885 batch loss 1.50478208 epoch total loss 1.28624225\n",
      "Trained batch 886 batch loss 1.51882136 epoch total loss 1.28650463\n",
      "Trained batch 887 batch loss 1.38988352 epoch total loss 1.28662121\n",
      "Trained batch 888 batch loss 1.49634838 epoch total loss 1.28685737\n",
      "Trained batch 889 batch loss 1.42281425 epoch total loss 1.28701043\n",
      "Trained batch 890 batch loss 1.33490229 epoch total loss 1.28706419\n",
      "Trained batch 891 batch loss 1.2923032 epoch total loss 1.28707016\n",
      "Trained batch 892 batch loss 1.30498338 epoch total loss 1.28709018\n",
      "Trained batch 893 batch loss 1.28792441 epoch total loss 1.28709114\n",
      "Trained batch 894 batch loss 1.43440855 epoch total loss 1.287256\n",
      "Trained batch 895 batch loss 1.44742632 epoch total loss 1.28743494\n",
      "Trained batch 896 batch loss 1.31656325 epoch total loss 1.28746736\n",
      "Trained batch 897 batch loss 1.31633472 epoch total loss 1.28749955\n",
      "Trained batch 898 batch loss 1.26928663 epoch total loss 1.28747928\n",
      "Trained batch 899 batch loss 1.26159 epoch total loss 1.28745043\n",
      "Trained batch 900 batch loss 1.25568378 epoch total loss 1.28741527\n",
      "Trained batch 901 batch loss 1.32215536 epoch total loss 1.28745377\n",
      "Trained batch 902 batch loss 1.48941851 epoch total loss 1.28767765\n",
      "Trained batch 903 batch loss 1.4362694 epoch total loss 1.28784215\n",
      "Trained batch 904 batch loss 1.36406255 epoch total loss 1.28792644\n",
      "Trained batch 905 batch loss 1.46985555 epoch total loss 1.28812742\n",
      "Trained batch 906 batch loss 1.3310653 epoch total loss 1.28817487\n",
      "Trained batch 907 batch loss 1.32320487 epoch total loss 1.28821349\n",
      "Trained batch 908 batch loss 1.44346118 epoch total loss 1.28838456\n",
      "Trained batch 909 batch loss 1.24631953 epoch total loss 1.2883383\n",
      "Trained batch 910 batch loss 1.23893738 epoch total loss 1.28828394\n",
      "Trained batch 911 batch loss 1.30282974 epoch total loss 1.28829992\n",
      "Trained batch 912 batch loss 1.14536381 epoch total loss 1.28814328\n",
      "Trained batch 913 batch loss 1.26038468 epoch total loss 1.28811276\n",
      "Trained batch 914 batch loss 1.35818458 epoch total loss 1.28818941\n",
      "Trained batch 915 batch loss 1.40840518 epoch total loss 1.2883209\n",
      "Trained batch 916 batch loss 1.41139317 epoch total loss 1.28845525\n",
      "Trained batch 917 batch loss 1.53653598 epoch total loss 1.28872573\n",
      "Trained batch 918 batch loss 1.28585231 epoch total loss 1.28872263\n",
      "Trained batch 919 batch loss 1.26232266 epoch total loss 1.2886939\n",
      "Trained batch 920 batch loss 1.28854525 epoch total loss 1.28869379\n",
      "Trained batch 921 batch loss 1.34221292 epoch total loss 1.28875184\n",
      "Trained batch 922 batch loss 1.28432465 epoch total loss 1.28874695\n",
      "Trained batch 923 batch loss 1.34740841 epoch total loss 1.28881061\n",
      "Trained batch 924 batch loss 1.40402484 epoch total loss 1.2889353\n",
      "Trained batch 925 batch loss 1.38179517 epoch total loss 1.28903568\n",
      "Trained batch 926 batch loss 1.42363882 epoch total loss 1.28918099\n",
      "Trained batch 927 batch loss 1.50301921 epoch total loss 1.28941178\n",
      "Trained batch 928 batch loss 1.53085685 epoch total loss 1.2896719\n",
      "Trained batch 929 batch loss 1.37374556 epoch total loss 1.2897625\n",
      "Trained batch 930 batch loss 1.44186568 epoch total loss 1.28992605\n",
      "Trained batch 931 batch loss 1.36461103 epoch total loss 1.29000628\n",
      "Trained batch 932 batch loss 1.39256215 epoch total loss 1.29011631\n",
      "Trained batch 933 batch loss 1.31218469 epoch total loss 1.29013991\n",
      "Trained batch 934 batch loss 1.35033917 epoch total loss 1.29020441\n",
      "Trained batch 935 batch loss 1.3085475 epoch total loss 1.29022408\n",
      "Trained batch 936 batch loss 1.33055639 epoch total loss 1.29026711\n",
      "Trained batch 937 batch loss 1.33642495 epoch total loss 1.29031646\n",
      "Trained batch 938 batch loss 1.31475973 epoch total loss 1.29034257\n",
      "Trained batch 939 batch loss 1.30402756 epoch total loss 1.29035723\n",
      "Trained batch 940 batch loss 1.36861897 epoch total loss 1.29044044\n",
      "Trained batch 941 batch loss 1.27671134 epoch total loss 1.2904259\n",
      "Trained batch 942 batch loss 1.36661947 epoch total loss 1.29050672\n",
      "Trained batch 943 batch loss 1.3492353 epoch total loss 1.29056907\n",
      "Trained batch 944 batch loss 1.25804377 epoch total loss 1.29053462\n",
      "Trained batch 945 batch loss 1.3706305 epoch total loss 1.29061937\n",
      "Trained batch 946 batch loss 1.28862762 epoch total loss 1.29061711\n",
      "Trained batch 947 batch loss 1.36594236 epoch total loss 1.29069674\n",
      "Trained batch 948 batch loss 1.33394158 epoch total loss 1.2907424\n",
      "Trained batch 949 batch loss 1.26455903 epoch total loss 1.29071474\n",
      "Trained batch 950 batch loss 1.37012863 epoch total loss 1.29079831\n",
      "Trained batch 951 batch loss 1.30344391 epoch total loss 1.29081166\n",
      "Trained batch 952 batch loss 1.22208738 epoch total loss 1.29073942\n",
      "Trained batch 953 batch loss 1.15871537 epoch total loss 1.2906009\n",
      "Trained batch 954 batch loss 1.17057908 epoch total loss 1.29047501\n",
      "Trained batch 955 batch loss 1.31880879 epoch total loss 1.29050469\n",
      "Trained batch 956 batch loss 1.27090335 epoch total loss 1.29048419\n",
      "Trained batch 957 batch loss 1.07822359 epoch total loss 1.29026246\n",
      "Trained batch 958 batch loss 1.07692468 epoch total loss 1.29003966\n",
      "Trained batch 959 batch loss 1.16366339 epoch total loss 1.28990793\n",
      "Trained batch 960 batch loss 1.14882469 epoch total loss 1.28976095\n",
      "Trained batch 961 batch loss 1.21399307 epoch total loss 1.28968215\n",
      "Trained batch 962 batch loss 1.20747077 epoch total loss 1.28959668\n",
      "Trained batch 963 batch loss 1.18654943 epoch total loss 1.28948963\n",
      "Trained batch 964 batch loss 1.21526885 epoch total loss 1.28941262\n",
      "Trained batch 965 batch loss 1.37599647 epoch total loss 1.28950238\n",
      "Trained batch 966 batch loss 1.34385848 epoch total loss 1.28955865\n",
      "Trained batch 967 batch loss 1.397012 epoch total loss 1.28966975\n",
      "Trained batch 968 batch loss 1.20720661 epoch total loss 1.2895844\n",
      "Trained batch 969 batch loss 1.18717742 epoch total loss 1.28947878\n",
      "Trained batch 970 batch loss 1.23814571 epoch total loss 1.28942585\n",
      "Trained batch 971 batch loss 1.29611206 epoch total loss 1.28943276\n",
      "Trained batch 972 batch loss 1.37686217 epoch total loss 1.28952265\n",
      "Trained batch 973 batch loss 1.36628234 epoch total loss 1.28960156\n",
      "Trained batch 974 batch loss 1.34743381 epoch total loss 1.28966093\n",
      "Trained batch 975 batch loss 1.16242516 epoch total loss 1.28953052\n",
      "Trained batch 976 batch loss 1.29851246 epoch total loss 1.28953969\n",
      "Trained batch 977 batch loss 1.35203969 epoch total loss 1.28960359\n",
      "Trained batch 978 batch loss 1.39775467 epoch total loss 1.28971422\n",
      "Trained batch 979 batch loss 1.43106031 epoch total loss 1.28985846\n",
      "Trained batch 980 batch loss 1.24309301 epoch total loss 1.28981078\n",
      "Trained batch 981 batch loss 1.24414492 epoch total loss 1.28976417\n",
      "Trained batch 982 batch loss 1.30501115 epoch total loss 1.28977978\n",
      "Trained batch 983 batch loss 1.31084192 epoch total loss 1.28980112\n",
      "Trained batch 984 batch loss 1.23551929 epoch total loss 1.28974593\n",
      "Trained batch 985 batch loss 1.23440373 epoch total loss 1.28968966\n",
      "Trained batch 986 batch loss 1.39778948 epoch total loss 1.28979933\n",
      "Trained batch 987 batch loss 1.20307124 epoch total loss 1.28971159\n",
      "Trained batch 988 batch loss 1.24884689 epoch total loss 1.28967023\n",
      "Trained batch 989 batch loss 1.30044389 epoch total loss 1.28968108\n",
      "Trained batch 990 batch loss 1.18356848 epoch total loss 1.28957391\n",
      "Trained batch 991 batch loss 1.21043921 epoch total loss 1.28949416\n",
      "Trained batch 992 batch loss 1.28279352 epoch total loss 1.28948736\n",
      "Trained batch 993 batch loss 1.17272532 epoch total loss 1.28936982\n",
      "Trained batch 994 batch loss 1.14425015 epoch total loss 1.28922391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 995 batch loss 1.17291307 epoch total loss 1.28910708\n",
      "Trained batch 996 batch loss 1.19038725 epoch total loss 1.2890079\n",
      "Trained batch 997 batch loss 1.27759778 epoch total loss 1.28899646\n",
      "Trained batch 998 batch loss 1.29551697 epoch total loss 1.28900301\n",
      "Trained batch 999 batch loss 1.18103647 epoch total loss 1.28889501\n",
      "Trained batch 1000 batch loss 1.19435668 epoch total loss 1.28880036\n",
      "Trained batch 1001 batch loss 1.16752064 epoch total loss 1.28867924\n",
      "Trained batch 1002 batch loss 1.21413219 epoch total loss 1.28860486\n",
      "Trained batch 1003 batch loss 1.24249899 epoch total loss 1.28855884\n",
      "Trained batch 1004 batch loss 1.33971572 epoch total loss 1.28860986\n",
      "Trained batch 1005 batch loss 1.38607907 epoch total loss 1.2887069\n",
      "Trained batch 1006 batch loss 1.32655442 epoch total loss 1.28874445\n",
      "Trained batch 1007 batch loss 1.30926037 epoch total loss 1.28876483\n",
      "Trained batch 1008 batch loss 1.27535248 epoch total loss 1.28875148\n",
      "Trained batch 1009 batch loss 1.33976269 epoch total loss 1.28880203\n",
      "Trained batch 1010 batch loss 1.24818039 epoch total loss 1.28876185\n",
      "Trained batch 1011 batch loss 1.29077578 epoch total loss 1.28876376\n",
      "Trained batch 1012 batch loss 1.3784672 epoch total loss 1.28885233\n",
      "Trained batch 1013 batch loss 1.2687968 epoch total loss 1.28883255\n",
      "Trained batch 1014 batch loss 1.30837631 epoch total loss 1.28885186\n",
      "Trained batch 1015 batch loss 1.36145365 epoch total loss 1.28892338\n",
      "Trained batch 1016 batch loss 1.39809453 epoch total loss 1.28903079\n",
      "Trained batch 1017 batch loss 1.51566 epoch total loss 1.28925359\n",
      "Trained batch 1018 batch loss 1.21844792 epoch total loss 1.28918409\n",
      "Trained batch 1019 batch loss 1.24858356 epoch total loss 1.28914416\n",
      "Trained batch 1020 batch loss 1.32291067 epoch total loss 1.2891773\n",
      "Trained batch 1021 batch loss 1.24679124 epoch total loss 1.28913581\n",
      "Trained batch 1022 batch loss 1.34500766 epoch total loss 1.28919041\n",
      "Trained batch 1023 batch loss 1.21772909 epoch total loss 1.28912055\n",
      "Trained batch 1024 batch loss 1.23994374 epoch total loss 1.28907263\n",
      "Trained batch 1025 batch loss 1.23293138 epoch total loss 1.2890178\n",
      "Trained batch 1026 batch loss 1.20372653 epoch total loss 1.28893471\n",
      "Trained batch 1027 batch loss 1.21229 epoch total loss 1.28886008\n",
      "Trained batch 1028 batch loss 1.28211904 epoch total loss 1.28885353\n",
      "Trained batch 1029 batch loss 1.27472591 epoch total loss 1.28883982\n",
      "Trained batch 1030 batch loss 1.39307797 epoch total loss 1.28894103\n",
      "Trained batch 1031 batch loss 1.41475713 epoch total loss 1.2890631\n",
      "Trained batch 1032 batch loss 1.36951685 epoch total loss 1.28914106\n",
      "Trained batch 1033 batch loss 1.35595655 epoch total loss 1.28920567\n",
      "Trained batch 1034 batch loss 1.32933223 epoch total loss 1.28924453\n",
      "Trained batch 1035 batch loss 1.27194989 epoch total loss 1.28922784\n",
      "Trained batch 1036 batch loss 1.36135554 epoch total loss 1.28929746\n",
      "Trained batch 1037 batch loss 1.17788231 epoch total loss 1.28918993\n",
      "Trained batch 1038 batch loss 1.22854435 epoch total loss 1.28913152\n",
      "Trained batch 1039 batch loss 1.19982457 epoch total loss 1.28904557\n",
      "Trained batch 1040 batch loss 1.19258237 epoch total loss 1.28895283\n",
      "Trained batch 1041 batch loss 1.16042233 epoch total loss 1.28882933\n",
      "Trained batch 1042 batch loss 1.26761186 epoch total loss 1.28880894\n",
      "Trained batch 1043 batch loss 1.28116548 epoch total loss 1.28880167\n",
      "Trained batch 1044 batch loss 1.01374519 epoch total loss 1.28853822\n",
      "Trained batch 1045 batch loss 1.14789271 epoch total loss 1.28840363\n",
      "Trained batch 1046 batch loss 1.15328431 epoch total loss 1.28827453\n",
      "Trained batch 1047 batch loss 1.1980772 epoch total loss 1.28818846\n",
      "Trained batch 1048 batch loss 1.15962744 epoch total loss 1.28806579\n",
      "Trained batch 1049 batch loss 1.18297243 epoch total loss 1.28796566\n",
      "Trained batch 1050 batch loss 1.11038923 epoch total loss 1.2877965\n",
      "Trained batch 1051 batch loss 1.14756632 epoch total loss 1.2876631\n",
      "Trained batch 1052 batch loss 1.20603228 epoch total loss 1.2875855\n",
      "Trained batch 1053 batch loss 1.13545358 epoch total loss 1.28744102\n",
      "Trained batch 1054 batch loss 1.23484433 epoch total loss 1.28739119\n",
      "Trained batch 1055 batch loss 1.24448073 epoch total loss 1.28735054\n",
      "Trained batch 1056 batch loss 1.28786075 epoch total loss 1.28735101\n",
      "Trained batch 1057 batch loss 1.43615687 epoch total loss 1.2874918\n",
      "Trained batch 1058 batch loss 1.25967062 epoch total loss 1.28746545\n",
      "Trained batch 1059 batch loss 1.29286432 epoch total loss 1.28747046\n",
      "Trained batch 1060 batch loss 1.2768178 epoch total loss 1.28746045\n",
      "Trained batch 1061 batch loss 1.23814297 epoch total loss 1.28741407\n",
      "Trained batch 1062 batch loss 1.20451963 epoch total loss 1.28733587\n",
      "Trained batch 1063 batch loss 1.14520442 epoch total loss 1.28720224\n",
      "Trained batch 1064 batch loss 1.18801105 epoch total loss 1.28710902\n",
      "Trained batch 1065 batch loss 1.16999638 epoch total loss 1.28699911\n",
      "Trained batch 1066 batch loss 1.18191898 epoch total loss 1.28690052\n",
      "Trained batch 1067 batch loss 1.19611502 epoch total loss 1.28681552\n",
      "Trained batch 1068 batch loss 1.3233875 epoch total loss 1.28684974\n",
      "Trained batch 1069 batch loss 1.23898327 epoch total loss 1.28680491\n",
      "Trained batch 1070 batch loss 1.28871322 epoch total loss 1.2868067\n",
      "Trained batch 1071 batch loss 1.27508605 epoch total loss 1.28679585\n",
      "Trained batch 1072 batch loss 1.40417135 epoch total loss 1.28690529\n",
      "Trained batch 1073 batch loss 1.32057309 epoch total loss 1.28693664\n",
      "Trained batch 1074 batch loss 1.37726474 epoch total loss 1.2870208\n",
      "Trained batch 1075 batch loss 1.24170363 epoch total loss 1.28697872\n",
      "Trained batch 1076 batch loss 1.15787768 epoch total loss 1.28685868\n",
      "Trained batch 1077 batch loss 1.15941465 epoch total loss 1.2867403\n",
      "Trained batch 1078 batch loss 1.34721398 epoch total loss 1.28679633\n",
      "Trained batch 1079 batch loss 1.30863142 epoch total loss 1.2868166\n",
      "Trained batch 1080 batch loss 1.28957295 epoch total loss 1.2868191\n",
      "Trained batch 1081 batch loss 1.41440821 epoch total loss 1.28693712\n",
      "Trained batch 1082 batch loss 1.30070853 epoch total loss 1.28694987\n",
      "Trained batch 1083 batch loss 1.17915595 epoch total loss 1.28685033\n",
      "Trained batch 1084 batch loss 1.22066486 epoch total loss 1.2867893\n",
      "Trained batch 1085 batch loss 1.15335608 epoch total loss 1.28666627\n",
      "Trained batch 1086 batch loss 1.19162226 epoch total loss 1.28657877\n",
      "Trained batch 1087 batch loss 1.24055231 epoch total loss 1.28653657\n",
      "Trained batch 1088 batch loss 1.25235212 epoch total loss 1.2865051\n",
      "Trained batch 1089 batch loss 1.25699091 epoch total loss 1.28647792\n",
      "Trained batch 1090 batch loss 1.2527709 epoch total loss 1.28644705\n",
      "Trained batch 1091 batch loss 1.36093235 epoch total loss 1.28651536\n",
      "Trained batch 1092 batch loss 1.39495957 epoch total loss 1.28661478\n",
      "Trained batch 1093 batch loss 1.4089427 epoch total loss 1.28672659\n",
      "Trained batch 1094 batch loss 1.41195226 epoch total loss 1.28684115\n",
      "Trained batch 1095 batch loss 1.26528931 epoch total loss 1.28682137\n",
      "Trained batch 1096 batch loss 1.20764256 epoch total loss 1.28674912\n",
      "Trained batch 1097 batch loss 1.28002393 epoch total loss 1.28674304\n",
      "Trained batch 1098 batch loss 1.34450543 epoch total loss 1.28679562\n",
      "Trained batch 1099 batch loss 1.25671196 epoch total loss 1.28676832\n",
      "Trained batch 1100 batch loss 1.26043439 epoch total loss 1.28674424\n",
      "Trained batch 1101 batch loss 1.16257596 epoch total loss 1.28663146\n",
      "Trained batch 1102 batch loss 1.19160545 epoch total loss 1.28654528\n",
      "Trained batch 1103 batch loss 1.17729449 epoch total loss 1.28644621\n",
      "Trained batch 1104 batch loss 1.14496255 epoch total loss 1.28631818\n",
      "Trained batch 1105 batch loss 1.17494643 epoch total loss 1.28621733\n",
      "Trained batch 1106 batch loss 1.34994984 epoch total loss 1.28627503\n",
      "Trained batch 1107 batch loss 1.38065088 epoch total loss 1.28636014\n",
      "Trained batch 1108 batch loss 1.33776331 epoch total loss 1.28640664\n",
      "Trained batch 1109 batch loss 1.3577354 epoch total loss 1.28647101\n",
      "Trained batch 1110 batch loss 1.35217142 epoch total loss 1.28653014\n",
      "Trained batch 1111 batch loss 1.2162106 epoch total loss 1.28646684\n",
      "Trained batch 1112 batch loss 1.24172306 epoch total loss 1.28642654\n",
      "Trained batch 1113 batch loss 1.23695326 epoch total loss 1.28638208\n",
      "Trained batch 1114 batch loss 1.29712749 epoch total loss 1.28639174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1115 batch loss 1.29042435 epoch total loss 1.28639531\n",
      "Trained batch 1116 batch loss 1.18699682 epoch total loss 1.28630626\n",
      "Trained batch 1117 batch loss 1.24137151 epoch total loss 1.28626609\n",
      "Trained batch 1118 batch loss 1.39506817 epoch total loss 1.28636336\n",
      "Trained batch 1119 batch loss 1.11881328 epoch total loss 1.28621352\n",
      "Trained batch 1120 batch loss 1.27810454 epoch total loss 1.28620625\n",
      "Trained batch 1121 batch loss 1.28307581 epoch total loss 1.2862035\n",
      "Trained batch 1122 batch loss 1.25310659 epoch total loss 1.28617394\n",
      "Trained batch 1123 batch loss 1.32267213 epoch total loss 1.28620636\n",
      "Trained batch 1124 batch loss 1.28571272 epoch total loss 1.28620601\n",
      "Trained batch 1125 batch loss 1.19855547 epoch total loss 1.28612816\n",
      "Trained batch 1126 batch loss 1.14662373 epoch total loss 1.28600419\n",
      "Trained batch 1127 batch loss 1.35601759 epoch total loss 1.28606629\n",
      "Trained batch 1128 batch loss 1.35648596 epoch total loss 1.28612876\n",
      "Trained batch 1129 batch loss 1.3254106 epoch total loss 1.28616357\n",
      "Trained batch 1130 batch loss 1.33018541 epoch total loss 1.28620243\n",
      "Trained batch 1131 batch loss 1.08720696 epoch total loss 1.28602648\n",
      "Trained batch 1132 batch loss 1.14593554 epoch total loss 1.28590286\n",
      "Trained batch 1133 batch loss 1.19040179 epoch total loss 1.28581858\n",
      "Trained batch 1134 batch loss 1.13163328 epoch total loss 1.28568256\n",
      "Trained batch 1135 batch loss 1.19599164 epoch total loss 1.28560352\n",
      "Trained batch 1136 batch loss 1.21772861 epoch total loss 1.2855438\n",
      "Trained batch 1137 batch loss 1.31254113 epoch total loss 1.28556752\n",
      "Trained batch 1138 batch loss 1.27141523 epoch total loss 1.28555512\n",
      "Trained batch 1139 batch loss 1.2461803 epoch total loss 1.28552055\n",
      "Trained batch 1140 batch loss 1.31578183 epoch total loss 1.28554714\n",
      "Trained batch 1141 batch loss 1.39427245 epoch total loss 1.28564239\n",
      "Trained batch 1142 batch loss 1.49867082 epoch total loss 1.28582895\n",
      "Trained batch 1143 batch loss 1.52818525 epoch total loss 1.28604102\n",
      "Trained batch 1144 batch loss 1.3652091 epoch total loss 1.28611016\n",
      "Trained batch 1145 batch loss 1.12555707 epoch total loss 1.28597\n",
      "Trained batch 1146 batch loss 1.25212705 epoch total loss 1.28594041\n",
      "Trained batch 1147 batch loss 1.41345286 epoch total loss 1.28605163\n",
      "Trained batch 1148 batch loss 1.48612273 epoch total loss 1.2862258\n",
      "Trained batch 1149 batch loss 1.43255186 epoch total loss 1.28635311\n",
      "Trained batch 1150 batch loss 1.39896584 epoch total loss 1.2864511\n",
      "Trained batch 1151 batch loss 1.3347472 epoch total loss 1.28649294\n",
      "Trained batch 1152 batch loss 1.32627678 epoch total loss 1.28652751\n",
      "Trained batch 1153 batch loss 1.27902102 epoch total loss 1.28652108\n",
      "Trained batch 1154 batch loss 1.33309829 epoch total loss 1.28656149\n",
      "Trained batch 1155 batch loss 1.3481611 epoch total loss 1.28661478\n",
      "Trained batch 1156 batch loss 1.27439809 epoch total loss 1.28660417\n",
      "Trained batch 1157 batch loss 1.37685275 epoch total loss 1.28668213\n",
      "Trained batch 1158 batch loss 1.32606673 epoch total loss 1.28671622\n",
      "Trained batch 1159 batch loss 1.26783609 epoch total loss 1.28669989\n",
      "Trained batch 1160 batch loss 1.34062958 epoch total loss 1.28674626\n",
      "Trained batch 1161 batch loss 1.34976935 epoch total loss 1.28680062\n",
      "Trained batch 1162 batch loss 1.28931248 epoch total loss 1.28680277\n",
      "Trained batch 1163 batch loss 1.20210361 epoch total loss 1.28672993\n",
      "Trained batch 1164 batch loss 1.26469386 epoch total loss 1.28671098\n",
      "Trained batch 1165 batch loss 1.09265578 epoch total loss 1.28654444\n",
      "Trained batch 1166 batch loss 1.13971817 epoch total loss 1.28641856\n",
      "Trained batch 1167 batch loss 1.16227555 epoch total loss 1.2863121\n",
      "Trained batch 1168 batch loss 1.1897018 epoch total loss 1.28622937\n",
      "Trained batch 1169 batch loss 1.24125552 epoch total loss 1.28619087\n",
      "Trained batch 1170 batch loss 1.07090747 epoch total loss 1.28600693\n",
      "Trained batch 1171 batch loss 1.32235992 epoch total loss 1.28603792\n",
      "Trained batch 1172 batch loss 1.29636598 epoch total loss 1.28604674\n",
      "Trained batch 1173 batch loss 1.25750065 epoch total loss 1.28602242\n",
      "Trained batch 1174 batch loss 1.33217549 epoch total loss 1.28606164\n",
      "Trained batch 1175 batch loss 1.22847557 epoch total loss 1.28601277\n",
      "Trained batch 1176 batch loss 1.43793476 epoch total loss 1.28614199\n",
      "Trained batch 1177 batch loss 1.36552167 epoch total loss 1.28620934\n",
      "Trained batch 1178 batch loss 1.16483974 epoch total loss 1.28610623\n",
      "Trained batch 1179 batch loss 1.37795365 epoch total loss 1.28618419\n",
      "Trained batch 1180 batch loss 1.42630553 epoch total loss 1.28630292\n",
      "Trained batch 1181 batch loss 1.3545891 epoch total loss 1.28636074\n",
      "Trained batch 1182 batch loss 1.22115278 epoch total loss 1.28630555\n",
      "Trained batch 1183 batch loss 1.26176429 epoch total loss 1.2862848\n",
      "Trained batch 1184 batch loss 1.23795569 epoch total loss 1.28624392\n",
      "Trained batch 1185 batch loss 1.23434341 epoch total loss 1.28620017\n",
      "Trained batch 1186 batch loss 1.25409389 epoch total loss 1.28617311\n",
      "Trained batch 1187 batch loss 1.27281141 epoch total loss 1.2861619\n",
      "Trained batch 1188 batch loss 1.31729603 epoch total loss 1.28618813\n",
      "Trained batch 1189 batch loss 1.16092 epoch total loss 1.28608274\n",
      "Trained batch 1190 batch loss 1.25860763 epoch total loss 1.28605962\n",
      "Trained batch 1191 batch loss 1.25187182 epoch total loss 1.28603089\n",
      "Trained batch 1192 batch loss 1.16452622 epoch total loss 1.28592896\n",
      "Trained batch 1193 batch loss 1.25204372 epoch total loss 1.28590059\n",
      "Trained batch 1194 batch loss 1.41422832 epoch total loss 1.28600812\n",
      "Trained batch 1195 batch loss 1.181741 epoch total loss 1.28592086\n",
      "Trained batch 1196 batch loss 1.21369362 epoch total loss 1.28586054\n",
      "Trained batch 1197 batch loss 1.37167716 epoch total loss 1.28593218\n",
      "Trained batch 1198 batch loss 1.1490227 epoch total loss 1.28581798\n",
      "Trained batch 1199 batch loss 1.31408978 epoch total loss 1.28584158\n",
      "Trained batch 1200 batch loss 1.26738155 epoch total loss 1.28582609\n",
      "Trained batch 1201 batch loss 1.3561846 epoch total loss 1.28588474\n",
      "Trained batch 1202 batch loss 1.34829187 epoch total loss 1.28593659\n",
      "Trained batch 1203 batch loss 1.10906827 epoch total loss 1.28578949\n",
      "Trained batch 1204 batch loss 1.08815622 epoch total loss 1.28562534\n",
      "Trained batch 1205 batch loss 1.14814961 epoch total loss 1.28551137\n",
      "Trained batch 1206 batch loss 1.1091466 epoch total loss 1.2853651\n",
      "Trained batch 1207 batch loss 1.06402326 epoch total loss 1.28518164\n",
      "Trained batch 1208 batch loss 1.09906781 epoch total loss 1.28502762\n",
      "Trained batch 1209 batch loss 1.24014139 epoch total loss 1.28499043\n",
      "Trained batch 1210 batch loss 1.16728008 epoch total loss 1.28489316\n",
      "Trained batch 1211 batch loss 1.24675035 epoch total loss 1.28486156\n",
      "Trained batch 1212 batch loss 1.36885357 epoch total loss 1.28493094\n",
      "Trained batch 1213 batch loss 1.29045141 epoch total loss 1.28493547\n",
      "Trained batch 1214 batch loss 1.21634066 epoch total loss 1.28487897\n",
      "Trained batch 1215 batch loss 1.14061689 epoch total loss 1.28476024\n",
      "Trained batch 1216 batch loss 1.1549294 epoch total loss 1.28465343\n",
      "Trained batch 1217 batch loss 1.25113118 epoch total loss 1.28462589\n",
      "Trained batch 1218 batch loss 1.32112718 epoch total loss 1.28465581\n",
      "Trained batch 1219 batch loss 1.34749985 epoch total loss 1.28470743\n",
      "Trained batch 1220 batch loss 1.30431199 epoch total loss 1.28472352\n",
      "Trained batch 1221 batch loss 1.25775218 epoch total loss 1.28470147\n",
      "Trained batch 1222 batch loss 1.12215126 epoch total loss 1.28456843\n",
      "Trained batch 1223 batch loss 1.26969361 epoch total loss 1.28455627\n",
      "Trained batch 1224 batch loss 1.24113131 epoch total loss 1.28452075\n",
      "Trained batch 1225 batch loss 1.21567202 epoch total loss 1.2844646\n",
      "Trained batch 1226 batch loss 1.1524117 epoch total loss 1.28435695\n",
      "Trained batch 1227 batch loss 1.20204675 epoch total loss 1.28428984\n",
      "Trained batch 1228 batch loss 1.35893047 epoch total loss 1.28435051\n",
      "Trained batch 1229 batch loss 1.19625092 epoch total loss 1.28427887\n",
      "Trained batch 1230 batch loss 1.31286395 epoch total loss 1.28430212\n",
      "Trained batch 1231 batch loss 1.25459588 epoch total loss 1.28427804\n",
      "Trained batch 1232 batch loss 1.13352108 epoch total loss 1.28415573\n",
      "Trained batch 1233 batch loss 1.04212713 epoch total loss 1.28395939\n",
      "Trained batch 1234 batch loss 1.05555129 epoch total loss 1.28377426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1235 batch loss 1.06531596 epoch total loss 1.28359735\n",
      "Trained batch 1236 batch loss 1.21855688 epoch total loss 1.28354478\n",
      "Trained batch 1237 batch loss 1.16543782 epoch total loss 1.28344929\n",
      "Trained batch 1238 batch loss 1.15539324 epoch total loss 1.28334582\n",
      "Trained batch 1239 batch loss 1.20362854 epoch total loss 1.28328145\n",
      "Trained batch 1240 batch loss 1.15106273 epoch total loss 1.28317487\n",
      "Trained batch 1241 batch loss 1.18463016 epoch total loss 1.28309536\n",
      "Trained batch 1242 batch loss 1.232054 epoch total loss 1.28305435\n",
      "Trained batch 1243 batch loss 1.25758421 epoch total loss 1.28303385\n",
      "Trained batch 1244 batch loss 1.15009 epoch total loss 1.28292704\n",
      "Trained batch 1245 batch loss 1.13998127 epoch total loss 1.28281224\n",
      "Trained batch 1246 batch loss 0.923448443 epoch total loss 1.28252375\n",
      "Trained batch 1247 batch loss 1.0538702 epoch total loss 1.28234041\n",
      "Trained batch 1248 batch loss 1.35592246 epoch total loss 1.28239942\n",
      "Trained batch 1249 batch loss 1.28367794 epoch total loss 1.28240049\n",
      "Trained batch 1250 batch loss 1.32999456 epoch total loss 1.28243852\n",
      "Trained batch 1251 batch loss 1.21618938 epoch total loss 1.28238547\n",
      "Trained batch 1252 batch loss 1.24883723 epoch total loss 1.28235865\n",
      "Trained batch 1253 batch loss 1.14307511 epoch total loss 1.28224754\n",
      "Trained batch 1254 batch loss 1.19696689 epoch total loss 1.28217959\n",
      "Trained batch 1255 batch loss 1.09475529 epoch total loss 1.28203022\n",
      "Trained batch 1256 batch loss 1.26397133 epoch total loss 1.2820158\n",
      "Trained batch 1257 batch loss 1.28880537 epoch total loss 1.28202116\n",
      "Trained batch 1258 batch loss 1.25213599 epoch total loss 1.28199732\n",
      "Trained batch 1259 batch loss 1.29582381 epoch total loss 1.28200829\n",
      "Trained batch 1260 batch loss 1.13924193 epoch total loss 1.28189504\n",
      "Trained batch 1261 batch loss 1.07287228 epoch total loss 1.28172922\n",
      "Trained batch 1262 batch loss 1.0457809 epoch total loss 1.2815423\n",
      "Trained batch 1263 batch loss 1.12708426 epoch total loss 1.28142\n",
      "Trained batch 1264 batch loss 1.28262591 epoch total loss 1.28142095\n",
      "Trained batch 1265 batch loss 1.30093169 epoch total loss 1.28143632\n",
      "Trained batch 1266 batch loss 1.4221561 epoch total loss 1.28154743\n",
      "Trained batch 1267 batch loss 1.39099622 epoch total loss 1.28163385\n",
      "Trained batch 1268 batch loss 1.25039124 epoch total loss 1.28160918\n",
      "Trained batch 1269 batch loss 1.15473628 epoch total loss 1.28150928\n",
      "Trained batch 1270 batch loss 1.33156908 epoch total loss 1.28154862\n",
      "Trained batch 1271 batch loss 1.25033939 epoch total loss 1.28152406\n",
      "Trained batch 1272 batch loss 1.29187489 epoch total loss 1.28153229\n",
      "Trained batch 1273 batch loss 1.34216487 epoch total loss 1.28157985\n",
      "Trained batch 1274 batch loss 1.40749598 epoch total loss 1.28167868\n",
      "Trained batch 1275 batch loss 1.43087614 epoch total loss 1.28179574\n",
      "Trained batch 1276 batch loss 1.31988013 epoch total loss 1.28182554\n",
      "Trained batch 1277 batch loss 1.2501049 epoch total loss 1.28180075\n",
      "Trained batch 1278 batch loss 1.18920541 epoch total loss 1.28172827\n",
      "Trained batch 1279 batch loss 1.17668343 epoch total loss 1.28164613\n",
      "Trained batch 1280 batch loss 1.47177982 epoch total loss 1.28179467\n",
      "Trained batch 1281 batch loss 1.33080876 epoch total loss 1.28183293\n",
      "Trained batch 1282 batch loss 1.37449288 epoch total loss 1.28190517\n",
      "Trained batch 1283 batch loss 1.50942969 epoch total loss 1.28208256\n",
      "Trained batch 1284 batch loss 1.32224011 epoch total loss 1.28211379\n",
      "Trained batch 1285 batch loss 1.36440837 epoch total loss 1.28217781\n",
      "Trained batch 1286 batch loss 1.18790245 epoch total loss 1.28210449\n",
      "Trained batch 1287 batch loss 1.2582742 epoch total loss 1.28208601\n",
      "Trained batch 1288 batch loss 1.34291613 epoch total loss 1.28213322\n",
      "Trained batch 1289 batch loss 1.20030248 epoch total loss 1.28206968\n",
      "Trained batch 1290 batch loss 1.31501532 epoch total loss 1.28209531\n",
      "Trained batch 1291 batch loss 1.28988206 epoch total loss 1.28210139\n",
      "Trained batch 1292 batch loss 1.35860562 epoch total loss 1.28216064\n",
      "Trained batch 1293 batch loss 1.36758828 epoch total loss 1.28222668\n",
      "Trained batch 1294 batch loss 1.27687466 epoch total loss 1.28222251\n",
      "Trained batch 1295 batch loss 1.36139488 epoch total loss 1.28228366\n",
      "Trained batch 1296 batch loss 1.18846869 epoch total loss 1.2822113\n",
      "Trained batch 1297 batch loss 1.27306533 epoch total loss 1.28220427\n",
      "Trained batch 1298 batch loss 1.15998268 epoch total loss 1.2821101\n",
      "Trained batch 1299 batch loss 1.36408722 epoch total loss 1.28217328\n",
      "Trained batch 1300 batch loss 1.28251433 epoch total loss 1.28217351\n",
      "Trained batch 1301 batch loss 1.2437768 epoch total loss 1.28214395\n",
      "Trained batch 1302 batch loss 1.39704311 epoch total loss 1.28223228\n",
      "Trained batch 1303 batch loss 1.36904573 epoch total loss 1.28229892\n",
      "Trained batch 1304 batch loss 1.27728534 epoch total loss 1.28229511\n",
      "Trained batch 1305 batch loss 1.35365582 epoch total loss 1.28234971\n",
      "Trained batch 1306 batch loss 1.3354162 epoch total loss 1.28239036\n",
      "Trained batch 1307 batch loss 1.30422854 epoch total loss 1.28240705\n",
      "Trained batch 1308 batch loss 1.20539606 epoch total loss 1.28234828\n",
      "Trained batch 1309 batch loss 1.22483575 epoch total loss 1.28230429\n",
      "Trained batch 1310 batch loss 1.14739418 epoch total loss 1.28220129\n",
      "Trained batch 1311 batch loss 1.03829622 epoch total loss 1.28201532\n",
      "Trained batch 1312 batch loss 1.11335182 epoch total loss 1.28188682\n",
      "Trained batch 1313 batch loss 1.22842479 epoch total loss 1.28184605\n",
      "Trained batch 1314 batch loss 1.00564539 epoch total loss 1.28163576\n",
      "Trained batch 1315 batch loss 1.02395487 epoch total loss 1.28143978\n",
      "Trained batch 1316 batch loss 0.954581797 epoch total loss 1.28119147\n",
      "Trained batch 1317 batch loss 0.899010897 epoch total loss 1.28090131\n",
      "Trained batch 1318 batch loss 1.13825548 epoch total loss 1.28079307\n",
      "Trained batch 1319 batch loss 1.27245188 epoch total loss 1.28078675\n",
      "Trained batch 1320 batch loss 1.24646902 epoch total loss 1.28076077\n",
      "Trained batch 1321 batch loss 1.30742848 epoch total loss 1.28078091\n",
      "Trained batch 1322 batch loss 1.22473156 epoch total loss 1.28073859\n",
      "Trained batch 1323 batch loss 1.25254548 epoch total loss 1.28071725\n",
      "Trained batch 1324 batch loss 1.2464242 epoch total loss 1.28069139\n",
      "Trained batch 1325 batch loss 1.30280697 epoch total loss 1.28070807\n",
      "Trained batch 1326 batch loss 1.39701974 epoch total loss 1.28079581\n",
      "Trained batch 1327 batch loss 1.33618402 epoch total loss 1.28083754\n",
      "Trained batch 1328 batch loss 1.23569846 epoch total loss 1.28080356\n",
      "Trained batch 1329 batch loss 1.39857602 epoch total loss 1.28089213\n",
      "Trained batch 1330 batch loss 1.24830377 epoch total loss 1.28086758\n",
      "Trained batch 1331 batch loss 1.38554251 epoch total loss 1.28094625\n",
      "Trained batch 1332 batch loss 1.2877233 epoch total loss 1.28095138\n",
      "Trained batch 1333 batch loss 1.3390218 epoch total loss 1.28099489\n",
      "Trained batch 1334 batch loss 1.19498324 epoch total loss 1.2809304\n",
      "Trained batch 1335 batch loss 1.49636841 epoch total loss 1.28109169\n",
      "Trained batch 1336 batch loss 1.30565751 epoch total loss 1.28111017\n",
      "Trained batch 1337 batch loss 1.35137165 epoch total loss 1.28116262\n",
      "Trained batch 1338 batch loss 1.24415851 epoch total loss 1.28113496\n",
      "Trained batch 1339 batch loss 1.11069858 epoch total loss 1.28100765\n",
      "Trained batch 1340 batch loss 1.10359812 epoch total loss 1.28087533\n",
      "Trained batch 1341 batch loss 1.22130406 epoch total loss 1.28083086\n",
      "Trained batch 1342 batch loss 1.19471526 epoch total loss 1.28076673\n",
      "Trained batch 1343 batch loss 1.33499086 epoch total loss 1.28080702\n",
      "Trained batch 1344 batch loss 1.36709762 epoch total loss 1.28087127\n",
      "Trained batch 1345 batch loss 1.34213054 epoch total loss 1.28091681\n",
      "Trained batch 1346 batch loss 1.35847104 epoch total loss 1.28097451\n",
      "Trained batch 1347 batch loss 1.26584911 epoch total loss 1.2809633\n",
      "Trained batch 1348 batch loss 1.24162364 epoch total loss 1.2809341\n",
      "Trained batch 1349 batch loss 1.3072015 epoch total loss 1.28095353\n",
      "Trained batch 1350 batch loss 1.27787066 epoch total loss 1.28095126\n",
      "Trained batch 1351 batch loss 1.36387539 epoch total loss 1.28101265\n",
      "Trained batch 1352 batch loss 1.34791541 epoch total loss 1.28106213\n",
      "Trained batch 1353 batch loss 1.19010389 epoch total loss 1.28099489\n",
      "Trained batch 1354 batch loss 1.37921739 epoch total loss 1.28106749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1355 batch loss 1.23262334 epoch total loss 1.28103173\n",
      "Trained batch 1356 batch loss 1.29664683 epoch total loss 1.28104317\n",
      "Trained batch 1357 batch loss 1.25476944 epoch total loss 1.28102386\n",
      "Trained batch 1358 batch loss 1.1748029 epoch total loss 1.28094566\n",
      "Trained batch 1359 batch loss 1.29145026 epoch total loss 1.28095341\n",
      "Trained batch 1360 batch loss 1.31583786 epoch total loss 1.28097904\n",
      "Trained batch 1361 batch loss 1.29946768 epoch total loss 1.28099263\n",
      "Trained batch 1362 batch loss 1.32946742 epoch total loss 1.28102815\n",
      "Trained batch 1363 batch loss 1.31425667 epoch total loss 1.28105247\n",
      "Trained batch 1364 batch loss 1.31049442 epoch total loss 1.28107417\n",
      "Trained batch 1365 batch loss 1.30382288 epoch total loss 1.28109086\n",
      "Trained batch 1366 batch loss 1.24215603 epoch total loss 1.28106236\n",
      "Trained batch 1367 batch loss 1.25057209 epoch total loss 1.28104007\n",
      "Trained batch 1368 batch loss 1.22538 epoch total loss 1.2809993\n",
      "Trained batch 1369 batch loss 1.06525862 epoch total loss 1.28084183\n",
      "Trained batch 1370 batch loss 1.14740086 epoch total loss 1.28074443\n",
      "Trained batch 1371 batch loss 1.19599891 epoch total loss 1.28068268\n",
      "Trained batch 1372 batch loss 1.16195035 epoch total loss 1.28059614\n",
      "Trained batch 1373 batch loss 1.29409933 epoch total loss 1.28060591\n",
      "Trained batch 1374 batch loss 1.20814812 epoch total loss 1.28055322\n",
      "Trained batch 1375 batch loss 1.3432498 epoch total loss 1.28059876\n",
      "Trained batch 1376 batch loss 1.29243112 epoch total loss 1.28060746\n",
      "Trained batch 1377 batch loss 1.21303618 epoch total loss 1.28055835\n",
      "Trained batch 1378 batch loss 1.23637414 epoch total loss 1.28052628\n",
      "Trained batch 1379 batch loss 1.17769468 epoch total loss 1.28045177\n",
      "Trained batch 1380 batch loss 1.27378237 epoch total loss 1.28044689\n",
      "Trained batch 1381 batch loss 1.1366111 epoch total loss 1.2803427\n",
      "Trained batch 1382 batch loss 0.970667958 epoch total loss 1.2801187\n",
      "Trained batch 1383 batch loss 1.006639 epoch total loss 1.27992094\n",
      "Trained batch 1384 batch loss 1.16547787 epoch total loss 1.2798382\n",
      "Trained batch 1385 batch loss 1.28385019 epoch total loss 1.27984107\n",
      "Trained batch 1386 batch loss 1.54935849 epoch total loss 1.2800355\n",
      "Trained batch 1387 batch loss 1.37131667 epoch total loss 1.28010142\n",
      "Trained batch 1388 batch loss 1.37861776 epoch total loss 1.28017235\n",
      "Epoch 3 train loss 1.280172348022461\n",
      "Validated batch 1 batch loss 1.31429362\n",
      "Validated batch 2 batch loss 1.21301746\n",
      "Validated batch 3 batch loss 1.18650961\n",
      "Validated batch 4 batch loss 1.19230807\n",
      "Validated batch 5 batch loss 1.18690121\n",
      "Validated batch 6 batch loss 1.27074027\n",
      "Validated batch 7 batch loss 1.22742105\n",
      "Validated batch 8 batch loss 1.19663525\n",
      "Validated batch 9 batch loss 1.32069409\n",
      "Validated batch 10 batch loss 1.25864029\n",
      "Validated batch 11 batch loss 1.17564178\n",
      "Validated batch 12 batch loss 1.1351254\n",
      "Validated batch 13 batch loss 1.27387917\n",
      "Validated batch 14 batch loss 1.28409266\n",
      "Validated batch 15 batch loss 1.39787674\n",
      "Validated batch 16 batch loss 1.36262429\n",
      "Validated batch 17 batch loss 1.20238519\n",
      "Validated batch 18 batch loss 1.3925066\n",
      "Validated batch 19 batch loss 1.25399327\n",
      "Validated batch 20 batch loss 1.26768446\n",
      "Validated batch 21 batch loss 1.31022811\n",
      "Validated batch 22 batch loss 1.03638864\n",
      "Validated batch 23 batch loss 1.29299664\n",
      "Validated batch 24 batch loss 1.21271515\n",
      "Validated batch 25 batch loss 1.21897721\n",
      "Validated batch 26 batch loss 1.1984489\n",
      "Validated batch 27 batch loss 1.12958992\n",
      "Validated batch 28 batch loss 1.12239957\n",
      "Validated batch 29 batch loss 1.28661203\n",
      "Validated batch 30 batch loss 1.22560239\n",
      "Validated batch 31 batch loss 1.09458828\n",
      "Validated batch 32 batch loss 1.18234265\n",
      "Validated batch 33 batch loss 1.21592045\n",
      "Validated batch 34 batch loss 1.17132211\n",
      "Validated batch 35 batch loss 1.1906848\n",
      "Validated batch 36 batch loss 1.23125827\n",
      "Validated batch 37 batch loss 1.22782981\n",
      "Validated batch 38 batch loss 1.32683539\n",
      "Validated batch 39 batch loss 1.27865529\n",
      "Validated batch 40 batch loss 1.22918057\n",
      "Validated batch 41 batch loss 1.32234156\n",
      "Validated batch 42 batch loss 1.04697227\n",
      "Validated batch 43 batch loss 1.18705857\n",
      "Validated batch 44 batch loss 1.16141129\n",
      "Validated batch 45 batch loss 1.24532855\n",
      "Validated batch 46 batch loss 1.38187706\n",
      "Validated batch 47 batch loss 1.3693521\n",
      "Validated batch 48 batch loss 1.26393831\n",
      "Validated batch 49 batch loss 1.19490802\n",
      "Validated batch 50 batch loss 1.19263685\n",
      "Validated batch 51 batch loss 1.19411063\n",
      "Validated batch 52 batch loss 1.25485468\n",
      "Validated batch 53 batch loss 1.28589118\n",
      "Validated batch 54 batch loss 1.1421597\n",
      "Validated batch 55 batch loss 1.28376961\n",
      "Validated batch 56 batch loss 1.23657846\n",
      "Validated batch 57 batch loss 1.28901815\n",
      "Validated batch 58 batch loss 1.2778337\n",
      "Validated batch 59 batch loss 1.27189898\n",
      "Validated batch 60 batch loss 1.19189513\n",
      "Validated batch 61 batch loss 1.2104789\n",
      "Validated batch 62 batch loss 1.2697252\n",
      "Validated batch 63 batch loss 1.25037158\n",
      "Validated batch 64 batch loss 1.29879761\n",
      "Validated batch 65 batch loss 1.32773364\n",
      "Validated batch 66 batch loss 1.49538231\n",
      "Validated batch 67 batch loss 1.320683\n",
      "Validated batch 68 batch loss 1.28495097\n",
      "Validated batch 69 batch loss 1.14828622\n",
      "Validated batch 70 batch loss 1.17390835\n",
      "Validated batch 71 batch loss 1.28726935\n",
      "Validated batch 72 batch loss 1.17841244\n",
      "Validated batch 73 batch loss 1.20306921\n",
      "Validated batch 74 batch loss 1.22921181\n",
      "Validated batch 75 batch loss 1.3095336\n",
      "Validated batch 76 batch loss 1.32137728\n",
      "Validated batch 77 batch loss 1.35873568\n",
      "Validated batch 78 batch loss 1.26370394\n",
      "Validated batch 79 batch loss 1.2363373\n",
      "Validated batch 80 batch loss 1.3041389\n",
      "Validated batch 81 batch loss 1.22092414\n",
      "Validated batch 82 batch loss 1.25492835\n",
      "Validated batch 83 batch loss 1.36909342\n",
      "Validated batch 84 batch loss 1.29627395\n",
      "Validated batch 85 batch loss 1.27256763\n",
      "Validated batch 86 batch loss 1.42921877\n",
      "Validated batch 87 batch loss 1.11220014\n",
      "Validated batch 88 batch loss 1.26934814\n",
      "Validated batch 89 batch loss 1.11355484\n",
      "Validated batch 90 batch loss 1.21276\n",
      "Validated batch 91 batch loss 1.36516321\n",
      "Validated batch 92 batch loss 1.19074416\n",
      "Validated batch 93 batch loss 1.22998846\n",
      "Validated batch 94 batch loss 1.18686914\n",
      "Validated batch 95 batch loss 1.24207878\n",
      "Validated batch 96 batch loss 1.1720314\n",
      "Validated batch 97 batch loss 1.21369219\n",
      "Validated batch 98 batch loss 1.32719707\n",
      "Validated batch 99 batch loss 1.26095164\n",
      "Validated batch 100 batch loss 1.3180778\n",
      "Validated batch 101 batch loss 1.34672654\n",
      "Validated batch 102 batch loss 1.3018626\n",
      "Validated batch 103 batch loss 1.23211896\n",
      "Validated batch 104 batch loss 1.39883161\n",
      "Validated batch 105 batch loss 1.25089085\n",
      "Validated batch 106 batch loss 1.32432914\n",
      "Validated batch 107 batch loss 1.31536376\n",
      "Validated batch 108 batch loss 1.36233211\n",
      "Validated batch 109 batch loss 1.32134235\n",
      "Validated batch 110 batch loss 1.16486061\n",
      "Validated batch 111 batch loss 1.25057256\n",
      "Validated batch 112 batch loss 1.29653752\n",
      "Validated batch 113 batch loss 1.31402242\n",
      "Validated batch 114 batch loss 1.18356371\n",
      "Validated batch 115 batch loss 1.21586156\n",
      "Validated batch 116 batch loss 1.17555737\n",
      "Validated batch 117 batch loss 1.23400116\n",
      "Validated batch 118 batch loss 1.32164431\n",
      "Validated batch 119 batch loss 1.209975\n",
      "Validated batch 120 batch loss 1.32361758\n",
      "Validated batch 121 batch loss 1.41613865\n",
      "Validated batch 122 batch loss 1.14641225\n",
      "Validated batch 123 batch loss 1.21201348\n",
      "Validated batch 124 batch loss 1.24789643\n",
      "Validated batch 125 batch loss 1.28091455\n",
      "Validated batch 126 batch loss 1.29945207\n",
      "Validated batch 127 batch loss 1.19787574\n",
      "Validated batch 128 batch loss 1.04401946\n",
      "Validated batch 129 batch loss 1.27804875\n",
      "Validated batch 130 batch loss 1.20929933\n",
      "Validated batch 131 batch loss 1.22861671\n",
      "Validated batch 132 batch loss 1.31354499\n",
      "Validated batch 133 batch loss 1.16965497\n",
      "Validated batch 134 batch loss 1.26867878\n",
      "Validated batch 135 batch loss 1.34023726\n",
      "Validated batch 136 batch loss 1.2666924\n",
      "Validated batch 137 batch loss 1.28088832\n",
      "Validated batch 138 batch loss 1.14307511\n",
      "Validated batch 139 batch loss 1.25321627\n",
      "Validated batch 140 batch loss 1.24618554\n",
      "Validated batch 141 batch loss 1.25774014\n",
      "Validated batch 142 batch loss 1.23254097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 143 batch loss 1.28762078\n",
      "Validated batch 144 batch loss 1.3823061\n",
      "Validated batch 145 batch loss 1.16532052\n",
      "Validated batch 146 batch loss 1.29047596\n",
      "Validated batch 147 batch loss 1.17375016\n",
      "Validated batch 148 batch loss 1.30487311\n",
      "Validated batch 149 batch loss 1.24064028\n",
      "Validated batch 150 batch loss 1.19233322\n",
      "Validated batch 151 batch loss 1.30583763\n",
      "Validated batch 152 batch loss 1.28838754\n",
      "Validated batch 153 batch loss 1.31411\n",
      "Validated batch 154 batch loss 1.25151348\n",
      "Validated batch 155 batch loss 1.2724998\n",
      "Validated batch 156 batch loss 1.21989846\n",
      "Validated batch 157 batch loss 1.15299129\n",
      "Validated batch 158 batch loss 1.26110363\n",
      "Validated batch 159 batch loss 1.19934\n",
      "Validated batch 160 batch loss 1.2377528\n",
      "Validated batch 161 batch loss 1.25267518\n",
      "Validated batch 162 batch loss 1.27901256\n",
      "Validated batch 163 batch loss 1.13935494\n",
      "Validated batch 164 batch loss 1.23807299\n",
      "Validated batch 165 batch loss 1.20922232\n",
      "Validated batch 166 batch loss 1.1927495\n",
      "Validated batch 167 batch loss 1.24429727\n",
      "Validated batch 168 batch loss 1.18954337\n",
      "Validated batch 169 batch loss 1.24856102\n",
      "Validated batch 170 batch loss 1.36472309\n",
      "Validated batch 171 batch loss 1.12330151\n",
      "Validated batch 172 batch loss 1.31750822\n",
      "Validated batch 173 batch loss 1.27492321\n",
      "Validated batch 174 batch loss 1.1451546\n",
      "Validated batch 175 batch loss 1.25277209\n",
      "Validated batch 176 batch loss 1.25017548\n",
      "Validated batch 177 batch loss 1.20073795\n",
      "Validated batch 178 batch loss 1.34761035\n",
      "Validated batch 179 batch loss 1.25447702\n",
      "Validated batch 180 batch loss 1.3067013\n",
      "Validated batch 181 batch loss 1.18170893\n",
      "Validated batch 182 batch loss 1.29145014\n",
      "Validated batch 183 batch loss 1.22877\n",
      "Validated batch 184 batch loss 1.17738914\n",
      "Validated batch 185 batch loss 1.30001736\n",
      "Epoch 3 val loss 1.2489079236984253\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-3-loss-1.2489.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.16883731 epoch total loss 1.16883731\n",
      "Trained batch 2 batch loss 1.10890067 epoch total loss 1.13886905\n",
      "Trained batch 3 batch loss 1.17534733 epoch total loss 1.15102851\n",
      "Trained batch 4 batch loss 1.18218124 epoch total loss 1.1588167\n",
      "Trained batch 5 batch loss 1.1513052 epoch total loss 1.15731442\n",
      "Trained batch 6 batch loss 1.28371382 epoch total loss 1.17838097\n",
      "Trained batch 7 batch loss 1.28885925 epoch total loss 1.19416356\n",
      "Trained batch 8 batch loss 1.33244395 epoch total loss 1.21144867\n",
      "Trained batch 9 batch loss 1.34296238 epoch total loss 1.22606134\n",
      "Trained batch 10 batch loss 1.27406144 epoch total loss 1.23086131\n",
      "Trained batch 11 batch loss 1.25306702 epoch total loss 1.23288\n",
      "Trained batch 12 batch loss 1.05538321 epoch total loss 1.21808863\n",
      "Trained batch 13 batch loss 1.18771744 epoch total loss 1.21575236\n",
      "Trained batch 14 batch loss 1.19766724 epoch total loss 1.21446061\n",
      "Trained batch 15 batch loss 1.29920828 epoch total loss 1.22011042\n",
      "Trained batch 16 batch loss 1.27318835 epoch total loss 1.22342777\n",
      "Trained batch 17 batch loss 1.29610634 epoch total loss 1.22770298\n",
      "Trained batch 18 batch loss 1.21013618 epoch total loss 1.22672701\n",
      "Trained batch 19 batch loss 1.21532929 epoch total loss 1.22612727\n",
      "Trained batch 20 batch loss 1.30307305 epoch total loss 1.22997451\n",
      "Trained batch 21 batch loss 1.07563829 epoch total loss 1.22262514\n",
      "Trained batch 22 batch loss 1.11577749 epoch total loss 1.21776855\n",
      "Trained batch 23 batch loss 1.22227967 epoch total loss 1.21796465\n",
      "Trained batch 24 batch loss 1.20281672 epoch total loss 1.21733344\n",
      "Trained batch 25 batch loss 1.16777074 epoch total loss 1.21535099\n",
      "Trained batch 26 batch loss 1.2636584 epoch total loss 1.21720898\n",
      "Trained batch 27 batch loss 1.24472225 epoch total loss 1.21822798\n",
      "Trained batch 28 batch loss 1.11587012 epoch total loss 1.21457231\n",
      "Trained batch 29 batch loss 1.20561504 epoch total loss 1.21426356\n",
      "Trained batch 30 batch loss 1.19754469 epoch total loss 1.21370625\n",
      "Trained batch 31 batch loss 1.25414348 epoch total loss 1.21501064\n",
      "Trained batch 32 batch loss 1.26151466 epoch total loss 1.2164638\n",
      "Trained batch 33 batch loss 1.19008589 epoch total loss 1.21566451\n",
      "Trained batch 34 batch loss 1.19242346 epoch total loss 1.21498096\n",
      "Trained batch 35 batch loss 1.21993542 epoch total loss 1.21512258\n",
      "Trained batch 36 batch loss 1.24969971 epoch total loss 1.21608305\n",
      "Trained batch 37 batch loss 1.26101542 epoch total loss 1.21729743\n",
      "Trained batch 38 batch loss 1.30835176 epoch total loss 1.21969366\n",
      "Trained batch 39 batch loss 1.18109906 epoch total loss 1.21870399\n",
      "Trained batch 40 batch loss 1.17098904 epoch total loss 1.21751118\n",
      "Trained batch 41 batch loss 1.13920164 epoch total loss 1.21560121\n",
      "Trained batch 42 batch loss 1.17710292 epoch total loss 1.21468449\n",
      "Trained batch 43 batch loss 1.17845011 epoch total loss 1.21384192\n",
      "Trained batch 44 batch loss 1.21047723 epoch total loss 1.21376538\n",
      "Trained batch 45 batch loss 1.22674978 epoch total loss 1.21405399\n",
      "Trained batch 46 batch loss 1.0863471 epoch total loss 1.21127772\n",
      "Trained batch 47 batch loss 1.14989638 epoch total loss 1.20997167\n",
      "Trained batch 48 batch loss 1.09619474 epoch total loss 1.20760131\n",
      "Trained batch 49 batch loss 1.13007116 epoch total loss 1.20601904\n",
      "Trained batch 50 batch loss 1.18830991 epoch total loss 1.20566487\n",
      "Trained batch 51 batch loss 1.11744797 epoch total loss 1.20393503\n",
      "Trained batch 52 batch loss 1.09129059 epoch total loss 1.20176876\n",
      "Trained batch 53 batch loss 1.11943817 epoch total loss 1.20021534\n",
      "Trained batch 54 batch loss 1.19641626 epoch total loss 1.20014501\n",
      "Trained batch 55 batch loss 1.1072638 epoch total loss 1.19845617\n",
      "Trained batch 56 batch loss 1.21977961 epoch total loss 1.19883704\n",
      "Trained batch 57 batch loss 1.25147843 epoch total loss 1.19976056\n",
      "Trained batch 58 batch loss 1.23621702 epoch total loss 1.20038903\n",
      "Trained batch 59 batch loss 1.33102667 epoch total loss 1.20260322\n",
      "Trained batch 60 batch loss 1.3439393 epoch total loss 1.2049588\n",
      "Trained batch 61 batch loss 1.23768568 epoch total loss 1.20549536\n",
      "Trained batch 62 batch loss 1.21142948 epoch total loss 1.20559096\n",
      "Trained batch 63 batch loss 1.16562128 epoch total loss 1.20495653\n",
      "Trained batch 64 batch loss 1.23894942 epoch total loss 1.20548773\n",
      "Trained batch 65 batch loss 1.18473291 epoch total loss 1.20516837\n",
      "Trained batch 66 batch loss 1.17757034 epoch total loss 1.2047503\n",
      "Trained batch 67 batch loss 1.36463952 epoch total loss 1.20713675\n",
      "Trained batch 68 batch loss 1.17369914 epoch total loss 1.20664501\n",
      "Trained batch 69 batch loss 1.16835785 epoch total loss 1.20609009\n",
      "Trained batch 70 batch loss 1.21191764 epoch total loss 1.2061733\n",
      "Trained batch 71 batch loss 1.08541059 epoch total loss 1.20447242\n",
      "Trained batch 72 batch loss 1.14945674 epoch total loss 1.20370829\n",
      "Trained batch 73 batch loss 1.13615251 epoch total loss 1.20278299\n",
      "Trained batch 74 batch loss 1.14010787 epoch total loss 1.20193601\n",
      "Trained batch 75 batch loss 1.09929502 epoch total loss 1.20056736\n",
      "Trained batch 76 batch loss 1.08665144 epoch total loss 1.19906855\n",
      "Trained batch 77 batch loss 1.19254148 epoch total loss 1.19898379\n",
      "Trained batch 78 batch loss 1.1995132 epoch total loss 1.1989907\n",
      "Trained batch 79 batch loss 1.39402902 epoch total loss 1.20145953\n",
      "Trained batch 80 batch loss 1.1700474 epoch total loss 1.20106673\n",
      "Trained batch 81 batch loss 1.28875971 epoch total loss 1.20214939\n",
      "Trained batch 82 batch loss 1.2375685 epoch total loss 1.20258141\n",
      "Trained batch 83 batch loss 1.19756937 epoch total loss 1.20252097\n",
      "Trained batch 84 batch loss 1.1568327 epoch total loss 1.20197701\n",
      "Trained batch 85 batch loss 1.18929911 epoch total loss 1.20182788\n",
      "Trained batch 86 batch loss 1.35487509 epoch total loss 1.20360756\n",
      "Trained batch 87 batch loss 1.3006494 epoch total loss 1.204723\n",
      "Trained batch 88 batch loss 1.40757084 epoch total loss 1.20702803\n",
      "Trained batch 89 batch loss 1.29755759 epoch total loss 1.20804524\n",
      "Trained batch 90 batch loss 1.19498873 epoch total loss 1.20790017\n",
      "Trained batch 91 batch loss 1.23997939 epoch total loss 1.20825267\n",
      "Trained batch 92 batch loss 1.14418912 epoch total loss 1.20755637\n",
      "Trained batch 93 batch loss 1.16338909 epoch total loss 1.20708144\n",
      "Trained batch 94 batch loss 1.24647021 epoch total loss 1.20750046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 95 batch loss 1.35969257 epoch total loss 1.20910251\n",
      "Trained batch 96 batch loss 1.31330836 epoch total loss 1.21018803\n",
      "Trained batch 97 batch loss 1.24906254 epoch total loss 1.21058869\n",
      "Trained batch 98 batch loss 1.16898465 epoch total loss 1.21016419\n",
      "Trained batch 99 batch loss 1.19852638 epoch total loss 1.21004665\n",
      "Trained batch 100 batch loss 1.13806307 epoch total loss 1.20932674\n",
      "Trained batch 101 batch loss 1.2568624 epoch total loss 1.20979738\n",
      "Trained batch 102 batch loss 1.11509633 epoch total loss 1.20886898\n",
      "Trained batch 103 batch loss 1.14849365 epoch total loss 1.20828271\n",
      "Trained batch 104 batch loss 1.23279715 epoch total loss 1.20851851\n",
      "Trained batch 105 batch loss 1.26409554 epoch total loss 1.20904779\n",
      "Trained batch 106 batch loss 1.2034421 epoch total loss 1.20899487\n",
      "Trained batch 107 batch loss 1.17219937 epoch total loss 1.20865095\n",
      "Trained batch 108 batch loss 1.09035015 epoch total loss 1.20755553\n",
      "Trained batch 109 batch loss 1.24553895 epoch total loss 1.2079041\n",
      "Trained batch 110 batch loss 1.26406205 epoch total loss 1.20841467\n",
      "Trained batch 111 batch loss 1.21417832 epoch total loss 1.20846653\n",
      "Trained batch 112 batch loss 1.21552086 epoch total loss 1.20852947\n",
      "Trained batch 113 batch loss 1.2322197 epoch total loss 1.20873916\n",
      "Trained batch 114 batch loss 1.22781491 epoch total loss 1.20890653\n",
      "Trained batch 115 batch loss 1.27527475 epoch total loss 1.2094835\n",
      "Trained batch 116 batch loss 1.21319187 epoch total loss 1.20951557\n",
      "Trained batch 117 batch loss 1.19188595 epoch total loss 1.20936477\n",
      "Trained batch 118 batch loss 1.1972183 epoch total loss 1.20926189\n",
      "Trained batch 119 batch loss 1.23925543 epoch total loss 1.2095139\n",
      "Trained batch 120 batch loss 1.28723454 epoch total loss 1.21016157\n",
      "Trained batch 121 batch loss 1.28533387 epoch total loss 1.21078289\n",
      "Trained batch 122 batch loss 1.35155773 epoch total loss 1.21193683\n",
      "Trained batch 123 batch loss 1.18577981 epoch total loss 1.21172416\n",
      "Trained batch 124 batch loss 1.21900797 epoch total loss 1.21178293\n",
      "Trained batch 125 batch loss 1.2323904 epoch total loss 1.2119478\n",
      "Trained batch 126 batch loss 1.06586361 epoch total loss 1.21078825\n",
      "Trained batch 127 batch loss 1.29504097 epoch total loss 1.21145177\n",
      "Trained batch 128 batch loss 1.18204498 epoch total loss 1.21122193\n",
      "Trained batch 129 batch loss 1.31210721 epoch total loss 1.21200395\n",
      "Trained batch 130 batch loss 1.28599107 epoch total loss 1.21257317\n",
      "Trained batch 131 batch loss 1.15014708 epoch total loss 1.21209657\n",
      "Trained batch 132 batch loss 1.25352478 epoch total loss 1.21241045\n",
      "Trained batch 133 batch loss 1.2956214 epoch total loss 1.21303606\n",
      "Trained batch 134 batch loss 1.29774022 epoch total loss 1.21366823\n",
      "Trained batch 135 batch loss 1.39149582 epoch total loss 1.21498549\n",
      "Trained batch 136 batch loss 1.24355805 epoch total loss 1.21519566\n",
      "Trained batch 137 batch loss 1.18266678 epoch total loss 1.21495819\n",
      "Trained batch 138 batch loss 1.30606079 epoch total loss 1.21561825\n",
      "Trained batch 139 batch loss 1.23329282 epoch total loss 1.21574545\n",
      "Trained batch 140 batch loss 1.31756759 epoch total loss 1.21647274\n",
      "Trained batch 141 batch loss 1.34378934 epoch total loss 1.21737576\n",
      "Trained batch 142 batch loss 1.2836374 epoch total loss 1.21784234\n",
      "Trained batch 143 batch loss 1.3111217 epoch total loss 1.21849465\n",
      "Trained batch 144 batch loss 1.33762 epoch total loss 1.21932185\n",
      "Trained batch 145 batch loss 1.19937527 epoch total loss 1.21918428\n",
      "Trained batch 146 batch loss 1.29550576 epoch total loss 1.21970701\n",
      "Trained batch 147 batch loss 1.31099343 epoch total loss 1.22032797\n",
      "Trained batch 148 batch loss 1.26715922 epoch total loss 1.22064447\n",
      "Trained batch 149 batch loss 1.28400993 epoch total loss 1.22106969\n",
      "Trained batch 150 batch loss 1.18719 epoch total loss 1.22084391\n",
      "Trained batch 151 batch loss 1.09739578 epoch total loss 1.22002637\n",
      "Trained batch 152 batch loss 1.25960803 epoch total loss 1.22028685\n",
      "Trained batch 153 batch loss 1.47725356 epoch total loss 1.22196627\n",
      "Trained batch 154 batch loss 1.35672712 epoch total loss 1.22284138\n",
      "Trained batch 155 batch loss 1.1419431 epoch total loss 1.22231936\n",
      "Trained batch 156 batch loss 1.21982837 epoch total loss 1.22230339\n",
      "Trained batch 157 batch loss 1.18075073 epoch total loss 1.22203875\n",
      "Trained batch 158 batch loss 1.30422139 epoch total loss 1.22255886\n",
      "Trained batch 159 batch loss 1.29515779 epoch total loss 1.22301543\n",
      "Trained batch 160 batch loss 1.19222665 epoch total loss 1.22282302\n",
      "Trained batch 161 batch loss 1.29615569 epoch total loss 1.22327852\n",
      "Trained batch 162 batch loss 1.3983047 epoch total loss 1.22435892\n",
      "Trained batch 163 batch loss 1.36759758 epoch total loss 1.22523773\n",
      "Trained batch 164 batch loss 1.28858876 epoch total loss 1.22562397\n",
      "Trained batch 165 batch loss 1.27048802 epoch total loss 1.22589588\n",
      "Trained batch 166 batch loss 1.24999344 epoch total loss 1.22604108\n",
      "Trained batch 167 batch loss 1.19653296 epoch total loss 1.22586441\n",
      "Trained batch 168 batch loss 1.28741705 epoch total loss 1.22623086\n",
      "Trained batch 169 batch loss 1.25147057 epoch total loss 1.22638011\n",
      "Trained batch 170 batch loss 1.18117237 epoch total loss 1.22611415\n",
      "Trained batch 171 batch loss 1.21615636 epoch total loss 1.22605598\n",
      "Trained batch 172 batch loss 1.21226454 epoch total loss 1.22597575\n",
      "Trained batch 173 batch loss 1.13912344 epoch total loss 1.22547376\n",
      "Trained batch 174 batch loss 1.22857857 epoch total loss 1.22549152\n",
      "Trained batch 175 batch loss 1.35363138 epoch total loss 1.22622383\n",
      "Trained batch 176 batch loss 1.17364025 epoch total loss 1.22592509\n",
      "Trained batch 177 batch loss 1.27716279 epoch total loss 1.22621453\n",
      "Trained batch 178 batch loss 1.28917 epoch total loss 1.22656822\n",
      "Trained batch 179 batch loss 1.29072595 epoch total loss 1.22692668\n",
      "Trained batch 180 batch loss 1.1365205 epoch total loss 1.22642446\n",
      "Trained batch 181 batch loss 1.08076668 epoch total loss 1.22561967\n",
      "Trained batch 182 batch loss 1.12975383 epoch total loss 1.22509301\n",
      "Trained batch 183 batch loss 1.16429472 epoch total loss 1.22476077\n",
      "Trained batch 184 batch loss 1.16483021 epoch total loss 1.22443497\n",
      "Trained batch 185 batch loss 1.18966973 epoch total loss 1.2242471\n",
      "Trained batch 186 batch loss 1.18687224 epoch total loss 1.22404611\n",
      "Trained batch 187 batch loss 1.12899625 epoch total loss 1.2235378\n",
      "Trained batch 188 batch loss 1.14196658 epoch total loss 1.223104\n",
      "Trained batch 189 batch loss 1.20180476 epoch total loss 1.22299123\n",
      "Trained batch 190 batch loss 1.16596353 epoch total loss 1.22269106\n",
      "Trained batch 191 batch loss 1.14237 epoch total loss 1.22227049\n",
      "Trained batch 192 batch loss 1.35676098 epoch total loss 1.22297108\n",
      "Trained batch 193 batch loss 1.2486186 epoch total loss 1.22310388\n",
      "Trained batch 194 batch loss 1.22581398 epoch total loss 1.22311783\n",
      "Trained batch 195 batch loss 1.26473868 epoch total loss 1.22333133\n",
      "Trained batch 196 batch loss 1.1906004 epoch total loss 1.22316432\n",
      "Trained batch 197 batch loss 1.23485124 epoch total loss 1.22322357\n",
      "Trained batch 198 batch loss 1.25408411 epoch total loss 1.22337949\n",
      "Trained batch 199 batch loss 1.24096346 epoch total loss 1.22346783\n",
      "Trained batch 200 batch loss 1.38496041 epoch total loss 1.22427535\n",
      "Trained batch 201 batch loss 1.35706711 epoch total loss 1.22493601\n",
      "Trained batch 202 batch loss 1.25763941 epoch total loss 1.22509801\n",
      "Trained batch 203 batch loss 1.20848036 epoch total loss 1.22501612\n",
      "Trained batch 204 batch loss 1.24059403 epoch total loss 1.22509253\n",
      "Trained batch 205 batch loss 1.16507101 epoch total loss 1.22479975\n",
      "Trained batch 206 batch loss 1.39315033 epoch total loss 1.22561693\n",
      "Trained batch 207 batch loss 1.25992203 epoch total loss 1.22578263\n",
      "Trained batch 208 batch loss 1.35492313 epoch total loss 1.22640347\n",
      "Trained batch 209 batch loss 1.46924663 epoch total loss 1.22756529\n",
      "Trained batch 210 batch loss 1.35367537 epoch total loss 1.22816586\n",
      "Trained batch 211 batch loss 1.36060107 epoch total loss 1.2287935\n",
      "Trained batch 212 batch loss 1.377087 epoch total loss 1.2294929\n",
      "Trained batch 213 batch loss 1.31889629 epoch total loss 1.22991276\n",
      "Trained batch 214 batch loss 1.33111596 epoch total loss 1.23038566\n",
      "Trained batch 215 batch loss 1.31414294 epoch total loss 1.23077524\n",
      "Trained batch 216 batch loss 1.14629018 epoch total loss 1.23038411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 217 batch loss 1.15745509 epoch total loss 1.23004794\n",
      "Trained batch 218 batch loss 1.09560847 epoch total loss 1.22943127\n",
      "Trained batch 219 batch loss 1.17847133 epoch total loss 1.22919858\n",
      "Trained batch 220 batch loss 1.1063056 epoch total loss 1.22864\n",
      "Trained batch 221 batch loss 1.1796633 epoch total loss 1.22841823\n",
      "Trained batch 222 batch loss 1.29931092 epoch total loss 1.22873759\n",
      "Trained batch 223 batch loss 1.28176355 epoch total loss 1.22897542\n",
      "Trained batch 224 batch loss 1.24532712 epoch total loss 1.22904849\n",
      "Trained batch 225 batch loss 1.26124763 epoch total loss 1.22919166\n",
      "Trained batch 226 batch loss 1.27969098 epoch total loss 1.22941506\n",
      "Trained batch 227 batch loss 1.36478972 epoch total loss 1.23001134\n",
      "Trained batch 228 batch loss 1.2903775 epoch total loss 1.23027611\n",
      "Trained batch 229 batch loss 1.16043162 epoch total loss 1.22997117\n",
      "Trained batch 230 batch loss 1.24377179 epoch total loss 1.23003113\n",
      "Trained batch 231 batch loss 1.19167626 epoch total loss 1.22986519\n",
      "Trained batch 232 batch loss 1.29273462 epoch total loss 1.23013604\n",
      "Trained batch 233 batch loss 1.31919885 epoch total loss 1.23051834\n",
      "Trained batch 234 batch loss 1.31496811 epoch total loss 1.23087931\n",
      "Trained batch 235 batch loss 1.31074953 epoch total loss 1.23121917\n",
      "Trained batch 236 batch loss 1.26272237 epoch total loss 1.23135269\n",
      "Trained batch 237 batch loss 1.15973926 epoch total loss 1.23105049\n",
      "Trained batch 238 batch loss 1.17511106 epoch total loss 1.23081553\n",
      "Trained batch 239 batch loss 1.31052852 epoch total loss 1.23114896\n",
      "Trained batch 240 batch loss 1.36670589 epoch total loss 1.23171377\n",
      "Trained batch 241 batch loss 1.25535631 epoch total loss 1.23181188\n",
      "Trained batch 242 batch loss 1.28891361 epoch total loss 1.2320478\n",
      "Trained batch 243 batch loss 1.31256974 epoch total loss 1.2323792\n",
      "Trained batch 244 batch loss 1.31604528 epoch total loss 1.23272204\n",
      "Trained batch 245 batch loss 1.2869041 epoch total loss 1.23294318\n",
      "Trained batch 246 batch loss 1.27667105 epoch total loss 1.23312092\n",
      "Trained batch 247 batch loss 1.26877236 epoch total loss 1.23326528\n",
      "Trained batch 248 batch loss 1.20244324 epoch total loss 1.23314106\n",
      "Trained batch 249 batch loss 1.15691 epoch total loss 1.23283494\n",
      "Trained batch 250 batch loss 1.21255159 epoch total loss 1.23275375\n",
      "Trained batch 251 batch loss 1.24060845 epoch total loss 1.23278511\n",
      "Trained batch 252 batch loss 1.20871437 epoch total loss 1.2326895\n",
      "Trained batch 253 batch loss 1.1769681 epoch total loss 1.23246932\n",
      "Trained batch 254 batch loss 1.11175764 epoch total loss 1.23199403\n",
      "Trained batch 255 batch loss 1.09764016 epoch total loss 1.23146713\n",
      "Trained batch 256 batch loss 1.30005479 epoch total loss 1.23173499\n",
      "Trained batch 257 batch loss 1.22451603 epoch total loss 1.23170686\n",
      "Trained batch 258 batch loss 1.24590719 epoch total loss 1.23176193\n",
      "Trained batch 259 batch loss 1.22814679 epoch total loss 1.23174798\n",
      "Trained batch 260 batch loss 1.28692782 epoch total loss 1.2319603\n",
      "Trained batch 261 batch loss 1.34504688 epoch total loss 1.2323935\n",
      "Trained batch 262 batch loss 1.23224926 epoch total loss 1.23239291\n",
      "Trained batch 263 batch loss 1.32414699 epoch total loss 1.23274183\n",
      "Trained batch 264 batch loss 1.15261054 epoch total loss 1.23243833\n",
      "Trained batch 265 batch loss 1.23937905 epoch total loss 1.23246443\n",
      "Trained batch 266 batch loss 1.23737049 epoch total loss 1.23248291\n",
      "Trained batch 267 batch loss 1.34416533 epoch total loss 1.23290122\n",
      "Trained batch 268 batch loss 1.22845602 epoch total loss 1.23288465\n",
      "Trained batch 269 batch loss 1.30980265 epoch total loss 1.23317063\n",
      "Trained batch 270 batch loss 1.14922976 epoch total loss 1.23285973\n",
      "Trained batch 271 batch loss 0.963374853 epoch total loss 1.23186541\n",
      "Trained batch 272 batch loss 1.17401898 epoch total loss 1.23165262\n",
      "Trained batch 273 batch loss 1.11122382 epoch total loss 1.23121154\n",
      "Trained batch 274 batch loss 1.1777966 epoch total loss 1.23101664\n",
      "Trained batch 275 batch loss 1.1594609 epoch total loss 1.2307564\n",
      "Trained batch 276 batch loss 1.33127809 epoch total loss 1.23112059\n",
      "Trained batch 277 batch loss 1.31642866 epoch total loss 1.23142862\n",
      "Trained batch 278 batch loss 1.34250391 epoch total loss 1.23182809\n",
      "Trained batch 279 batch loss 1.22078061 epoch total loss 1.23178852\n",
      "Trained batch 280 batch loss 1.4771384 epoch total loss 1.23266482\n",
      "Trained batch 281 batch loss 1.41954517 epoch total loss 1.23332989\n",
      "Trained batch 282 batch loss 1.50905836 epoch total loss 1.23430765\n",
      "Trained batch 283 batch loss 1.24378884 epoch total loss 1.23434114\n",
      "Trained batch 284 batch loss 1.30762017 epoch total loss 1.23459911\n",
      "Trained batch 285 batch loss 1.16191113 epoch total loss 1.23434412\n",
      "Trained batch 286 batch loss 1.12651038 epoch total loss 1.23396707\n",
      "Trained batch 287 batch loss 1.0772959 epoch total loss 1.23342121\n",
      "Trained batch 288 batch loss 1.01783121 epoch total loss 1.23267257\n",
      "Trained batch 289 batch loss 1.31734121 epoch total loss 1.23296559\n",
      "Trained batch 290 batch loss 1.31365728 epoch total loss 1.23324382\n",
      "Trained batch 291 batch loss 1.4009707 epoch total loss 1.2338202\n",
      "Trained batch 292 batch loss 1.41136789 epoch total loss 1.23442829\n",
      "Trained batch 293 batch loss 1.36647439 epoch total loss 1.23487902\n",
      "Trained batch 294 batch loss 1.3078562 epoch total loss 1.23512721\n",
      "Trained batch 295 batch loss 1.2642436 epoch total loss 1.23522592\n",
      "Trained batch 296 batch loss 1.13736594 epoch total loss 1.23489535\n",
      "Trained batch 297 batch loss 1.14497578 epoch total loss 1.23459268\n",
      "Trained batch 298 batch loss 1.28242207 epoch total loss 1.23475313\n",
      "Trained batch 299 batch loss 1.25991559 epoch total loss 1.23483729\n",
      "Trained batch 300 batch loss 1.27020955 epoch total loss 1.23495519\n",
      "Trained batch 301 batch loss 1.25306273 epoch total loss 1.23501527\n",
      "Trained batch 302 batch loss 1.31185281 epoch total loss 1.23526967\n",
      "Trained batch 303 batch loss 1.26288748 epoch total loss 1.23536086\n",
      "Trained batch 304 batch loss 1.18934166 epoch total loss 1.23520947\n",
      "Trained batch 305 batch loss 1.21636045 epoch total loss 1.2351476\n",
      "Trained batch 306 batch loss 1.5162704 epoch total loss 1.23606634\n",
      "Trained batch 307 batch loss 1.26506042 epoch total loss 1.23616076\n",
      "Trained batch 308 batch loss 1.18856 epoch total loss 1.23600626\n",
      "Trained batch 309 batch loss 1.10323286 epoch total loss 1.23557651\n",
      "Trained batch 310 batch loss 1.1010437 epoch total loss 1.23514259\n",
      "Trained batch 311 batch loss 1.19688272 epoch total loss 1.23501945\n",
      "Trained batch 312 batch loss 1.36459851 epoch total loss 1.23543477\n",
      "Trained batch 313 batch loss 1.28546822 epoch total loss 1.23559463\n",
      "Trained batch 314 batch loss 1.20943081 epoch total loss 1.2355113\n",
      "Trained batch 315 batch loss 1.17115402 epoch total loss 1.23530698\n",
      "Trained batch 316 batch loss 1.23106933 epoch total loss 1.23529363\n",
      "Trained batch 317 batch loss 1.20433056 epoch total loss 1.23519599\n",
      "Trained batch 318 batch loss 1.25108695 epoch total loss 1.23524594\n",
      "Trained batch 319 batch loss 1.33346736 epoch total loss 1.23555386\n",
      "Trained batch 320 batch loss 1.31086159 epoch total loss 1.23578918\n",
      "Trained batch 321 batch loss 1.31920445 epoch total loss 1.23604906\n",
      "Trained batch 322 batch loss 1.11627197 epoch total loss 1.23567712\n",
      "Trained batch 323 batch loss 1.11865401 epoch total loss 1.23531485\n",
      "Trained batch 324 batch loss 1.16222334 epoch total loss 1.2350893\n",
      "Trained batch 325 batch loss 1.29951406 epoch total loss 1.23528743\n",
      "Trained batch 326 batch loss 1.2915715 epoch total loss 1.23546\n",
      "Trained batch 327 batch loss 1.17744613 epoch total loss 1.23528266\n",
      "Trained batch 328 batch loss 1.35665941 epoch total loss 1.23565269\n",
      "Trained batch 329 batch loss 1.19766474 epoch total loss 1.23553729\n",
      "Trained batch 330 batch loss 1.22833514 epoch total loss 1.23551548\n",
      "Trained batch 331 batch loss 1.17487299 epoch total loss 1.23533225\n",
      "Trained batch 332 batch loss 1.17820573 epoch total loss 1.23516011\n",
      "Trained batch 333 batch loss 1.20442343 epoch total loss 1.23506784\n",
      "Trained batch 334 batch loss 1.22584319 epoch total loss 1.23504019\n",
      "Trained batch 335 batch loss 1.39969218 epoch total loss 1.23553169\n",
      "Trained batch 336 batch loss 1.33601844 epoch total loss 1.23583078\n",
      "Trained batch 337 batch loss 1.4043498 epoch total loss 1.23633087\n",
      "Trained batch 338 batch loss 1.30547225 epoch total loss 1.23653543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 339 batch loss 1.28241205 epoch total loss 1.23667073\n",
      "Trained batch 340 batch loss 1.33545208 epoch total loss 1.23696125\n",
      "Trained batch 341 batch loss 1.31104231 epoch total loss 1.23717844\n",
      "Trained batch 342 batch loss 1.26011515 epoch total loss 1.23724556\n",
      "Trained batch 343 batch loss 1.18696129 epoch total loss 1.23709893\n",
      "Trained batch 344 batch loss 1.19896746 epoch total loss 1.23698807\n",
      "Trained batch 345 batch loss 1.16359258 epoch total loss 1.2367754\n",
      "Trained batch 346 batch loss 1.10988081 epoch total loss 1.23640871\n",
      "Trained batch 347 batch loss 1.21902955 epoch total loss 1.23635852\n",
      "Trained batch 348 batch loss 1.25486708 epoch total loss 1.23641169\n",
      "Trained batch 349 batch loss 1.23948491 epoch total loss 1.23642051\n",
      "Trained batch 350 batch loss 1.20301652 epoch total loss 1.23632503\n",
      "Trained batch 351 batch loss 1.31495559 epoch total loss 1.23654902\n",
      "Trained batch 352 batch loss 1.2703284 epoch total loss 1.23664486\n",
      "Trained batch 353 batch loss 1.23146367 epoch total loss 1.23663032\n",
      "Trained batch 354 batch loss 1.28432524 epoch total loss 1.23676503\n",
      "Trained batch 355 batch loss 1.15380573 epoch total loss 1.23653138\n",
      "Trained batch 356 batch loss 1.20631421 epoch total loss 1.2364465\n",
      "Trained batch 357 batch loss 1.09208822 epoch total loss 1.23604214\n",
      "Trained batch 358 batch loss 1.15423787 epoch total loss 1.23581362\n",
      "Trained batch 359 batch loss 1.19163394 epoch total loss 1.23569059\n",
      "Trained batch 360 batch loss 1.30193019 epoch total loss 1.23587465\n",
      "Trained batch 361 batch loss 1.43108702 epoch total loss 1.23641539\n",
      "Trained batch 362 batch loss 1.44894314 epoch total loss 1.23700249\n",
      "Trained batch 363 batch loss 1.36694813 epoch total loss 1.23736048\n",
      "Trained batch 364 batch loss 1.36829305 epoch total loss 1.23772013\n",
      "Trained batch 365 batch loss 1.18087411 epoch total loss 1.23756433\n",
      "Trained batch 366 batch loss 1.04532301 epoch total loss 1.23703909\n",
      "Trained batch 367 batch loss 0.955745876 epoch total loss 1.23627269\n",
      "Trained batch 368 batch loss 0.994750798 epoch total loss 1.23561633\n",
      "Trained batch 369 batch loss 1.17892051 epoch total loss 1.23546267\n",
      "Trained batch 370 batch loss 1.27547526 epoch total loss 1.23557091\n",
      "Trained batch 371 batch loss 1.29486334 epoch total loss 1.23573065\n",
      "Trained batch 372 batch loss 1.03549409 epoch total loss 1.23519242\n",
      "Trained batch 373 batch loss 1.15254188 epoch total loss 1.23497081\n",
      "Trained batch 374 batch loss 1.2102108 epoch total loss 1.23490453\n",
      "Trained batch 375 batch loss 1.15229571 epoch total loss 1.23468423\n",
      "Trained batch 376 batch loss 1.13126493 epoch total loss 1.23440921\n",
      "Trained batch 377 batch loss 1.18763947 epoch total loss 1.23428512\n",
      "Trained batch 378 batch loss 1.1945256 epoch total loss 1.23418\n",
      "Trained batch 379 batch loss 1.21222734 epoch total loss 1.23412204\n",
      "Trained batch 380 batch loss 1.28952014 epoch total loss 1.23426783\n",
      "Trained batch 381 batch loss 1.35608435 epoch total loss 1.23458755\n",
      "Trained batch 382 batch loss 1.36613154 epoch total loss 1.23493183\n",
      "Trained batch 383 batch loss 1.47738719 epoch total loss 1.23556483\n",
      "Trained batch 384 batch loss 1.48263085 epoch total loss 1.23620832\n",
      "Trained batch 385 batch loss 1.27534616 epoch total loss 1.23631\n",
      "Trained batch 386 batch loss 1.15947604 epoch total loss 1.23611093\n",
      "Trained batch 387 batch loss 1.17087817 epoch total loss 1.23594236\n",
      "Trained batch 388 batch loss 1.45773566 epoch total loss 1.23651397\n",
      "Trained batch 389 batch loss 1.36556113 epoch total loss 1.23684573\n",
      "Trained batch 390 batch loss 1.24019349 epoch total loss 1.23685431\n",
      "Trained batch 391 batch loss 1.25238025 epoch total loss 1.23689401\n",
      "Trained batch 392 batch loss 1.31802368 epoch total loss 1.23710108\n",
      "Trained batch 393 batch loss 1.34428108 epoch total loss 1.23737371\n",
      "Trained batch 394 batch loss 1.25996053 epoch total loss 1.23743105\n",
      "Trained batch 395 batch loss 1.2809341 epoch total loss 1.2375412\n",
      "Trained batch 396 batch loss 1.28343081 epoch total loss 1.23765707\n",
      "Trained batch 397 batch loss 1.32696342 epoch total loss 1.23788202\n",
      "Trained batch 398 batch loss 1.37333643 epoch total loss 1.23822224\n",
      "Trained batch 399 batch loss 1.37450218 epoch total loss 1.2385639\n",
      "Trained batch 400 batch loss 1.32723284 epoch total loss 1.23878551\n",
      "Trained batch 401 batch loss 1.21992826 epoch total loss 1.23873854\n",
      "Trained batch 402 batch loss 1.27459788 epoch total loss 1.23882771\n",
      "Trained batch 403 batch loss 1.27141118 epoch total loss 1.23890865\n",
      "Trained batch 404 batch loss 1.26443601 epoch total loss 1.23897183\n",
      "Trained batch 405 batch loss 1.27250898 epoch total loss 1.23905468\n",
      "Trained batch 406 batch loss 1.32092202 epoch total loss 1.23925626\n",
      "Trained batch 407 batch loss 1.21503079 epoch total loss 1.23919678\n",
      "Trained batch 408 batch loss 1.2424202 epoch total loss 1.23920476\n",
      "Trained batch 409 batch loss 1.2519871 epoch total loss 1.239236\n",
      "Trained batch 410 batch loss 1.33642697 epoch total loss 1.23947299\n",
      "Trained batch 411 batch loss 1.34227395 epoch total loss 1.23972321\n",
      "Trained batch 412 batch loss 1.1049819 epoch total loss 1.2393961\n",
      "Trained batch 413 batch loss 1.24503195 epoch total loss 1.23940969\n",
      "Trained batch 414 batch loss 1.17222476 epoch total loss 1.23924744\n",
      "Trained batch 415 batch loss 1.23071253 epoch total loss 1.23922694\n",
      "Trained batch 416 batch loss 1.1423707 epoch total loss 1.23899412\n",
      "Trained batch 417 batch loss 1.24670315 epoch total loss 1.2390126\n",
      "Trained batch 418 batch loss 1.36481214 epoch total loss 1.2393136\n",
      "Trained batch 419 batch loss 1.34855092 epoch total loss 1.23957431\n",
      "Trained batch 420 batch loss 1.1720587 epoch total loss 1.23941362\n",
      "Trained batch 421 batch loss 1.19974017 epoch total loss 1.23931944\n",
      "Trained batch 422 batch loss 1.2947706 epoch total loss 1.23945093\n",
      "Trained batch 423 batch loss 1.2164495 epoch total loss 1.23939645\n",
      "Trained batch 424 batch loss 1.18534446 epoch total loss 1.23926902\n",
      "Trained batch 425 batch loss 1.19240868 epoch total loss 1.23915875\n",
      "Trained batch 426 batch loss 1.26046443 epoch total loss 1.2392087\n",
      "Trained batch 427 batch loss 1.21666944 epoch total loss 1.23915589\n",
      "Trained batch 428 batch loss 1.30059052 epoch total loss 1.23929942\n",
      "Trained batch 429 batch loss 1.27984047 epoch total loss 1.23939395\n",
      "Trained batch 430 batch loss 1.26826835 epoch total loss 1.23946106\n",
      "Trained batch 431 batch loss 1.36455441 epoch total loss 1.23975134\n",
      "Trained batch 432 batch loss 1.30639184 epoch total loss 1.2399056\n",
      "Trained batch 433 batch loss 1.25968313 epoch total loss 1.23995137\n",
      "Trained batch 434 batch loss 1.19518065 epoch total loss 1.23984814\n",
      "Trained batch 435 batch loss 1.27748334 epoch total loss 1.23993468\n",
      "Trained batch 436 batch loss 1.18831515 epoch total loss 1.23981619\n",
      "Trained batch 437 batch loss 1.18024588 epoch total loss 1.23967993\n",
      "Trained batch 438 batch loss 1.075037 epoch total loss 1.23930395\n",
      "Trained batch 439 batch loss 1.04697716 epoch total loss 1.23886585\n",
      "Trained batch 440 batch loss 1.450387 epoch total loss 1.23934662\n",
      "Trained batch 441 batch loss 1.33153129 epoch total loss 1.2395556\n",
      "Trained batch 442 batch loss 1.22077894 epoch total loss 1.23951316\n",
      "Trained batch 443 batch loss 1.209149 epoch total loss 1.23944461\n",
      "Trained batch 444 batch loss 1.25194252 epoch total loss 1.23947287\n",
      "Trained batch 445 batch loss 1.17285776 epoch total loss 1.23932314\n",
      "Trained batch 446 batch loss 1.15255904 epoch total loss 1.23912859\n",
      "Trained batch 447 batch loss 1.26839447 epoch total loss 1.23919404\n",
      "Trained batch 448 batch loss 1.16372967 epoch total loss 1.23902571\n",
      "Trained batch 449 batch loss 1.18469119 epoch total loss 1.2389046\n",
      "Trained batch 450 batch loss 1.27679753 epoch total loss 1.23898888\n",
      "Trained batch 451 batch loss 1.30276549 epoch total loss 1.23913038\n",
      "Trained batch 452 batch loss 1.19011021 epoch total loss 1.2390219\n",
      "Trained batch 453 batch loss 1.18051934 epoch total loss 1.23889279\n",
      "Trained batch 454 batch loss 1.17120075 epoch total loss 1.23874366\n",
      "Trained batch 455 batch loss 1.1370852 epoch total loss 1.23852026\n",
      "Trained batch 456 batch loss 1.14912474 epoch total loss 1.23832417\n",
      "Trained batch 457 batch loss 1.12981033 epoch total loss 1.23808682\n",
      "Trained batch 458 batch loss 1.19071209 epoch total loss 1.23798335\n",
      "Trained batch 459 batch loss 1.28217983 epoch total loss 1.23807967\n",
      "Trained batch 460 batch loss 1.21970534 epoch total loss 1.23803973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 461 batch loss 1.29070711 epoch total loss 1.23815405\n",
      "Trained batch 462 batch loss 1.1628294 epoch total loss 1.23799098\n",
      "Trained batch 463 batch loss 1.20144403 epoch total loss 1.23791206\n",
      "Trained batch 464 batch loss 1.23754573 epoch total loss 1.23791122\n",
      "Trained batch 465 batch loss 1.20489931 epoch total loss 1.23784018\n",
      "Trained batch 466 batch loss 1.12149763 epoch total loss 1.23759055\n",
      "Trained batch 467 batch loss 1.21065784 epoch total loss 1.23753285\n",
      "Trained batch 468 batch loss 1.07354379 epoch total loss 1.2371825\n",
      "Trained batch 469 batch loss 1.30988431 epoch total loss 1.23733747\n",
      "Trained batch 470 batch loss 1.2590009 epoch total loss 1.23738348\n",
      "Trained batch 471 batch loss 1.31538415 epoch total loss 1.23754907\n",
      "Trained batch 472 batch loss 1.20183766 epoch total loss 1.23747349\n",
      "Trained batch 473 batch loss 1.02030075 epoch total loss 1.23701429\n",
      "Trained batch 474 batch loss 1.02610993 epoch total loss 1.2365694\n",
      "Trained batch 475 batch loss 1.016909 epoch total loss 1.23610699\n",
      "Trained batch 476 batch loss 1.00488377 epoch total loss 1.23562121\n",
      "Trained batch 477 batch loss 1.14716852 epoch total loss 1.23543572\n",
      "Trained batch 478 batch loss 1.20051229 epoch total loss 1.23536265\n",
      "Trained batch 479 batch loss 1.15611112 epoch total loss 1.23519731\n",
      "Trained batch 480 batch loss 1.24929631 epoch total loss 1.23522651\n",
      "Trained batch 481 batch loss 1.21070707 epoch total loss 1.23517561\n",
      "Trained batch 482 batch loss 1.23548722 epoch total loss 1.23517621\n",
      "Trained batch 483 batch loss 1.12522495 epoch total loss 1.23494864\n",
      "Trained batch 484 batch loss 1.31613159 epoch total loss 1.23511624\n",
      "Trained batch 485 batch loss 1.44479871 epoch total loss 1.23554862\n",
      "Trained batch 486 batch loss 1.17936659 epoch total loss 1.2354331\n",
      "Trained batch 487 batch loss 1.0889554 epoch total loss 1.23513222\n",
      "Trained batch 488 batch loss 0.983524382 epoch total loss 1.23461664\n",
      "Trained batch 489 batch loss 1.04687726 epoch total loss 1.23423266\n",
      "Trained batch 490 batch loss 1.07827902 epoch total loss 1.23391449\n",
      "Trained batch 491 batch loss 1.10746 epoch total loss 1.233657\n",
      "Trained batch 492 batch loss 1.0683496 epoch total loss 1.23332107\n",
      "Trained batch 493 batch loss 1.06484842 epoch total loss 1.2329793\n",
      "Trained batch 494 batch loss 1.15961957 epoch total loss 1.23283076\n",
      "Trained batch 495 batch loss 1.19000542 epoch total loss 1.23274422\n",
      "Trained batch 496 batch loss 1.28400791 epoch total loss 1.23284757\n",
      "Trained batch 497 batch loss 1.30070364 epoch total loss 1.23298407\n",
      "Trained batch 498 batch loss 1.26655769 epoch total loss 1.23305142\n",
      "Trained batch 499 batch loss 1.28720236 epoch total loss 1.23316\n",
      "Trained batch 500 batch loss 1.12466896 epoch total loss 1.23294306\n",
      "Trained batch 501 batch loss 1.26451802 epoch total loss 1.23300612\n",
      "Trained batch 502 batch loss 1.30989027 epoch total loss 1.2331593\n",
      "Trained batch 503 batch loss 1.42247868 epoch total loss 1.23353565\n",
      "Trained batch 504 batch loss 1.46092606 epoch total loss 1.23398685\n",
      "Trained batch 505 batch loss 1.56538141 epoch total loss 1.2346431\n",
      "Trained batch 506 batch loss 1.477507 epoch total loss 1.23512304\n",
      "Trained batch 507 batch loss 1.12602949 epoch total loss 1.23490787\n",
      "Trained batch 508 batch loss 1.17588413 epoch total loss 1.23479164\n",
      "Trained batch 509 batch loss 1.27689266 epoch total loss 1.23487449\n",
      "Trained batch 510 batch loss 1.10192478 epoch total loss 1.23461378\n",
      "Trained batch 511 batch loss 1.15185189 epoch total loss 1.23445177\n",
      "Trained batch 512 batch loss 1.14098287 epoch total loss 1.23426926\n",
      "Trained batch 513 batch loss 1.19546878 epoch total loss 1.23419368\n",
      "Trained batch 514 batch loss 1.0699842 epoch total loss 1.2338742\n",
      "Trained batch 515 batch loss 1.10652041 epoch total loss 1.23362696\n",
      "Trained batch 516 batch loss 1.04098785 epoch total loss 1.2332536\n",
      "Trained batch 517 batch loss 1.19233048 epoch total loss 1.23317444\n",
      "Trained batch 518 batch loss 1.20362985 epoch total loss 1.23311746\n",
      "Trained batch 519 batch loss 1.11946857 epoch total loss 1.23289835\n",
      "Trained batch 520 batch loss 0.975216389 epoch total loss 1.2324028\n",
      "Trained batch 521 batch loss 1.24254537 epoch total loss 1.23242235\n",
      "Trained batch 522 batch loss 1.18278122 epoch total loss 1.23232722\n",
      "Trained batch 523 batch loss 1.18906951 epoch total loss 1.23224461\n",
      "Trained batch 524 batch loss 1.37776935 epoch total loss 1.23252225\n",
      "Trained batch 525 batch loss 1.43443131 epoch total loss 1.23290694\n",
      "Trained batch 526 batch loss 1.39217734 epoch total loss 1.23320961\n",
      "Trained batch 527 batch loss 1.37627673 epoch total loss 1.23348117\n",
      "Trained batch 528 batch loss 1.30085266 epoch total loss 1.23360872\n",
      "Trained batch 529 batch loss 1.29904342 epoch total loss 1.23373246\n",
      "Trained batch 530 batch loss 1.19327116 epoch total loss 1.23365617\n",
      "Trained batch 531 batch loss 1.31835485 epoch total loss 1.23381567\n",
      "Trained batch 532 batch loss 1.29267097 epoch total loss 1.2339263\n",
      "Trained batch 533 batch loss 1.39400339 epoch total loss 1.23422658\n",
      "Trained batch 534 batch loss 1.45284772 epoch total loss 1.23463595\n",
      "Trained batch 535 batch loss 1.38426948 epoch total loss 1.23491561\n",
      "Trained batch 536 batch loss 1.4558146 epoch total loss 1.23532772\n",
      "Trained batch 537 batch loss 1.45536697 epoch total loss 1.23573756\n",
      "Trained batch 538 batch loss 1.39589012 epoch total loss 1.23603523\n",
      "Trained batch 539 batch loss 1.37151 epoch total loss 1.23628652\n",
      "Trained batch 540 batch loss 1.32802713 epoch total loss 1.23645639\n",
      "Trained batch 541 batch loss 1.34466743 epoch total loss 1.23665643\n",
      "Trained batch 542 batch loss 1.40727127 epoch total loss 1.23697126\n",
      "Trained batch 543 batch loss 1.22582817 epoch total loss 1.23695076\n",
      "Trained batch 544 batch loss 1.26410913 epoch total loss 1.23700058\n",
      "Trained batch 545 batch loss 1.2565037 epoch total loss 1.23703647\n",
      "Trained batch 546 batch loss 1.23803663 epoch total loss 1.23703825\n",
      "Trained batch 547 batch loss 1.30560684 epoch total loss 1.23716366\n",
      "Trained batch 548 batch loss 1.28231335 epoch total loss 1.23724604\n",
      "Trained batch 549 batch loss 1.15959728 epoch total loss 1.23710454\n",
      "Trained batch 550 batch loss 1.23288655 epoch total loss 1.23709691\n",
      "Trained batch 551 batch loss 1.322348 epoch total loss 1.23725164\n",
      "Trained batch 552 batch loss 1.2366693 epoch total loss 1.23725057\n",
      "Trained batch 553 batch loss 1.24770844 epoch total loss 1.23726952\n",
      "Trained batch 554 batch loss 1.22091067 epoch total loss 1.23724\n",
      "Trained batch 555 batch loss 1.20599174 epoch total loss 1.23718357\n",
      "Trained batch 556 batch loss 1.25764477 epoch total loss 1.23722041\n",
      "Trained batch 557 batch loss 1.23873079 epoch total loss 1.23722303\n",
      "Trained batch 558 batch loss 1.30634 epoch total loss 1.23734689\n",
      "Trained batch 559 batch loss 1.40471756 epoch total loss 1.23764634\n",
      "Trained batch 560 batch loss 1.36172628 epoch total loss 1.23786795\n",
      "Trained batch 561 batch loss 1.32701087 epoch total loss 1.23802686\n",
      "Trained batch 562 batch loss 1.38311422 epoch total loss 1.23828506\n",
      "Trained batch 563 batch loss 1.21331894 epoch total loss 1.23824072\n",
      "Trained batch 564 batch loss 1.16565108 epoch total loss 1.23811197\n",
      "Trained batch 565 batch loss 1.34920168 epoch total loss 1.23830855\n",
      "Trained batch 566 batch loss 1.13579893 epoch total loss 1.23812747\n",
      "Trained batch 567 batch loss 1.14936876 epoch total loss 1.23797095\n",
      "Trained batch 568 batch loss 1.04692078 epoch total loss 1.23763454\n",
      "Trained batch 569 batch loss 1.08403313 epoch total loss 1.23736465\n",
      "Trained batch 570 batch loss 1.14376926 epoch total loss 1.2372005\n",
      "Trained batch 571 batch loss 1.09378338 epoch total loss 1.23694932\n",
      "Trained batch 572 batch loss 1.17532909 epoch total loss 1.23684168\n",
      "Trained batch 573 batch loss 1.14542198 epoch total loss 1.23668218\n",
      "Trained batch 574 batch loss 1.29577398 epoch total loss 1.23678517\n",
      "Trained batch 575 batch loss 1.28769362 epoch total loss 1.23687375\n",
      "Trained batch 576 batch loss 1.35841656 epoch total loss 1.23708475\n",
      "Trained batch 577 batch loss 1.27316713 epoch total loss 1.23714733\n",
      "Trained batch 578 batch loss 1.06317258 epoch total loss 1.23684633\n",
      "Trained batch 579 batch loss 1.14405942 epoch total loss 1.23668599\n",
      "Trained batch 580 batch loss 1.25370085 epoch total loss 1.23671544\n",
      "Trained batch 581 batch loss 1.3095876 epoch total loss 1.23684072\n",
      "Trained batch 582 batch loss 1.24869752 epoch total loss 1.23686123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 583 batch loss 1.25608647 epoch total loss 1.23689425\n",
      "Trained batch 584 batch loss 1.27165794 epoch total loss 1.23695374\n",
      "Trained batch 585 batch loss 1.19146812 epoch total loss 1.23687601\n",
      "Trained batch 586 batch loss 0.990798593 epoch total loss 1.23645604\n",
      "Trained batch 587 batch loss 0.953583121 epoch total loss 1.23597419\n",
      "Trained batch 588 batch loss 1.10051441 epoch total loss 1.23574388\n",
      "Trained batch 589 batch loss 1.14057255 epoch total loss 1.23558223\n",
      "Trained batch 590 batch loss 1.45369387 epoch total loss 1.2359519\n",
      "Trained batch 591 batch loss 1.60072923 epoch total loss 1.23656905\n",
      "Trained batch 592 batch loss 1.22140074 epoch total loss 1.23654342\n",
      "Trained batch 593 batch loss 1.30226445 epoch total loss 1.23665416\n",
      "Trained batch 594 batch loss 1.16353655 epoch total loss 1.23653102\n",
      "Trained batch 595 batch loss 1.2724694 epoch total loss 1.23659146\n",
      "Trained batch 596 batch loss 1.25563097 epoch total loss 1.23662341\n",
      "Trained batch 597 batch loss 1.31664205 epoch total loss 1.2367574\n",
      "Trained batch 598 batch loss 1.23835564 epoch total loss 1.23676\n",
      "Trained batch 599 batch loss 1.30960691 epoch total loss 1.23688173\n",
      "Trained batch 600 batch loss 1.21115565 epoch total loss 1.23683894\n",
      "Trained batch 601 batch loss 1.27819979 epoch total loss 1.23690772\n",
      "Trained batch 602 batch loss 1.1374228 epoch total loss 1.2367425\n",
      "Trained batch 603 batch loss 1.13675117 epoch total loss 1.23657668\n",
      "Trained batch 604 batch loss 1.26501346 epoch total loss 1.23662376\n",
      "Trained batch 605 batch loss 1.29009032 epoch total loss 1.23671222\n",
      "Trained batch 606 batch loss 1.02516794 epoch total loss 1.23636305\n",
      "Trained batch 607 batch loss 1.04279518 epoch total loss 1.23604417\n",
      "Trained batch 608 batch loss 1.16792047 epoch total loss 1.23593211\n",
      "Trained batch 609 batch loss 1.14911819 epoch total loss 1.23578954\n",
      "Trained batch 610 batch loss 1.28015161 epoch total loss 1.23586226\n",
      "Trained batch 611 batch loss 1.35937476 epoch total loss 1.23606443\n",
      "Trained batch 612 batch loss 1.14323616 epoch total loss 1.2359128\n",
      "Trained batch 613 batch loss 1.14566553 epoch total loss 1.23576558\n",
      "Trained batch 614 batch loss 1.20968628 epoch total loss 1.23572314\n",
      "Trained batch 615 batch loss 1.20422566 epoch total loss 1.23567188\n",
      "Trained batch 616 batch loss 1.16021693 epoch total loss 1.23554945\n",
      "Trained batch 617 batch loss 1.02666235 epoch total loss 1.2352109\n",
      "Trained batch 618 batch loss 1.050354 epoch total loss 1.2349118\n",
      "Trained batch 619 batch loss 1.17382669 epoch total loss 1.23481309\n",
      "Trained batch 620 batch loss 1.13955498 epoch total loss 1.23465943\n",
      "Trained batch 621 batch loss 1.10922921 epoch total loss 1.23445749\n",
      "Trained batch 622 batch loss 1.15607738 epoch total loss 1.23433149\n",
      "Trained batch 623 batch loss 1.16588962 epoch total loss 1.23422158\n",
      "Trained batch 624 batch loss 1.29767013 epoch total loss 1.23432326\n",
      "Trained batch 625 batch loss 1.29814041 epoch total loss 1.23442543\n",
      "Trained batch 626 batch loss 1.28191006 epoch total loss 1.23450124\n",
      "Trained batch 627 batch loss 1.25855827 epoch total loss 1.23453963\n",
      "Trained batch 628 batch loss 1.35434067 epoch total loss 1.23473048\n",
      "Trained batch 629 batch loss 1.34532452 epoch total loss 1.23490632\n",
      "Trained batch 630 batch loss 1.28736782 epoch total loss 1.23498952\n",
      "Trained batch 631 batch loss 1.19316232 epoch total loss 1.23492324\n",
      "Trained batch 632 batch loss 1.26474178 epoch total loss 1.23497045\n",
      "Trained batch 633 batch loss 1.27200699 epoch total loss 1.23502898\n",
      "Trained batch 634 batch loss 1.18087804 epoch total loss 1.23494363\n",
      "Trained batch 635 batch loss 1.11430919 epoch total loss 1.23475373\n",
      "Trained batch 636 batch loss 1.26582396 epoch total loss 1.23480248\n",
      "Trained batch 637 batch loss 1.22409344 epoch total loss 1.2347858\n",
      "Trained batch 638 batch loss 1.20940471 epoch total loss 1.23474598\n",
      "Trained batch 639 batch loss 1.17044091 epoch total loss 1.23464537\n",
      "Trained batch 640 batch loss 1.23738444 epoch total loss 1.23464966\n",
      "Trained batch 641 batch loss 1.25758088 epoch total loss 1.23468542\n",
      "Trained batch 642 batch loss 1.17287624 epoch total loss 1.2345891\n",
      "Trained batch 643 batch loss 1.18958342 epoch total loss 1.23451912\n",
      "Trained batch 644 batch loss 1.24870706 epoch total loss 1.23454118\n",
      "Trained batch 645 batch loss 1.20759678 epoch total loss 1.23449934\n",
      "Trained batch 646 batch loss 1.25394464 epoch total loss 1.2345295\n",
      "Trained batch 647 batch loss 1.30690217 epoch total loss 1.23464131\n",
      "Trained batch 648 batch loss 1.0821557 epoch total loss 1.23440599\n",
      "Trained batch 649 batch loss 1.20639825 epoch total loss 1.23436284\n",
      "Trained batch 650 batch loss 1.24240232 epoch total loss 1.23437524\n",
      "Trained batch 651 batch loss 1.20098341 epoch total loss 1.23432398\n",
      "Trained batch 652 batch loss 1.22988784 epoch total loss 1.23431718\n",
      "Trained batch 653 batch loss 1.08804488 epoch total loss 1.23409319\n",
      "Trained batch 654 batch loss 1.03046489 epoch total loss 1.23378181\n",
      "Trained batch 655 batch loss 1.16597569 epoch total loss 1.23367822\n",
      "Trained batch 656 batch loss 1.21882164 epoch total loss 1.23365557\n",
      "Trained batch 657 batch loss 1.14375198 epoch total loss 1.23351872\n",
      "Trained batch 658 batch loss 1.21286368 epoch total loss 1.23348737\n",
      "Trained batch 659 batch loss 1.23232234 epoch total loss 1.23348558\n",
      "Trained batch 660 batch loss 1.38842809 epoch total loss 1.2337203\n",
      "Trained batch 661 batch loss 1.17011142 epoch total loss 1.2336241\n",
      "Trained batch 662 batch loss 1.13588572 epoch total loss 1.2334764\n",
      "Trained batch 663 batch loss 1.1947974 epoch total loss 1.23341811\n",
      "Trained batch 664 batch loss 1.24287987 epoch total loss 1.23343241\n",
      "Trained batch 665 batch loss 1.16089058 epoch total loss 1.23332322\n",
      "Trained batch 666 batch loss 0.985274136 epoch total loss 1.23295081\n",
      "Trained batch 667 batch loss 0.89885968 epoch total loss 1.23245\n",
      "Trained batch 668 batch loss 1.16696858 epoch total loss 1.23235202\n",
      "Trained batch 669 batch loss 1.18519163 epoch total loss 1.23228145\n",
      "Trained batch 670 batch loss 1.38135839 epoch total loss 1.23250401\n",
      "Trained batch 671 batch loss 1.39914763 epoch total loss 1.23275232\n",
      "Trained batch 672 batch loss 1.27851045 epoch total loss 1.23282039\n",
      "Trained batch 673 batch loss 1.17552876 epoch total loss 1.23273528\n",
      "Trained batch 674 batch loss 1.19045019 epoch total loss 1.23267257\n",
      "Trained batch 675 batch loss 1.19745779 epoch total loss 1.23262036\n",
      "Trained batch 676 batch loss 1.20071948 epoch total loss 1.23257315\n",
      "Trained batch 677 batch loss 1.21240366 epoch total loss 1.23254335\n",
      "Trained batch 678 batch loss 1.25877941 epoch total loss 1.23258209\n",
      "Trained batch 679 batch loss 1.25951838 epoch total loss 1.23262179\n",
      "Trained batch 680 batch loss 1.34149456 epoch total loss 1.23278189\n",
      "Trained batch 681 batch loss 1.3951472 epoch total loss 1.23302031\n",
      "Trained batch 682 batch loss 1.35601461 epoch total loss 1.23320067\n",
      "Trained batch 683 batch loss 1.27679706 epoch total loss 1.23326445\n",
      "Trained batch 684 batch loss 1.11275244 epoch total loss 1.23308825\n",
      "Trained batch 685 batch loss 1.29350853 epoch total loss 1.23317647\n",
      "Trained batch 686 batch loss 1.35741627 epoch total loss 1.23335755\n",
      "Trained batch 687 batch loss 1.1725167 epoch total loss 1.2332691\n",
      "Trained batch 688 batch loss 1.137712 epoch total loss 1.23313022\n",
      "Trained batch 689 batch loss 1.20713568 epoch total loss 1.23309243\n",
      "Trained batch 690 batch loss 1.07848585 epoch total loss 1.23286843\n",
      "Trained batch 691 batch loss 1.10295188 epoch total loss 1.23268044\n",
      "Trained batch 692 batch loss 1.19166744 epoch total loss 1.23262107\n",
      "Trained batch 693 batch loss 1.17253256 epoch total loss 1.23253441\n",
      "Trained batch 694 batch loss 1.08675063 epoch total loss 1.23232436\n",
      "Trained batch 695 batch loss 1.2853415 epoch total loss 1.23240066\n",
      "Trained batch 696 batch loss 1.26341224 epoch total loss 1.23244524\n",
      "Trained batch 697 batch loss 1.32828104 epoch total loss 1.23258269\n",
      "Trained batch 698 batch loss 1.30995178 epoch total loss 1.23269355\n",
      "Trained batch 699 batch loss 1.51095402 epoch total loss 1.23309159\n",
      "Trained batch 700 batch loss 1.33626962 epoch total loss 1.23323894\n",
      "Trained batch 701 batch loss 1.33931601 epoch total loss 1.23339021\n",
      "Trained batch 702 batch loss 1.23286963 epoch total loss 1.2333895\n",
      "Trained batch 703 batch loss 1.07280636 epoch total loss 1.23316109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 704 batch loss 1.01355338 epoch total loss 1.23284912\n",
      "Trained batch 705 batch loss 1.22204924 epoch total loss 1.23283374\n",
      "Trained batch 706 batch loss 1.14550793 epoch total loss 1.23271012\n",
      "Trained batch 707 batch loss 0.989219964 epoch total loss 1.23236573\n",
      "Trained batch 708 batch loss 0.94749 epoch total loss 1.23196328\n",
      "Trained batch 709 batch loss 1.02482367 epoch total loss 1.23167121\n",
      "Trained batch 710 batch loss 1.1065203 epoch total loss 1.2314949\n",
      "Trained batch 711 batch loss 1.16370785 epoch total loss 1.23139954\n",
      "Trained batch 712 batch loss 1.22089672 epoch total loss 1.23138475\n",
      "Trained batch 713 batch loss 1.15337181 epoch total loss 1.23127544\n",
      "Trained batch 714 batch loss 1.25928712 epoch total loss 1.23131466\n",
      "Trained batch 715 batch loss 1.36253071 epoch total loss 1.23149812\n",
      "Trained batch 716 batch loss 1.12927938 epoch total loss 1.23135543\n",
      "Trained batch 717 batch loss 1.02272403 epoch total loss 1.23106432\n",
      "Trained batch 718 batch loss 1.11615038 epoch total loss 1.23090434\n",
      "Trained batch 719 batch loss 1.35574484 epoch total loss 1.23107803\n",
      "Trained batch 720 batch loss 1.34102583 epoch total loss 1.23123062\n",
      "Trained batch 721 batch loss 1.25402081 epoch total loss 1.23126233\n",
      "Trained batch 722 batch loss 1.31448114 epoch total loss 1.23137748\n",
      "Trained batch 723 batch loss 1.35350251 epoch total loss 1.2315464\n",
      "Trained batch 724 batch loss 1.32434189 epoch total loss 1.23167467\n",
      "Trained batch 725 batch loss 1.29523909 epoch total loss 1.23176229\n",
      "Trained batch 726 batch loss 1.30912685 epoch total loss 1.23186886\n",
      "Trained batch 727 batch loss 1.31053126 epoch total loss 1.23197711\n",
      "Trained batch 728 batch loss 1.22397411 epoch total loss 1.23196614\n",
      "Trained batch 729 batch loss 1.16207993 epoch total loss 1.23187029\n",
      "Trained batch 730 batch loss 1.02472615 epoch total loss 1.23158658\n",
      "Trained batch 731 batch loss 1.18462133 epoch total loss 1.23152232\n",
      "Trained batch 732 batch loss 1.12121511 epoch total loss 1.23137164\n",
      "Trained batch 733 batch loss 1.18844175 epoch total loss 1.23131299\n",
      "Trained batch 734 batch loss 1.26485217 epoch total loss 1.23135865\n",
      "Trained batch 735 batch loss 1.34204257 epoch total loss 1.23150921\n",
      "Trained batch 736 batch loss 1.25298047 epoch total loss 1.23153841\n",
      "Trained batch 737 batch loss 1.4166851 epoch total loss 1.23178971\n",
      "Trained batch 738 batch loss 1.47226918 epoch total loss 1.23211551\n",
      "Trained batch 739 batch loss 1.33860409 epoch total loss 1.23225963\n",
      "Trained batch 740 batch loss 1.23825 epoch total loss 1.23226774\n",
      "Trained batch 741 batch loss 1.4070704 epoch total loss 1.23250353\n",
      "Trained batch 742 batch loss 1.35900974 epoch total loss 1.23267412\n",
      "Trained batch 743 batch loss 1.3165741 epoch total loss 1.23278701\n",
      "Trained batch 744 batch loss 1.17440176 epoch total loss 1.23270845\n",
      "Trained batch 745 batch loss 1.20653677 epoch total loss 1.23267341\n",
      "Trained batch 746 batch loss 1.32379079 epoch total loss 1.23279548\n",
      "Trained batch 747 batch loss 1.20343733 epoch total loss 1.23275626\n",
      "Trained batch 748 batch loss 1.27153254 epoch total loss 1.23280811\n",
      "Trained batch 749 batch loss 1.2612375 epoch total loss 1.23284602\n",
      "Trained batch 750 batch loss 1.38062346 epoch total loss 1.23304307\n",
      "Trained batch 751 batch loss 1.34684253 epoch total loss 1.23319459\n",
      "Trained batch 752 batch loss 1.27896416 epoch total loss 1.23325551\n",
      "Trained batch 753 batch loss 1.24303842 epoch total loss 1.2332685\n",
      "Trained batch 754 batch loss 1.20242333 epoch total loss 1.23322761\n",
      "Trained batch 755 batch loss 1.13453138 epoch total loss 1.23309696\n",
      "Trained batch 756 batch loss 1.09394574 epoch total loss 1.23291278\n",
      "Trained batch 757 batch loss 1.29617774 epoch total loss 1.23299646\n",
      "Trained batch 758 batch loss 1.24079132 epoch total loss 1.23300672\n",
      "Trained batch 759 batch loss 1.3208549 epoch total loss 1.23312247\n",
      "Trained batch 760 batch loss 1.32329142 epoch total loss 1.23324108\n",
      "Trained batch 761 batch loss 1.35212243 epoch total loss 1.23339736\n",
      "Trained batch 762 batch loss 1.26470971 epoch total loss 1.23343837\n",
      "Trained batch 763 batch loss 1.22877502 epoch total loss 1.23343229\n",
      "Trained batch 764 batch loss 1.19930017 epoch total loss 1.23338759\n",
      "Trained batch 765 batch loss 1.27837574 epoch total loss 1.23344636\n",
      "Trained batch 766 batch loss 1.18169141 epoch total loss 1.23337889\n",
      "Trained batch 767 batch loss 1.13696194 epoch total loss 1.23325312\n",
      "Trained batch 768 batch loss 1.25549746 epoch total loss 1.23328209\n",
      "Trained batch 769 batch loss 1.39078104 epoch total loss 1.23348689\n",
      "Trained batch 770 batch loss 1.20639014 epoch total loss 1.23345172\n",
      "Trained batch 771 batch loss 1.21651077 epoch total loss 1.23342967\n",
      "Trained batch 772 batch loss 1.29876661 epoch total loss 1.23351431\n",
      "Trained batch 773 batch loss 1.11687708 epoch total loss 1.23336351\n",
      "Trained batch 774 batch loss 1.12072611 epoch total loss 1.23321795\n",
      "Trained batch 775 batch loss 1.18400121 epoch total loss 1.23315442\n",
      "Trained batch 776 batch loss 1.13328171 epoch total loss 1.23302579\n",
      "Trained batch 777 batch loss 1.10580826 epoch total loss 1.23286211\n",
      "Trained batch 778 batch loss 1.10946059 epoch total loss 1.23270345\n",
      "Trained batch 779 batch loss 1.10630882 epoch total loss 1.2325412\n",
      "Trained batch 780 batch loss 1.25859463 epoch total loss 1.23257458\n",
      "Trained batch 781 batch loss 1.13684607 epoch total loss 1.23245203\n",
      "Trained batch 782 batch loss 1.02816844 epoch total loss 1.23219085\n",
      "Trained batch 783 batch loss 1.09790933 epoch total loss 1.23201931\n",
      "Trained batch 784 batch loss 1.19685841 epoch total loss 1.23197448\n",
      "Trained batch 785 batch loss 1.09973276 epoch total loss 1.23180604\n",
      "Trained batch 786 batch loss 1.17879796 epoch total loss 1.23173857\n",
      "Trained batch 787 batch loss 1.04581022 epoch total loss 1.23150229\n",
      "Trained batch 788 batch loss 1.07642198 epoch total loss 1.23130548\n",
      "Trained batch 789 batch loss 1.27827096 epoch total loss 1.23136497\n",
      "Trained batch 790 batch loss 1.13700724 epoch total loss 1.23124564\n",
      "Trained batch 791 batch loss 1.12258685 epoch total loss 1.23110819\n",
      "Trained batch 792 batch loss 1.25425792 epoch total loss 1.23113739\n",
      "Trained batch 793 batch loss 1.16372633 epoch total loss 1.2310524\n",
      "Trained batch 794 batch loss 1.10485399 epoch total loss 1.23089349\n",
      "Trained batch 795 batch loss 1.10916114 epoch total loss 1.23074031\n",
      "Trained batch 796 batch loss 1.21338868 epoch total loss 1.23071849\n",
      "Trained batch 797 batch loss 1.36953783 epoch total loss 1.23089266\n",
      "Trained batch 798 batch loss 1.46985 epoch total loss 1.23119211\n",
      "Trained batch 799 batch loss 1.30294764 epoch total loss 1.23128188\n",
      "Trained batch 800 batch loss 1.20032048 epoch total loss 1.23124325\n",
      "Trained batch 801 batch loss 1.26637197 epoch total loss 1.231287\n",
      "Trained batch 802 batch loss 1.17538762 epoch total loss 1.23121738\n",
      "Trained batch 803 batch loss 0.926769853 epoch total loss 1.23083818\n",
      "Trained batch 804 batch loss 1.1206286 epoch total loss 1.23070109\n",
      "Trained batch 805 batch loss 1.24785876 epoch total loss 1.23072243\n",
      "Trained batch 806 batch loss 1.29080641 epoch total loss 1.23079705\n",
      "Trained batch 807 batch loss 1.28748274 epoch total loss 1.23086727\n",
      "Trained batch 808 batch loss 1.2371881 epoch total loss 1.23087502\n",
      "Trained batch 809 batch loss 1.27333426 epoch total loss 1.23092759\n",
      "Trained batch 810 batch loss 1.19117546 epoch total loss 1.23087847\n",
      "Trained batch 811 batch loss 1.13735747 epoch total loss 1.23076308\n",
      "Trained batch 812 batch loss 1.13338971 epoch total loss 1.23064315\n",
      "Trained batch 813 batch loss 1.23207223 epoch total loss 1.23064482\n",
      "Trained batch 814 batch loss 1.19082284 epoch total loss 1.23059595\n",
      "Trained batch 815 batch loss 1.31494904 epoch total loss 1.23069942\n",
      "Trained batch 816 batch loss 1.28695858 epoch total loss 1.23076844\n",
      "Trained batch 817 batch loss 1.19743192 epoch total loss 1.23072755\n",
      "Trained batch 818 batch loss 1.39757216 epoch total loss 1.23093164\n",
      "Trained batch 819 batch loss 1.31005454 epoch total loss 1.2310282\n",
      "Trained batch 820 batch loss 1.45495176 epoch total loss 1.23130131\n",
      "Trained batch 821 batch loss 1.32385874 epoch total loss 1.23141396\n",
      "Trained batch 822 batch loss 1.33763802 epoch total loss 1.2315433\n",
      "Trained batch 823 batch loss 1.33580244 epoch total loss 1.2316699\n",
      "Trained batch 824 batch loss 1.26142 epoch total loss 1.23170602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 825 batch loss 1.27493429 epoch total loss 1.23175848\n",
      "Trained batch 826 batch loss 1.27386916 epoch total loss 1.2318095\n",
      "Trained batch 827 batch loss 1.25208628 epoch total loss 1.23183393\n",
      "Trained batch 828 batch loss 1.20644152 epoch total loss 1.2318033\n",
      "Trained batch 829 batch loss 1.39847398 epoch total loss 1.2320044\n",
      "Trained batch 830 batch loss 1.45829248 epoch total loss 1.23227704\n",
      "Trained batch 831 batch loss 1.10760593 epoch total loss 1.23212695\n",
      "Trained batch 832 batch loss 1.28044629 epoch total loss 1.23218513\n",
      "Trained batch 833 batch loss 1.34224904 epoch total loss 1.23231721\n",
      "Trained batch 834 batch loss 1.29478693 epoch total loss 1.23239219\n",
      "Trained batch 835 batch loss 1.3079536 epoch total loss 1.23248267\n",
      "Trained batch 836 batch loss 1.26641905 epoch total loss 1.23252332\n",
      "Trained batch 837 batch loss 1.16154706 epoch total loss 1.23243845\n",
      "Trained batch 838 batch loss 1.19544303 epoch total loss 1.23239434\n",
      "Trained batch 839 batch loss 1.23297215 epoch total loss 1.23239505\n",
      "Trained batch 840 batch loss 1.2995019 epoch total loss 1.23247504\n",
      "Trained batch 841 batch loss 1.19861269 epoch total loss 1.23243475\n",
      "Trained batch 842 batch loss 1.10570693 epoch total loss 1.23228431\n",
      "Trained batch 843 batch loss 1.25919175 epoch total loss 1.23231614\n",
      "Trained batch 844 batch loss 1.15981817 epoch total loss 1.23223019\n",
      "Trained batch 845 batch loss 1.21893656 epoch total loss 1.23221457\n",
      "Trained batch 846 batch loss 1.2123704 epoch total loss 1.23219121\n",
      "Trained batch 847 batch loss 1.11717558 epoch total loss 1.23205543\n",
      "Trained batch 848 batch loss 1.13414311 epoch total loss 1.23193991\n",
      "Trained batch 849 batch loss 1.14457035 epoch total loss 1.23183692\n",
      "Trained batch 850 batch loss 1.25488305 epoch total loss 1.23186409\n",
      "Trained batch 851 batch loss 1.1967845 epoch total loss 1.23182285\n",
      "Trained batch 852 batch loss 1.25743389 epoch total loss 1.23185289\n",
      "Trained batch 853 batch loss 1.25014722 epoch total loss 1.23187435\n",
      "Trained batch 854 batch loss 1.45132172 epoch total loss 1.23213124\n",
      "Trained batch 855 batch loss 1.18027198 epoch total loss 1.23207068\n",
      "Trained batch 856 batch loss 1.29912901 epoch total loss 1.23214889\n",
      "Trained batch 857 batch loss 1.25677264 epoch total loss 1.23217762\n",
      "Trained batch 858 batch loss 1.20999599 epoch total loss 1.23215175\n",
      "Trained batch 859 batch loss 1.29652047 epoch total loss 1.23222661\n",
      "Trained batch 860 batch loss 1.31201959 epoch total loss 1.23231936\n",
      "Trained batch 861 batch loss 1.29424846 epoch total loss 1.23239124\n",
      "Trained batch 862 batch loss 1.20038927 epoch total loss 1.23235416\n",
      "Trained batch 863 batch loss 1.27523518 epoch total loss 1.23240387\n",
      "Trained batch 864 batch loss 1.32004178 epoch total loss 1.23250532\n",
      "Trained batch 865 batch loss 1.46738076 epoch total loss 1.23277688\n",
      "Trained batch 866 batch loss 1.18956 epoch total loss 1.23272705\n",
      "Trained batch 867 batch loss 1.23354936 epoch total loss 1.232728\n",
      "Trained batch 868 batch loss 1.27981687 epoch total loss 1.23278213\n",
      "Trained batch 869 batch loss 1.33448017 epoch total loss 1.23289919\n",
      "Trained batch 870 batch loss 1.32764864 epoch total loss 1.23300815\n",
      "Trained batch 871 batch loss 1.31741071 epoch total loss 1.23310494\n",
      "Trained batch 872 batch loss 1.30237615 epoch total loss 1.23318434\n",
      "Trained batch 873 batch loss 1.3777144 epoch total loss 1.23334992\n",
      "Trained batch 874 batch loss 1.16136146 epoch total loss 1.23326755\n",
      "Trained batch 875 batch loss 1.19739318 epoch total loss 1.23322654\n",
      "Trained batch 876 batch loss 1.03615177 epoch total loss 1.23300159\n",
      "Trained batch 877 batch loss 1.05493033 epoch total loss 1.23279858\n",
      "Trained batch 878 batch loss 1.03666365 epoch total loss 1.23257506\n",
      "Trained batch 879 batch loss 1.1313796 epoch total loss 1.2324599\n",
      "Trained batch 880 batch loss 1.10820413 epoch total loss 1.23231864\n",
      "Trained batch 881 batch loss 0.977921128 epoch total loss 1.23202991\n",
      "Trained batch 882 batch loss 0.916467369 epoch total loss 1.23167217\n",
      "Trained batch 883 batch loss 0.882565379 epoch total loss 1.23127675\n",
      "Trained batch 884 batch loss 1.00598049 epoch total loss 1.23102188\n",
      "Trained batch 885 batch loss 1.09880424 epoch total loss 1.23087251\n",
      "Trained batch 886 batch loss 1.18509686 epoch total loss 1.23082078\n",
      "Trained batch 887 batch loss 1.20108533 epoch total loss 1.23078716\n",
      "Trained batch 888 batch loss 1.18679929 epoch total loss 1.23073769\n",
      "Trained batch 889 batch loss 1.30637991 epoch total loss 1.23082268\n",
      "Trained batch 890 batch loss 1.3164115 epoch total loss 1.23091888\n",
      "Trained batch 891 batch loss 1.31683135 epoch total loss 1.23101521\n",
      "Trained batch 892 batch loss 1.30542207 epoch total loss 1.23109865\n",
      "Trained batch 893 batch loss 1.2510035 epoch total loss 1.23112094\n",
      "Trained batch 894 batch loss 1.30815589 epoch total loss 1.23120701\n",
      "Trained batch 895 batch loss 1.33800054 epoch total loss 1.23132634\n",
      "Trained batch 896 batch loss 1.24455094 epoch total loss 1.23134112\n",
      "Trained batch 897 batch loss 1.24796569 epoch total loss 1.2313596\n",
      "Trained batch 898 batch loss 1.19768846 epoch total loss 1.23132205\n",
      "Trained batch 899 batch loss 1.345474 epoch total loss 1.23144901\n",
      "Trained batch 900 batch loss 1.391752 epoch total loss 1.23162711\n",
      "Trained batch 901 batch loss 1.32188249 epoch total loss 1.23172724\n",
      "Trained batch 902 batch loss 1.16785145 epoch total loss 1.23165643\n",
      "Trained batch 903 batch loss 1.2341423 epoch total loss 1.23165917\n",
      "Trained batch 904 batch loss 1.29541206 epoch total loss 1.23172975\n",
      "Trained batch 905 batch loss 1.14568973 epoch total loss 1.2316345\n",
      "Trained batch 906 batch loss 1.40780079 epoch total loss 1.23182905\n",
      "Trained batch 907 batch loss 1.42679346 epoch total loss 1.23204398\n",
      "Trained batch 908 batch loss 1.28040814 epoch total loss 1.23209715\n",
      "Trained batch 909 batch loss 1.27436 epoch total loss 1.23214376\n",
      "Trained batch 910 batch loss 1.09603715 epoch total loss 1.23199427\n",
      "Trained batch 911 batch loss 1.26810968 epoch total loss 1.23203385\n",
      "Trained batch 912 batch loss 1.17278397 epoch total loss 1.23196876\n",
      "Trained batch 913 batch loss 1.21210599 epoch total loss 1.23194706\n",
      "Trained batch 914 batch loss 1.17707455 epoch total loss 1.2318871\n",
      "Trained batch 915 batch loss 1.10611296 epoch total loss 1.23174965\n",
      "Trained batch 916 batch loss 1.1457814 epoch total loss 1.23165572\n",
      "Trained batch 917 batch loss 1.14106297 epoch total loss 1.23155701\n",
      "Trained batch 918 batch loss 1.26240623 epoch total loss 1.23159063\n",
      "Trained batch 919 batch loss 1.26534796 epoch total loss 1.23162746\n",
      "Trained batch 920 batch loss 1.26623464 epoch total loss 1.23166502\n",
      "Trained batch 921 batch loss 1.19072413 epoch total loss 1.23162055\n",
      "Trained batch 922 batch loss 1.11909258 epoch total loss 1.23149848\n",
      "Trained batch 923 batch loss 1.30158889 epoch total loss 1.23157454\n",
      "Trained batch 924 batch loss 1.29629028 epoch total loss 1.23164451\n",
      "Trained batch 925 batch loss 1.23798656 epoch total loss 1.23165143\n",
      "Trained batch 926 batch loss 1.23876262 epoch total loss 1.23165917\n",
      "Trained batch 927 batch loss 1.16559172 epoch total loss 1.23158789\n",
      "Trained batch 928 batch loss 1.16292024 epoch total loss 1.23151398\n",
      "Trained batch 929 batch loss 1.19769859 epoch total loss 1.23147762\n",
      "Trained batch 930 batch loss 1.17196548 epoch total loss 1.23141372\n",
      "Trained batch 931 batch loss 1.1940136 epoch total loss 1.23137343\n",
      "Trained batch 932 batch loss 1.19698858 epoch total loss 1.23133659\n",
      "Trained batch 933 batch loss 1.21550858 epoch total loss 1.23131955\n",
      "Trained batch 934 batch loss 1.10160768 epoch total loss 1.23118067\n",
      "Trained batch 935 batch loss 1.23365569 epoch total loss 1.23118329\n",
      "Trained batch 936 batch loss 1.07981586 epoch total loss 1.23102164\n",
      "Trained batch 937 batch loss 1.17302442 epoch total loss 1.23095965\n",
      "Trained batch 938 batch loss 1.20681953 epoch total loss 1.2309339\n",
      "Trained batch 939 batch loss 1.24706709 epoch total loss 1.23095107\n",
      "Trained batch 940 batch loss 1.05793524 epoch total loss 1.23076701\n",
      "Trained batch 941 batch loss 0.994614482 epoch total loss 1.23051608\n",
      "Trained batch 942 batch loss 1.06030655 epoch total loss 1.23033535\n",
      "Trained batch 943 batch loss 1.00784254 epoch total loss 1.23009944\n",
      "Trained batch 944 batch loss 1.1834923 epoch total loss 1.23005009\n",
      "Trained batch 945 batch loss 1.15986073 epoch total loss 1.22997582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 946 batch loss 1.27767229 epoch total loss 1.23002625\n",
      "Trained batch 947 batch loss 1.24638987 epoch total loss 1.23004353\n",
      "Trained batch 948 batch loss 1.16805589 epoch total loss 1.2299782\n",
      "Trained batch 949 batch loss 1.18655169 epoch total loss 1.22993231\n",
      "Trained batch 950 batch loss 1.08285213 epoch total loss 1.22977757\n",
      "Trained batch 951 batch loss 1.12496948 epoch total loss 1.22966743\n",
      "Trained batch 952 batch loss 1.2397449 epoch total loss 1.22967803\n",
      "Trained batch 953 batch loss 1.21015048 epoch total loss 1.22965753\n",
      "Trained batch 954 batch loss 1.15051532 epoch total loss 1.22957456\n",
      "Trained batch 955 batch loss 1.05723047 epoch total loss 1.2293942\n",
      "Trained batch 956 batch loss 1.13437939 epoch total loss 1.22929478\n",
      "Trained batch 957 batch loss 1.11245692 epoch total loss 1.22917271\n",
      "Trained batch 958 batch loss 1.23789608 epoch total loss 1.22918177\n",
      "Trained batch 959 batch loss 1.22083628 epoch total loss 1.22917306\n",
      "Trained batch 960 batch loss 1.18221927 epoch total loss 1.22912419\n",
      "Trained batch 961 batch loss 1.30208635 epoch total loss 1.22920012\n",
      "Trained batch 962 batch loss 1.19326353 epoch total loss 1.22916281\n",
      "Trained batch 963 batch loss 1.06335068 epoch total loss 1.22899055\n",
      "Trained batch 964 batch loss 1.13088071 epoch total loss 1.22888875\n",
      "Trained batch 965 batch loss 1.16870427 epoch total loss 1.2288264\n",
      "Trained batch 966 batch loss 1.16055512 epoch total loss 1.22875571\n",
      "Trained batch 967 batch loss 1.29497147 epoch total loss 1.22882414\n",
      "Trained batch 968 batch loss 1.23720765 epoch total loss 1.22883272\n",
      "Trained batch 969 batch loss 1.25920224 epoch total loss 1.22886407\n",
      "Trained batch 970 batch loss 1.21950257 epoch total loss 1.22885442\n",
      "Trained batch 971 batch loss 1.22023892 epoch total loss 1.22884548\n",
      "Trained batch 972 batch loss 1.1677134 epoch total loss 1.22878265\n",
      "Trained batch 973 batch loss 1.14773118 epoch total loss 1.22869933\n",
      "Trained batch 974 batch loss 1.25024533 epoch total loss 1.22872138\n",
      "Trained batch 975 batch loss 1.22670949 epoch total loss 1.22871935\n",
      "Trained batch 976 batch loss 1.19039845 epoch total loss 1.22868013\n",
      "Trained batch 977 batch loss 1.21876132 epoch total loss 1.22866988\n",
      "Trained batch 978 batch loss 1.17181849 epoch total loss 1.22861183\n",
      "Trained batch 979 batch loss 1.21917677 epoch total loss 1.22860217\n",
      "Trained batch 980 batch loss 1.4224546 epoch total loss 1.2288\n",
      "Trained batch 981 batch loss 1.16561544 epoch total loss 1.22873557\n",
      "Trained batch 982 batch loss 1.18802071 epoch total loss 1.22869408\n",
      "Trained batch 983 batch loss 1.17090869 epoch total loss 1.22863531\n",
      "Trained batch 984 batch loss 1.05339694 epoch total loss 1.22845721\n",
      "Trained batch 985 batch loss 1.18342364 epoch total loss 1.22841156\n",
      "Trained batch 986 batch loss 1.19401109 epoch total loss 1.22837663\n",
      "Trained batch 987 batch loss 1.31834471 epoch total loss 1.2284677\n",
      "Trained batch 988 batch loss 1.11995685 epoch total loss 1.22835803\n",
      "Trained batch 989 batch loss 1.05990851 epoch total loss 1.22818768\n",
      "Trained batch 990 batch loss 1.12124956 epoch total loss 1.22807968\n",
      "Trained batch 991 batch loss 1.42447674 epoch total loss 1.2282778\n",
      "Trained batch 992 batch loss 1.28781402 epoch total loss 1.22833776\n",
      "Trained batch 993 batch loss 1.16940355 epoch total loss 1.22827852\n",
      "Trained batch 994 batch loss 1.16211772 epoch total loss 1.22821188\n",
      "Trained batch 995 batch loss 0.971411526 epoch total loss 1.22795391\n",
      "Trained batch 996 batch loss 0.895451486 epoch total loss 1.22762012\n",
      "Trained batch 997 batch loss 0.917375684 epoch total loss 1.22730887\n",
      "Trained batch 998 batch loss 1.24540436 epoch total loss 1.22732699\n",
      "Trained batch 999 batch loss 1.35443306 epoch total loss 1.2274543\n",
      "Trained batch 1000 batch loss 1.21833551 epoch total loss 1.22744524\n",
      "Trained batch 1001 batch loss 1.25273824 epoch total loss 1.2274704\n",
      "Trained batch 1002 batch loss 1.15855324 epoch total loss 1.22740161\n",
      "Trained batch 1003 batch loss 1.19216681 epoch total loss 1.22736645\n",
      "Trained batch 1004 batch loss 1.32451916 epoch total loss 1.22746325\n",
      "Trained batch 1005 batch loss 1.09153056 epoch total loss 1.22732794\n",
      "Trained batch 1006 batch loss 1.06357992 epoch total loss 1.22716522\n",
      "Trained batch 1007 batch loss 1.04991281 epoch total loss 1.22698915\n",
      "Trained batch 1008 batch loss 1.17843199 epoch total loss 1.22694111\n",
      "Trained batch 1009 batch loss 1.07556057 epoch total loss 1.22679102\n",
      "Trained batch 1010 batch loss 1.01255488 epoch total loss 1.22657895\n",
      "Trained batch 1011 batch loss 1.12723923 epoch total loss 1.2264806\n",
      "Trained batch 1012 batch loss 1.23365033 epoch total loss 1.22648776\n",
      "Trained batch 1013 batch loss 1.20714104 epoch total loss 1.22646868\n",
      "Trained batch 1014 batch loss 1.18008709 epoch total loss 1.22642291\n",
      "Trained batch 1015 batch loss 1.13748491 epoch total loss 1.22633517\n",
      "Trained batch 1016 batch loss 1.19737411 epoch total loss 1.22630668\n",
      "Trained batch 1017 batch loss 1.39735973 epoch total loss 1.22647488\n",
      "Trained batch 1018 batch loss 1.28762698 epoch total loss 1.22653496\n",
      "Trained batch 1019 batch loss 1.20806074 epoch total loss 1.22651672\n",
      "Trained batch 1020 batch loss 1.16184831 epoch total loss 1.2264533\n",
      "Trained batch 1021 batch loss 1.27581406 epoch total loss 1.2265017\n",
      "Trained batch 1022 batch loss 1.25562668 epoch total loss 1.22653008\n",
      "Trained batch 1023 batch loss 1.21830785 epoch total loss 1.22652209\n",
      "Trained batch 1024 batch loss 1.18212152 epoch total loss 1.2264787\n",
      "Trained batch 1025 batch loss 1.24545932 epoch total loss 1.22649729\n",
      "Trained batch 1026 batch loss 1.15411711 epoch total loss 1.22642672\n",
      "Trained batch 1027 batch loss 1.08927405 epoch total loss 1.22629321\n",
      "Trained batch 1028 batch loss 1.19947886 epoch total loss 1.2262671\n",
      "Trained batch 1029 batch loss 1.21496844 epoch total loss 1.22625613\n",
      "Trained batch 1030 batch loss 1.21363854 epoch total loss 1.22624385\n",
      "Trained batch 1031 batch loss 1.14471102 epoch total loss 1.2261647\n",
      "Trained batch 1032 batch loss 1.16085148 epoch total loss 1.2261014\n",
      "Trained batch 1033 batch loss 1.09637237 epoch total loss 1.22597575\n",
      "Trained batch 1034 batch loss 1.14918208 epoch total loss 1.22590148\n",
      "Trained batch 1035 batch loss 1.17264771 epoch total loss 1.22585\n",
      "Trained batch 1036 batch loss 1.0917871 epoch total loss 1.22572064\n",
      "Trained batch 1037 batch loss 1.19110322 epoch total loss 1.22568727\n",
      "Trained batch 1038 batch loss 1.13703501 epoch total loss 1.22560191\n",
      "Trained batch 1039 batch loss 1.21222925 epoch total loss 1.22558916\n",
      "Trained batch 1040 batch loss 1.30509233 epoch total loss 1.22566557\n",
      "Trained batch 1041 batch loss 1.21773219 epoch total loss 1.22565794\n",
      "Trained batch 1042 batch loss 1.29015207 epoch total loss 1.22571981\n",
      "Trained batch 1043 batch loss 1.23551035 epoch total loss 1.22572923\n",
      "Trained batch 1044 batch loss 1.23601067 epoch total loss 1.225739\n",
      "Trained batch 1045 batch loss 1.22827172 epoch total loss 1.22574139\n",
      "Trained batch 1046 batch loss 1.29276347 epoch total loss 1.2258054\n",
      "Trained batch 1047 batch loss 1.25243592 epoch total loss 1.22583091\n",
      "Trained batch 1048 batch loss 1.25042605 epoch total loss 1.22585428\n",
      "Trained batch 1049 batch loss 1.254318 epoch total loss 1.22588134\n",
      "Trained batch 1050 batch loss 1.30014563 epoch total loss 1.22595215\n",
      "Trained batch 1051 batch loss 1.39715099 epoch total loss 1.22611499\n",
      "Trained batch 1052 batch loss 1.24757469 epoch total loss 1.22613537\n",
      "Trained batch 1053 batch loss 1.39092052 epoch total loss 1.22629178\n",
      "Trained batch 1054 batch loss 1.26636064 epoch total loss 1.2263298\n",
      "Trained batch 1055 batch loss 1.14724648 epoch total loss 1.22625482\n",
      "Trained batch 1056 batch loss 1.22258091 epoch total loss 1.22625136\n",
      "Trained batch 1057 batch loss 1.22726274 epoch total loss 1.22625232\n",
      "Trained batch 1058 batch loss 1.22730553 epoch total loss 1.22625327\n",
      "Trained batch 1059 batch loss 1.24837899 epoch total loss 1.22627425\n",
      "Trained batch 1060 batch loss 1.21751511 epoch total loss 1.22626591\n",
      "Trained batch 1061 batch loss 1.22964287 epoch total loss 1.22626913\n",
      "Trained batch 1062 batch loss 1.24262118 epoch total loss 1.22628462\n",
      "Trained batch 1063 batch loss 1.24735856 epoch total loss 1.22630429\n",
      "Trained batch 1064 batch loss 1.2634995 epoch total loss 1.22633934\n",
      "Trained batch 1065 batch loss 1.25725913 epoch total loss 1.22636831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1066 batch loss 1.47865057 epoch total loss 1.22660494\n",
      "Trained batch 1067 batch loss 1.44140148 epoch total loss 1.22680628\n",
      "Trained batch 1068 batch loss 1.40593612 epoch total loss 1.22697401\n",
      "Trained batch 1069 batch loss 1.48946309 epoch total loss 1.22721958\n",
      "Trained batch 1070 batch loss 1.26312649 epoch total loss 1.2272532\n",
      "Trained batch 1071 batch loss 1.1828295 epoch total loss 1.22721171\n",
      "Trained batch 1072 batch loss 1.14825392 epoch total loss 1.22713804\n",
      "Trained batch 1073 batch loss 1.13255382 epoch total loss 1.22704983\n",
      "Trained batch 1074 batch loss 1.28497863 epoch total loss 1.22710383\n",
      "Trained batch 1075 batch loss 1.26505184 epoch total loss 1.22713912\n",
      "Trained batch 1076 batch loss 1.27475607 epoch total loss 1.22718346\n",
      "Trained batch 1077 batch loss 1.16852152 epoch total loss 1.22712898\n",
      "Trained batch 1078 batch loss 1.19760871 epoch total loss 1.22710168\n",
      "Trained batch 1079 batch loss 1.04873824 epoch total loss 1.22693634\n",
      "Trained batch 1080 batch loss 1.1054647 epoch total loss 1.22682381\n",
      "Trained batch 1081 batch loss 1.18108499 epoch total loss 1.22678149\n",
      "Trained batch 1082 batch loss 1.18194497 epoch total loss 1.22674\n",
      "Trained batch 1083 batch loss 1.13429868 epoch total loss 1.22665453\n",
      "Trained batch 1084 batch loss 1.20519519 epoch total loss 1.22663474\n",
      "Trained batch 1085 batch loss 1.16732109 epoch total loss 1.22658014\n",
      "Trained batch 1086 batch loss 1.41552222 epoch total loss 1.22675419\n",
      "Trained batch 1087 batch loss 1.31928 epoch total loss 1.2268393\n",
      "Trained batch 1088 batch loss 1.46669424 epoch total loss 1.22705972\n",
      "Trained batch 1089 batch loss 1.3694526 epoch total loss 1.22719061\n",
      "Trained batch 1090 batch loss 1.38745511 epoch total loss 1.2273376\n",
      "Trained batch 1091 batch loss 1.23285496 epoch total loss 1.22734272\n",
      "Trained batch 1092 batch loss 1.28850412 epoch total loss 1.22739863\n",
      "Trained batch 1093 batch loss 1.20330596 epoch total loss 1.22737658\n",
      "Trained batch 1094 batch loss 1.28674936 epoch total loss 1.22743082\n",
      "Trained batch 1095 batch loss 1.21516407 epoch total loss 1.22741961\n",
      "Trained batch 1096 batch loss 1.23611331 epoch total loss 1.2274276\n",
      "Trained batch 1097 batch loss 1.26098931 epoch total loss 1.22745812\n",
      "Trained batch 1098 batch loss 1.19459069 epoch total loss 1.2274282\n",
      "Trained batch 1099 batch loss 1.16206145 epoch total loss 1.22736883\n",
      "Trained batch 1100 batch loss 1.08495557 epoch total loss 1.22723937\n",
      "Trained batch 1101 batch loss 1.05851507 epoch total loss 1.22708607\n",
      "Trained batch 1102 batch loss 1.1447047 epoch total loss 1.2270112\n",
      "Trained batch 1103 batch loss 1.24537563 epoch total loss 1.22702789\n",
      "Trained batch 1104 batch loss 1.40554547 epoch total loss 1.22718954\n",
      "Trained batch 1105 batch loss 1.41584241 epoch total loss 1.22736037\n",
      "Trained batch 1106 batch loss 1.22344136 epoch total loss 1.22735667\n",
      "Trained batch 1107 batch loss 1.27900314 epoch total loss 1.2274034\n",
      "Trained batch 1108 batch loss 1.18724608 epoch total loss 1.22736716\n",
      "Trained batch 1109 batch loss 1.21134925 epoch total loss 1.22735274\n",
      "Trained batch 1110 batch loss 1.19420099 epoch total loss 1.22732282\n",
      "Trained batch 1111 batch loss 1.20048189 epoch total loss 1.22729862\n",
      "Trained batch 1112 batch loss 1.41117501 epoch total loss 1.22746396\n",
      "Trained batch 1113 batch loss 1.41071153 epoch total loss 1.22762871\n",
      "Trained batch 1114 batch loss 1.25386906 epoch total loss 1.22765231\n",
      "Trained batch 1115 batch loss 1.3382529 epoch total loss 1.22775149\n",
      "Trained batch 1116 batch loss 1.34480727 epoch total loss 1.2278564\n",
      "Trained batch 1117 batch loss 1.36185861 epoch total loss 1.22797632\n",
      "Trained batch 1118 batch loss 1.26875913 epoch total loss 1.2280128\n",
      "Trained batch 1119 batch loss 1.24176478 epoch total loss 1.2280252\n",
      "Trained batch 1120 batch loss 1.21062064 epoch total loss 1.22800958\n",
      "Trained batch 1121 batch loss 1.16254866 epoch total loss 1.22795117\n",
      "Trained batch 1122 batch loss 1.19775271 epoch total loss 1.22792435\n",
      "Trained batch 1123 batch loss 1.30980158 epoch total loss 1.22799718\n",
      "Trained batch 1124 batch loss 1.19602108 epoch total loss 1.22796881\n",
      "Trained batch 1125 batch loss 1.17150903 epoch total loss 1.22791862\n",
      "Trained batch 1126 batch loss 1.09583879 epoch total loss 1.22780132\n",
      "Trained batch 1127 batch loss 1.22051489 epoch total loss 1.22779477\n",
      "Trained batch 1128 batch loss 1.29648614 epoch total loss 1.22785568\n",
      "Trained batch 1129 batch loss 1.37556958 epoch total loss 1.22798657\n",
      "Trained batch 1130 batch loss 1.17881083 epoch total loss 1.22794306\n",
      "Trained batch 1131 batch loss 1.11258841 epoch total loss 1.22784102\n",
      "Trained batch 1132 batch loss 1.103019 epoch total loss 1.22773075\n",
      "Trained batch 1133 batch loss 1.30023324 epoch total loss 1.22779489\n",
      "Trained batch 1134 batch loss 1.36062372 epoch total loss 1.22791195\n",
      "Trained batch 1135 batch loss 1.28905272 epoch total loss 1.22796583\n",
      "Trained batch 1136 batch loss 1.2873801 epoch total loss 1.22801805\n",
      "Trained batch 1137 batch loss 1.28800106 epoch total loss 1.22807086\n",
      "Trained batch 1138 batch loss 1.30750847 epoch total loss 1.22814059\n",
      "Trained batch 1139 batch loss 1.16791415 epoch total loss 1.22808778\n",
      "Trained batch 1140 batch loss 1.22742093 epoch total loss 1.22808719\n",
      "Trained batch 1141 batch loss 1.26457858 epoch total loss 1.22811913\n",
      "Trained batch 1142 batch loss 1.26334023 epoch total loss 1.22814989\n",
      "Trained batch 1143 batch loss 1.21072173 epoch total loss 1.22813463\n",
      "Trained batch 1144 batch loss 1.19204378 epoch total loss 1.22810316\n",
      "Trained batch 1145 batch loss 1.19524717 epoch total loss 1.22807431\n",
      "Trained batch 1146 batch loss 1.2394464 epoch total loss 1.22808433\n",
      "Trained batch 1147 batch loss 1.28503764 epoch total loss 1.22813404\n",
      "Trained batch 1148 batch loss 1.30398989 epoch total loss 1.22820008\n",
      "Trained batch 1149 batch loss 1.23266852 epoch total loss 1.22820389\n",
      "Trained batch 1150 batch loss 1.3490864 epoch total loss 1.22830904\n",
      "Trained batch 1151 batch loss 1.27544761 epoch total loss 1.22834992\n",
      "Trained batch 1152 batch loss 1.19103158 epoch total loss 1.22831762\n",
      "Trained batch 1153 batch loss 1.22075725 epoch total loss 1.22831094\n",
      "Trained batch 1154 batch loss 1.26934874 epoch total loss 1.22834659\n",
      "Trained batch 1155 batch loss 1.18413663 epoch total loss 1.22830832\n",
      "Trained batch 1156 batch loss 1.26962566 epoch total loss 1.22834408\n",
      "Trained batch 1157 batch loss 1.10074186 epoch total loss 1.2282337\n",
      "Trained batch 1158 batch loss 1.15008473 epoch total loss 1.22816622\n",
      "Trained batch 1159 batch loss 1.20234954 epoch total loss 1.22814393\n",
      "Trained batch 1160 batch loss 1.22882414 epoch total loss 1.22814453\n",
      "Trained batch 1161 batch loss 1.27215028 epoch total loss 1.22818244\n",
      "Trained batch 1162 batch loss 1.19385123 epoch total loss 1.22815287\n",
      "Trained batch 1163 batch loss 1.24515605 epoch total loss 1.22816741\n",
      "Trained batch 1164 batch loss 1.27005506 epoch total loss 1.22820342\n",
      "Trained batch 1165 batch loss 1.27618849 epoch total loss 1.22824466\n",
      "Trained batch 1166 batch loss 1.23315847 epoch total loss 1.22824883\n",
      "Trained batch 1167 batch loss 1.21916962 epoch total loss 1.22824109\n",
      "Trained batch 1168 batch loss 1.04894388 epoch total loss 1.22808754\n",
      "Trained batch 1169 batch loss 1.17932689 epoch total loss 1.22804582\n",
      "Trained batch 1170 batch loss 1.1521672 epoch total loss 1.22798097\n",
      "Trained batch 1171 batch loss 1.26108718 epoch total loss 1.22800934\n",
      "Trained batch 1172 batch loss 1.25179172 epoch total loss 1.22802961\n",
      "Trained batch 1173 batch loss 1.19011927 epoch total loss 1.2279973\n",
      "Trained batch 1174 batch loss 1.24058127 epoch total loss 1.22800803\n",
      "Trained batch 1175 batch loss 1.12590766 epoch total loss 1.22792101\n",
      "Trained batch 1176 batch loss 1.08580732 epoch total loss 1.22780025\n",
      "Trained batch 1177 batch loss 1.2517494 epoch total loss 1.22782052\n",
      "Trained batch 1178 batch loss 1.10863793 epoch total loss 1.22771943\n",
      "Trained batch 1179 batch loss 1.10238504 epoch total loss 1.22761309\n",
      "Trained batch 1180 batch loss 1.0826416 epoch total loss 1.22749019\n",
      "Trained batch 1181 batch loss 1.17591119 epoch total loss 1.22744656\n",
      "Trained batch 1182 batch loss 1.09996808 epoch total loss 1.22733867\n",
      "Trained batch 1183 batch loss 1.12398171 epoch total loss 1.22725141\n",
      "Trained batch 1184 batch loss 1.19605327 epoch total loss 1.22722507\n",
      "Trained batch 1185 batch loss 1.18413723 epoch total loss 1.22718859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1186 batch loss 1.16763115 epoch total loss 1.2271384\n",
      "Trained batch 1187 batch loss 1.15422165 epoch total loss 1.22707689\n",
      "Trained batch 1188 batch loss 1.16911983 epoch total loss 1.22702813\n",
      "Trained batch 1189 batch loss 1.32292616 epoch total loss 1.22710872\n",
      "Trained batch 1190 batch loss 1.27684474 epoch total loss 1.22715044\n",
      "Trained batch 1191 batch loss 1.25263941 epoch total loss 1.2271719\n",
      "Trained batch 1192 batch loss 1.2657907 epoch total loss 1.22720432\n",
      "Trained batch 1193 batch loss 1.22815883 epoch total loss 1.22720504\n",
      "Trained batch 1194 batch loss 1.20048642 epoch total loss 1.22718263\n",
      "Trained batch 1195 batch loss 1.09530854 epoch total loss 1.22707236\n",
      "Trained batch 1196 batch loss 1.11843348 epoch total loss 1.22698152\n",
      "Trained batch 1197 batch loss 1.25766063 epoch total loss 1.22700715\n",
      "Trained batch 1198 batch loss 1.21921992 epoch total loss 1.22700059\n",
      "Trained batch 1199 batch loss 1.33476901 epoch total loss 1.22709048\n",
      "Trained batch 1200 batch loss 1.17579031 epoch total loss 1.22704768\n",
      "Trained batch 1201 batch loss 1.18471491 epoch total loss 1.22701252\n",
      "Trained batch 1202 batch loss 1.32092631 epoch total loss 1.2270906\n",
      "Trained batch 1203 batch loss 1.26843083 epoch total loss 1.22712493\n",
      "Trained batch 1204 batch loss 1.15699792 epoch total loss 1.22706676\n",
      "Trained batch 1205 batch loss 1.19807899 epoch total loss 1.22704268\n",
      "Trained batch 1206 batch loss 1.15325129 epoch total loss 1.2269814\n",
      "Trained batch 1207 batch loss 1.20603454 epoch total loss 1.22696412\n",
      "Trained batch 1208 batch loss 1.15171051 epoch total loss 1.22690189\n",
      "Trained batch 1209 batch loss 1.12890828 epoch total loss 1.22682083\n",
      "Trained batch 1210 batch loss 1.26148808 epoch total loss 1.22684944\n",
      "Trained batch 1211 batch loss 1.10444295 epoch total loss 1.22674835\n",
      "Trained batch 1212 batch loss 1.05625558 epoch total loss 1.22660768\n",
      "Trained batch 1213 batch loss 1.09990668 epoch total loss 1.22650325\n",
      "Trained batch 1214 batch loss 1.17408228 epoch total loss 1.22646\n",
      "Trained batch 1215 batch loss 1.25877404 epoch total loss 1.22648668\n",
      "Trained batch 1216 batch loss 1.22156942 epoch total loss 1.22648263\n",
      "Trained batch 1217 batch loss 1.2421788 epoch total loss 1.2264955\n",
      "Trained batch 1218 batch loss 1.30030596 epoch total loss 1.22655606\n",
      "Trained batch 1219 batch loss 1.22804046 epoch total loss 1.22655725\n",
      "Trained batch 1220 batch loss 1.27535963 epoch total loss 1.22659731\n",
      "Trained batch 1221 batch loss 1.31258142 epoch total loss 1.22666776\n",
      "Trained batch 1222 batch loss 1.33019459 epoch total loss 1.22675252\n",
      "Trained batch 1223 batch loss 1.26068604 epoch total loss 1.2267803\n",
      "Trained batch 1224 batch loss 1.25856221 epoch total loss 1.22680628\n",
      "Trained batch 1225 batch loss 1.20851 epoch total loss 1.22679126\n",
      "Trained batch 1226 batch loss 1.2660377 epoch total loss 1.22682321\n",
      "Trained batch 1227 batch loss 1.36014521 epoch total loss 1.22693193\n",
      "Trained batch 1228 batch loss 1.3279314 epoch total loss 1.22701406\n",
      "Trained batch 1229 batch loss 1.23434544 epoch total loss 1.22702014\n",
      "Trained batch 1230 batch loss 1.22004068 epoch total loss 1.22701442\n",
      "Trained batch 1231 batch loss 0.998136878 epoch total loss 1.22682858\n",
      "Trained batch 1232 batch loss 1.04356241 epoch total loss 1.2266798\n",
      "Trained batch 1233 batch loss 1.14587784 epoch total loss 1.22661424\n",
      "Trained batch 1234 batch loss 1.2302593 epoch total loss 1.22661722\n",
      "Trained batch 1235 batch loss 1.30141342 epoch total loss 1.22667778\n",
      "Trained batch 1236 batch loss 1.31241345 epoch total loss 1.22674704\n",
      "Trained batch 1237 batch loss 1.28798151 epoch total loss 1.22679663\n",
      "Trained batch 1238 batch loss 1.18418658 epoch total loss 1.22676218\n",
      "Trained batch 1239 batch loss 1.03382134 epoch total loss 1.22660649\n",
      "Trained batch 1240 batch loss 1.00529468 epoch total loss 1.22642791\n",
      "Trained batch 1241 batch loss 1.06962836 epoch total loss 1.22630155\n",
      "Trained batch 1242 batch loss 1.18081284 epoch total loss 1.22626483\n",
      "Trained batch 1243 batch loss 1.30882132 epoch total loss 1.22633135\n",
      "Trained batch 1244 batch loss 1.32056737 epoch total loss 1.22640705\n",
      "Trained batch 1245 batch loss 1.48570371 epoch total loss 1.22661531\n",
      "Trained batch 1246 batch loss 1.15458059 epoch total loss 1.22655749\n",
      "Trained batch 1247 batch loss 1.06283593 epoch total loss 1.22642624\n",
      "Trained batch 1248 batch loss 1.21530807 epoch total loss 1.2264173\n",
      "Trained batch 1249 batch loss 1.33985591 epoch total loss 1.22650814\n",
      "Trained batch 1250 batch loss 1.30094028 epoch total loss 1.22656763\n",
      "Trained batch 1251 batch loss 1.21410131 epoch total loss 1.22655773\n",
      "Trained batch 1252 batch loss 1.25419462 epoch total loss 1.22657979\n",
      "Trained batch 1253 batch loss 1.40692592 epoch total loss 1.22672379\n",
      "Trained batch 1254 batch loss 1.2555871 epoch total loss 1.2267468\n",
      "Trained batch 1255 batch loss 1.28848672 epoch total loss 1.22679591\n",
      "Trained batch 1256 batch loss 1.18219852 epoch total loss 1.22676051\n",
      "Trained batch 1257 batch loss 1.08680165 epoch total loss 1.22664917\n",
      "Trained batch 1258 batch loss 1.25159132 epoch total loss 1.22666895\n",
      "Trained batch 1259 batch loss 1.21226931 epoch total loss 1.22665751\n",
      "Trained batch 1260 batch loss 1.20208943 epoch total loss 1.22663808\n",
      "Trained batch 1261 batch loss 1.23029947 epoch total loss 1.22664106\n",
      "Trained batch 1262 batch loss 1.27730823 epoch total loss 1.22668123\n",
      "Trained batch 1263 batch loss 1.23317516 epoch total loss 1.22668636\n",
      "Trained batch 1264 batch loss 1.34454179 epoch total loss 1.22677946\n",
      "Trained batch 1265 batch loss 1.2000792 epoch total loss 1.22675836\n",
      "Trained batch 1266 batch loss 1.32912743 epoch total loss 1.22683918\n",
      "Trained batch 1267 batch loss 1.18143952 epoch total loss 1.22680342\n",
      "Trained batch 1268 batch loss 1.25690627 epoch total loss 1.22682714\n",
      "Trained batch 1269 batch loss 1.22711623 epoch total loss 1.22682738\n",
      "Trained batch 1270 batch loss 1.30484629 epoch total loss 1.22688878\n",
      "Trained batch 1271 batch loss 1.24247956 epoch total loss 1.22690105\n",
      "Trained batch 1272 batch loss 1.13578343 epoch total loss 1.22682941\n",
      "Trained batch 1273 batch loss 1.05936265 epoch total loss 1.2266978\n",
      "Trained batch 1274 batch loss 1.07998955 epoch total loss 1.22658265\n",
      "Trained batch 1275 batch loss 1.13277614 epoch total loss 1.22650909\n",
      "Trained batch 1276 batch loss 1.115242 epoch total loss 1.22642183\n",
      "Trained batch 1277 batch loss 1.11173666 epoch total loss 1.22633207\n",
      "Trained batch 1278 batch loss 1.00704634 epoch total loss 1.22616053\n",
      "Trained batch 1279 batch loss 1.15961277 epoch total loss 1.22610843\n",
      "Trained batch 1280 batch loss 1.15037203 epoch total loss 1.2260493\n",
      "Trained batch 1281 batch loss 1.14881587 epoch total loss 1.22598898\n",
      "Trained batch 1282 batch loss 1.32893205 epoch total loss 1.22606933\n",
      "Trained batch 1283 batch loss 1.39888644 epoch total loss 1.22620404\n",
      "Trained batch 1284 batch loss 1.38936245 epoch total loss 1.22633123\n",
      "Trained batch 1285 batch loss 1.35753775 epoch total loss 1.22643328\n",
      "Trained batch 1286 batch loss 1.18873048 epoch total loss 1.22640395\n",
      "Trained batch 1287 batch loss 1.14594913 epoch total loss 1.22634149\n",
      "Trained batch 1288 batch loss 1.10517204 epoch total loss 1.22624743\n",
      "Trained batch 1289 batch loss 1.27144444 epoch total loss 1.2262826\n",
      "Trained batch 1290 batch loss 1.24390411 epoch total loss 1.22629619\n",
      "Trained batch 1291 batch loss 1.06496012 epoch total loss 1.22617126\n",
      "Trained batch 1292 batch loss 1.11203897 epoch total loss 1.22608292\n",
      "Trained batch 1293 batch loss 1.08757877 epoch total loss 1.22597575\n",
      "Trained batch 1294 batch loss 0.980187058 epoch total loss 1.22578585\n",
      "Trained batch 1295 batch loss 1.25259614 epoch total loss 1.22580647\n",
      "Trained batch 1296 batch loss 1.40110397 epoch total loss 1.22594178\n",
      "Trained batch 1297 batch loss 1.43393469 epoch total loss 1.22610211\n",
      "Trained batch 1298 batch loss 1.33617735 epoch total loss 1.22618699\n",
      "Trained batch 1299 batch loss 1.41580772 epoch total loss 1.2263329\n",
      "Trained batch 1300 batch loss 1.43051875 epoch total loss 1.22649\n",
      "Trained batch 1301 batch loss 1.26391315 epoch total loss 1.22651875\n",
      "Trained batch 1302 batch loss 1.28012371 epoch total loss 1.22656\n",
      "Trained batch 1303 batch loss 1.20708907 epoch total loss 1.22654498\n",
      "Trained batch 1304 batch loss 1.10435748 epoch total loss 1.22645128\n",
      "Trained batch 1305 batch loss 1.08914256 epoch total loss 1.22634602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1306 batch loss 1.1750555 epoch total loss 1.2263068\n",
      "Trained batch 1307 batch loss 1.00283384 epoch total loss 1.22613573\n",
      "Trained batch 1308 batch loss 1.15034115 epoch total loss 1.22607791\n",
      "Trained batch 1309 batch loss 1.12178659 epoch total loss 1.22599816\n",
      "Trained batch 1310 batch loss 1.18369901 epoch total loss 1.22596598\n",
      "Trained batch 1311 batch loss 1.076123 epoch total loss 1.22585166\n",
      "Trained batch 1312 batch loss 1.09863698 epoch total loss 1.22575474\n",
      "Trained batch 1313 batch loss 1.1353687 epoch total loss 1.22568583\n",
      "Trained batch 1314 batch loss 1.16842198 epoch total loss 1.22564232\n",
      "Trained batch 1315 batch loss 1.42482805 epoch total loss 1.22579384\n",
      "Trained batch 1316 batch loss 1.37383556 epoch total loss 1.22590625\n",
      "Trained batch 1317 batch loss 1.19319832 epoch total loss 1.22588146\n",
      "Trained batch 1318 batch loss 1.25013614 epoch total loss 1.22589982\n",
      "Trained batch 1319 batch loss 1.35620356 epoch total loss 1.22599864\n",
      "Trained batch 1320 batch loss 1.34366238 epoch total loss 1.22608769\n",
      "Trained batch 1321 batch loss 1.15052748 epoch total loss 1.22603047\n",
      "Trained batch 1322 batch loss 1.12380075 epoch total loss 1.22595322\n",
      "Trained batch 1323 batch loss 1.10774851 epoch total loss 1.22586381\n",
      "Trained batch 1324 batch loss 1.23797894 epoch total loss 1.22587299\n",
      "Trained batch 1325 batch loss 1.12706375 epoch total loss 1.22579849\n",
      "Trained batch 1326 batch loss 1.20175779 epoch total loss 1.22578037\n",
      "Trained batch 1327 batch loss 1.16226268 epoch total loss 1.22573245\n",
      "Trained batch 1328 batch loss 1.23802376 epoch total loss 1.22574174\n",
      "Trained batch 1329 batch loss 1.15853834 epoch total loss 1.2256912\n",
      "Trained batch 1330 batch loss 1.14824522 epoch total loss 1.22563291\n",
      "Trained batch 1331 batch loss 1.14680552 epoch total loss 1.22557378\n",
      "Trained batch 1332 batch loss 1.16279328 epoch total loss 1.22552669\n",
      "Trained batch 1333 batch loss 1.14526105 epoch total loss 1.22546649\n",
      "Trained batch 1334 batch loss 1.29208255 epoch total loss 1.22551644\n",
      "Trained batch 1335 batch loss 1.22281718 epoch total loss 1.22551429\n",
      "Trained batch 1336 batch loss 1.21105981 epoch total loss 1.22550356\n",
      "Trained batch 1337 batch loss 1.18169904 epoch total loss 1.22547066\n",
      "Trained batch 1338 batch loss 1.21970391 epoch total loss 1.22546637\n",
      "Trained batch 1339 batch loss 1.31857896 epoch total loss 1.22553599\n",
      "Trained batch 1340 batch loss 1.28092861 epoch total loss 1.22557724\n",
      "Trained batch 1341 batch loss 1.21057725 epoch total loss 1.22556615\n",
      "Trained batch 1342 batch loss 1.14426351 epoch total loss 1.22550547\n",
      "Trained batch 1343 batch loss 1.14941609 epoch total loss 1.22544885\n",
      "Trained batch 1344 batch loss 1.21703362 epoch total loss 1.22544265\n",
      "Trained batch 1345 batch loss 1.26869094 epoch total loss 1.22547472\n",
      "Trained batch 1346 batch loss 1.26225781 epoch total loss 1.22550201\n",
      "Trained batch 1347 batch loss 1.21373677 epoch total loss 1.22549331\n",
      "Trained batch 1348 batch loss 1.14573431 epoch total loss 1.22543418\n",
      "Trained batch 1349 batch loss 1.15317845 epoch total loss 1.22538066\n",
      "Trained batch 1350 batch loss 1.15346301 epoch total loss 1.22532737\n",
      "Trained batch 1351 batch loss 1.26735008 epoch total loss 1.22535837\n",
      "Trained batch 1352 batch loss 1.18740857 epoch total loss 1.22533035\n",
      "Trained batch 1353 batch loss 1.24027073 epoch total loss 1.22534132\n",
      "Trained batch 1354 batch loss 1.26788867 epoch total loss 1.22537279\n",
      "Trained batch 1355 batch loss 1.23781848 epoch total loss 1.22538197\n",
      "Trained batch 1356 batch loss 1.37267148 epoch total loss 1.22549057\n",
      "Trained batch 1357 batch loss 1.27165222 epoch total loss 1.22552454\n",
      "Trained batch 1358 batch loss 1.36241627 epoch total loss 1.2256254\n",
      "Trained batch 1359 batch loss 1.34546328 epoch total loss 1.22571361\n",
      "Trained batch 1360 batch loss 1.21084654 epoch total loss 1.22570264\n",
      "Trained batch 1361 batch loss 1.09865022 epoch total loss 1.2256093\n",
      "Trained batch 1362 batch loss 1.20010018 epoch total loss 1.22559047\n",
      "Trained batch 1363 batch loss 1.13724363 epoch total loss 1.22552562\n",
      "Trained batch 1364 batch loss 1.07050216 epoch total loss 1.22541201\n",
      "Trained batch 1365 batch loss 1.13732815 epoch total loss 1.22534752\n",
      "Trained batch 1366 batch loss 1.21104 epoch total loss 1.22533703\n",
      "Trained batch 1367 batch loss 1.22148681 epoch total loss 1.22533417\n",
      "Trained batch 1368 batch loss 1.24979651 epoch total loss 1.22535205\n",
      "Trained batch 1369 batch loss 1.21323252 epoch total loss 1.22534323\n",
      "Trained batch 1370 batch loss 1.33330727 epoch total loss 1.22542202\n",
      "Trained batch 1371 batch loss 1.18909204 epoch total loss 1.22539544\n",
      "Trained batch 1372 batch loss 1.04519582 epoch total loss 1.22526407\n",
      "Trained batch 1373 batch loss 1.0060451 epoch total loss 1.22510445\n",
      "Trained batch 1374 batch loss 1.22179985 epoch total loss 1.22510207\n",
      "Trained batch 1375 batch loss 1.20628548 epoch total loss 1.22508848\n",
      "Trained batch 1376 batch loss 1.33939505 epoch total loss 1.22517145\n",
      "Trained batch 1377 batch loss 1.34230757 epoch total loss 1.22525656\n",
      "Trained batch 1378 batch loss 1.26163387 epoch total loss 1.22528291\n",
      "Trained batch 1379 batch loss 1.32202256 epoch total loss 1.225353\n",
      "Trained batch 1380 batch loss 1.26200557 epoch total loss 1.22537959\n",
      "Trained batch 1381 batch loss 1.29079008 epoch total loss 1.22542691\n",
      "Trained batch 1382 batch loss 1.27390695 epoch total loss 1.22546196\n",
      "Trained batch 1383 batch loss 1.2433548 epoch total loss 1.22547495\n",
      "Trained batch 1384 batch loss 1.33719659 epoch total loss 1.22555566\n",
      "Trained batch 1385 batch loss 1.330019 epoch total loss 1.22563112\n",
      "Trained batch 1386 batch loss 1.25398457 epoch total loss 1.22565162\n",
      "Trained batch 1387 batch loss 1.29032874 epoch total loss 1.22569823\n",
      "Trained batch 1388 batch loss 1.19750369 epoch total loss 1.22567797\n",
      "Epoch 4 train loss 1.2256779670715332\n",
      "Validated batch 1 batch loss 1.19830143\n",
      "Validated batch 2 batch loss 1.18808949\n",
      "Validated batch 3 batch loss 1.12194848\n",
      "Validated batch 4 batch loss 1.23373544\n",
      "Validated batch 5 batch loss 1.17238522\n",
      "Validated batch 6 batch loss 1.2460916\n",
      "Validated batch 7 batch loss 1.31508124\n",
      "Validated batch 8 batch loss 1.26101899\n",
      "Validated batch 9 batch loss 1.23038721\n",
      "Validated batch 10 batch loss 1.18228853\n",
      "Validated batch 11 batch loss 1.25991142\n",
      "Validated batch 12 batch loss 1.17107284\n",
      "Validated batch 13 batch loss 1.19487476\n",
      "Validated batch 14 batch loss 1.29483175\n",
      "Validated batch 15 batch loss 1.26545894\n",
      "Validated batch 16 batch loss 1.1978178\n",
      "Validated batch 17 batch loss 1.34244466\n",
      "Validated batch 18 batch loss 1.09572387\n",
      "Validated batch 19 batch loss 1.30163789\n",
      "Validated batch 20 batch loss 0.978500068\n",
      "Validated batch 21 batch loss 1.20941436\n",
      "Validated batch 22 batch loss 1.25789857\n",
      "Validated batch 23 batch loss 1.10639954\n",
      "Validated batch 24 batch loss 1.18625236\n",
      "Validated batch 25 batch loss 1.10650885\n",
      "Validated batch 26 batch loss 1.14107966\n",
      "Validated batch 27 batch loss 1.15133524\n",
      "Validated batch 28 batch loss 1.18571234\n",
      "Validated batch 29 batch loss 1.21653128\n",
      "Validated batch 30 batch loss 1.24695\n",
      "Validated batch 31 batch loss 1.14052141\n",
      "Validated batch 32 batch loss 1.1874181\n",
      "Validated batch 33 batch loss 1.18963552\n",
      "Validated batch 34 batch loss 1.21018541\n",
      "Validated batch 35 batch loss 1.20299268\n",
      "Validated batch 36 batch loss 1.13877892\n",
      "Validated batch 37 batch loss 1.17756748\n",
      "Validated batch 38 batch loss 1.20497561\n",
      "Validated batch 39 batch loss 1.14959311\n",
      "Validated batch 40 batch loss 1.31302857\n",
      "Validated batch 41 batch loss 1.27546525\n",
      "Validated batch 42 batch loss 1.07397974\n",
      "Validated batch 43 batch loss 1.27932858\n",
      "Validated batch 44 batch loss 1.16136467\n",
      "Validated batch 45 batch loss 1.12055421\n",
      "Validated batch 46 batch loss 1.23807907\n",
      "Validated batch 47 batch loss 1.11106253\n",
      "Validated batch 48 batch loss 1.23073399\n",
      "Validated batch 49 batch loss 1.25540197\n",
      "Validated batch 50 batch loss 1.14749384\n",
      "Validated batch 51 batch loss 1.29211652\n",
      "Validated batch 52 batch loss 1.36477947\n",
      "Validated batch 53 batch loss 1.04836774\n",
      "Validated batch 54 batch loss 1.21864426\n",
      "Validated batch 55 batch loss 1.20356226\n",
      "Validated batch 56 batch loss 1.27453637\n",
      "Validated batch 57 batch loss 1.24555218\n",
      "Validated batch 58 batch loss 1.0650363\n",
      "Validated batch 59 batch loss 1.05784905\n",
      "Validated batch 60 batch loss 1.16503799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 61 batch loss 1.15918863\n",
      "Validated batch 62 batch loss 1.12506294\n",
      "Validated batch 63 batch loss 1.24536633\n",
      "Validated batch 64 batch loss 1.10666275\n",
      "Validated batch 65 batch loss 1.24013746\n",
      "Validated batch 66 batch loss 1.26226044\n",
      "Validated batch 67 batch loss 1.24644721\n",
      "Validated batch 68 batch loss 1.2027446\n",
      "Validated batch 69 batch loss 1.09182978\n",
      "Validated batch 70 batch loss 1.22879696\n",
      "Validated batch 71 batch loss 1.21843457\n",
      "Validated batch 72 batch loss 1.10510349\n",
      "Validated batch 73 batch loss 1.18603075\n",
      "Validated batch 74 batch loss 1.19243801\n",
      "Validated batch 75 batch loss 1.18281198\n",
      "Validated batch 76 batch loss 1.23122239\n",
      "Validated batch 77 batch loss 1.17471981\n",
      "Validated batch 78 batch loss 1.16226172\n",
      "Validated batch 79 batch loss 1.31116211\n",
      "Validated batch 80 batch loss 1.10277271\n",
      "Validated batch 81 batch loss 1.06875551\n",
      "Validated batch 82 batch loss 1.24565291\n",
      "Validated batch 83 batch loss 1.27851081\n",
      "Validated batch 84 batch loss 1.31916451\n",
      "Validated batch 85 batch loss 1.34592783\n",
      "Validated batch 86 batch loss 1.1777972\n",
      "Validated batch 87 batch loss 1.36143351\n",
      "Validated batch 88 batch loss 1.16317165\n",
      "Validated batch 89 batch loss 1.2573334\n",
      "Validated batch 90 batch loss 1.2660315\n",
      "Validated batch 91 batch loss 0.958718538\n",
      "Validated batch 92 batch loss 1.19094872\n",
      "Validated batch 93 batch loss 1.20623803\n",
      "Validated batch 94 batch loss 1.13898718\n",
      "Validated batch 95 batch loss 1.17708457\n",
      "Validated batch 96 batch loss 1.13265848\n",
      "Validated batch 97 batch loss 1.16366386\n",
      "Validated batch 98 batch loss 1.27697313\n",
      "Validated batch 99 batch loss 1.21380126\n",
      "Validated batch 100 batch loss 1.28631854\n",
      "Validated batch 101 batch loss 1.26955104\n",
      "Validated batch 102 batch loss 1.23487604\n",
      "Validated batch 103 batch loss 1.20640254\n",
      "Validated batch 104 batch loss 1.36787748\n",
      "Validated batch 105 batch loss 1.2519989\n",
      "Validated batch 106 batch loss 1.28866\n",
      "Validated batch 107 batch loss 1.27870369\n",
      "Validated batch 108 batch loss 1.31012571\n",
      "Validated batch 109 batch loss 1.27476311\n",
      "Validated batch 110 batch loss 1.12692356\n",
      "Validated batch 111 batch loss 1.1981796\n",
      "Validated batch 112 batch loss 1.27028441\n",
      "Validated batch 113 batch loss 1.28521419\n",
      "Validated batch 114 batch loss 1.15196824\n",
      "Validated batch 115 batch loss 1.20425761\n",
      "Validated batch 116 batch loss 1.31514549\n",
      "Validated batch 117 batch loss 1.22398806\n",
      "Validated batch 118 batch loss 1.1585803\n",
      "Validated batch 119 batch loss 1.19861865\n",
      "Validated batch 120 batch loss 1.13212264\n",
      "Validated batch 121 batch loss 1.20384753\n",
      "Validated batch 122 batch loss 1.23905993\n",
      "Validated batch 123 batch loss 1.15359151\n",
      "Validated batch 124 batch loss 1.15598226\n",
      "Validated batch 125 batch loss 1.20895553\n",
      "Validated batch 126 batch loss 1.23449337\n",
      "Validated batch 127 batch loss 1.28425705\n",
      "Validated batch 128 batch loss 1.25496316\n",
      "Validated batch 129 batch loss 1.14048851\n",
      "Validated batch 130 batch loss 1.16860235\n",
      "Validated batch 131 batch loss 1.21913099\n",
      "Validated batch 132 batch loss 1.17264605\n",
      "Validated batch 133 batch loss 1.23562157\n",
      "Validated batch 134 batch loss 1.23254\n",
      "Validated batch 135 batch loss 1.38576007\n",
      "Validated batch 136 batch loss 1.33521986\n",
      "Validated batch 137 batch loss 1.20275915\n",
      "Validated batch 138 batch loss 1.13820934\n",
      "Validated batch 139 batch loss 1.07903886\n",
      "Validated batch 140 batch loss 1.24704289\n",
      "Validated batch 141 batch loss 1.19761467\n",
      "Validated batch 142 batch loss 1.07442\n",
      "Validated batch 143 batch loss 1.10601449\n",
      "Validated batch 144 batch loss 1.28408265\n",
      "Validated batch 145 batch loss 1.09045887\n",
      "Validated batch 146 batch loss 1.06846702\n",
      "Validated batch 147 batch loss 1.13889778\n",
      "Validated batch 148 batch loss 1.21021378\n",
      "Validated batch 149 batch loss 1.04716682\n",
      "Validated batch 150 batch loss 1.2083019\n",
      "Validated batch 151 batch loss 1.09668839\n",
      "Validated batch 152 batch loss 1.19192123\n",
      "Validated batch 153 batch loss 1.28991735\n",
      "Validated batch 154 batch loss 1.30286741\n",
      "Validated batch 155 batch loss 1.11340511\n",
      "Validated batch 156 batch loss 1.32117057\n",
      "Validated batch 157 batch loss 1.01724\n",
      "Validated batch 158 batch loss 1.06304121\n",
      "Validated batch 159 batch loss 1.18250728\n",
      "Validated batch 160 batch loss 1.15746522\n",
      "Validated batch 161 batch loss 1.29690874\n",
      "Validated batch 162 batch loss 1.28675318\n",
      "Validated batch 163 batch loss 1.22063935\n",
      "Validated batch 164 batch loss 1.23555887\n",
      "Validated batch 165 batch loss 1.1853267\n",
      "Validated batch 166 batch loss 1.27259684\n",
      "Validated batch 167 batch loss 1.401088\n",
      "Validated batch 168 batch loss 1.16154921\n",
      "Validated batch 169 batch loss 1.23286235\n",
      "Validated batch 170 batch loss 1.17895818\n",
      "Validated batch 171 batch loss 1.27780366\n",
      "Validated batch 172 batch loss 1.2158767\n",
      "Validated batch 173 batch loss 1.20016646\n",
      "Validated batch 174 batch loss 1.24634778\n",
      "Validated batch 175 batch loss 1.2772578\n",
      "Validated batch 176 batch loss 1.2416048\n",
      "Validated batch 177 batch loss 1.25500643\n",
      "Validated batch 178 batch loss 1.29556811\n",
      "Validated batch 179 batch loss 1.18161023\n",
      "Validated batch 180 batch loss 1.17474198\n",
      "Validated batch 181 batch loss 1.22960365\n",
      "Validated batch 182 batch loss 1.16284668\n",
      "Validated batch 183 batch loss 1.19415903\n",
      "Validated batch 184 batch loss 1.21348262\n",
      "Validated batch 185 batch loss 1.31746876\n",
      "Epoch 4 val loss 1.2052220106124878\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-4-loss-1.2052.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.14597166 epoch total loss 1.14597166\n",
      "Trained batch 2 batch loss 1.13709068 epoch total loss 1.14153123\n",
      "Trained batch 3 batch loss 1.18142545 epoch total loss 1.15482938\n",
      "Trained batch 4 batch loss 1.14056015 epoch total loss 1.15126204\n",
      "Trained batch 5 batch loss 0.989874363 epoch total loss 1.11898446\n",
      "Trained batch 6 batch loss 0.930725932 epoch total loss 1.0876081\n",
      "Trained batch 7 batch loss 1.06189609 epoch total loss 1.0839349\n",
      "Trained batch 8 batch loss 1.11598623 epoch total loss 1.08794129\n",
      "Trained batch 9 batch loss 1.17343283 epoch total loss 1.09744024\n",
      "Trained batch 10 batch loss 1.23479342 epoch total loss 1.11117566\n",
      "Trained batch 11 batch loss 1.35969663 epoch total loss 1.13376844\n",
      "Trained batch 12 batch loss 1.10563695 epoch total loss 1.13142407\n",
      "Trained batch 13 batch loss 1.12981474 epoch total loss 1.13130033\n",
      "Trained batch 14 batch loss 1.24799597 epoch total loss 1.1396358\n",
      "Trained batch 15 batch loss 1.20858669 epoch total loss 1.14423239\n",
      "Trained batch 16 batch loss 1.12697148 epoch total loss 1.14315367\n",
      "Trained batch 17 batch loss 1.21354151 epoch total loss 1.14729404\n",
      "Trained batch 18 batch loss 1.24260068 epoch total loss 1.15258896\n",
      "Trained batch 19 batch loss 1.23915386 epoch total loss 1.15714502\n",
      "Trained batch 20 batch loss 1.13755631 epoch total loss 1.1561656\n",
      "Trained batch 21 batch loss 1.22055459 epoch total loss 1.15923178\n",
      "Trained batch 22 batch loss 1.21545172 epoch total loss 1.16178715\n",
      "Trained batch 23 batch loss 1.39506042 epoch total loss 1.17192948\n",
      "Trained batch 24 batch loss 1.26976621 epoch total loss 1.17600596\n",
      "Trained batch 25 batch loss 1.28261912 epoch total loss 1.18027055\n",
      "Trained batch 26 batch loss 1.29133487 epoch total loss 1.18454218\n",
      "Trained batch 27 batch loss 1.34543812 epoch total loss 1.19050133\n",
      "Trained batch 28 batch loss 1.184021 epoch total loss 1.19026983\n",
      "Trained batch 29 batch loss 1.09270692 epoch total loss 1.18690574\n",
      "Trained batch 30 batch loss 1.0497539 epoch total loss 1.18233407\n",
      "Trained batch 31 batch loss 1.06881261 epoch total loss 1.17867208\n",
      "Trained batch 32 batch loss 1.08542967 epoch total loss 1.17575824\n",
      "Trained batch 33 batch loss 1.05876422 epoch total loss 1.17221296\n",
      "Trained batch 34 batch loss 0.98888725 epoch total loss 1.16682112\n",
      "Trained batch 35 batch loss 0.942677498 epoch total loss 1.16041696\n",
      "Trained batch 36 batch loss 0.942549706 epoch total loss 1.15436506\n",
      "Trained batch 37 batch loss 0.859778583 epoch total loss 1.14640331\n",
      "Trained batch 38 batch loss 1.01637971 epoch total loss 1.14298165\n",
      "Trained batch 39 batch loss 1.14861822 epoch total loss 1.14312613\n",
      "Trained batch 40 batch loss 1.12985826 epoch total loss 1.14279449\n",
      "Trained batch 41 batch loss 1.12781978 epoch total loss 1.14242923\n",
      "Trained batch 42 batch loss 1.12810326 epoch total loss 1.14208817\n",
      "Trained batch 43 batch loss 1.24246538 epoch total loss 1.14442253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 44 batch loss 1.34478903 epoch total loss 1.14897633\n",
      "Trained batch 45 batch loss 1.18894875 epoch total loss 1.14986467\n",
      "Trained batch 46 batch loss 1.23008537 epoch total loss 1.15160847\n",
      "Trained batch 47 batch loss 1.17720151 epoch total loss 1.15215302\n",
      "Trained batch 48 batch loss 1.27786744 epoch total loss 1.15477204\n",
      "Trained batch 49 batch loss 1.19176626 epoch total loss 1.155527\n",
      "Trained batch 50 batch loss 1.21385574 epoch total loss 1.15669358\n",
      "Trained batch 51 batch loss 1.29267955 epoch total loss 1.15935993\n",
      "Trained batch 52 batch loss 1.25006008 epoch total loss 1.1611042\n",
      "Trained batch 53 batch loss 1.35457778 epoch total loss 1.16475463\n",
      "Trained batch 54 batch loss 1.21759748 epoch total loss 1.16573322\n",
      "Trained batch 55 batch loss 1.30766726 epoch total loss 1.16831386\n",
      "Trained batch 56 batch loss 1.30042434 epoch total loss 1.17067301\n",
      "Trained batch 57 batch loss 1.189749 epoch total loss 1.17100763\n",
      "Trained batch 58 batch loss 1.17239738 epoch total loss 1.17103159\n",
      "Trained batch 59 batch loss 1.1849041 epoch total loss 1.17126667\n",
      "Trained batch 60 batch loss 1.20158362 epoch total loss 1.171772\n",
      "Trained batch 61 batch loss 1.21912122 epoch total loss 1.17254829\n",
      "Trained batch 62 batch loss 1.26365352 epoch total loss 1.17401779\n",
      "Trained batch 63 batch loss 1.21188772 epoch total loss 1.17461896\n",
      "Trained batch 64 batch loss 1.22698534 epoch total loss 1.17543709\n",
      "Trained batch 65 batch loss 1.20902359 epoch total loss 1.17595375\n",
      "Trained batch 66 batch loss 1.17481208 epoch total loss 1.17593646\n",
      "Trained batch 67 batch loss 1.26512766 epoch total loss 1.17726767\n",
      "Trained batch 68 batch loss 1.22583485 epoch total loss 1.17798197\n",
      "Trained batch 69 batch loss 1.13026774 epoch total loss 1.17729044\n",
      "Trained batch 70 batch loss 1.13953733 epoch total loss 1.17675102\n",
      "Trained batch 71 batch loss 1.12729096 epoch total loss 1.17605436\n",
      "Trained batch 72 batch loss 1.167665 epoch total loss 1.17593789\n",
      "Trained batch 73 batch loss 1.15442371 epoch total loss 1.17564321\n",
      "Trained batch 74 batch loss 1.10788381 epoch total loss 1.17472756\n",
      "Trained batch 75 batch loss 1.13995624 epoch total loss 1.17426395\n",
      "Trained batch 76 batch loss 1.17737138 epoch total loss 1.17430472\n",
      "Trained batch 77 batch loss 1.09573913 epoch total loss 1.17328441\n",
      "Trained batch 78 batch loss 1.20947564 epoch total loss 1.17374837\n",
      "Trained batch 79 batch loss 1.10897839 epoch total loss 1.17292857\n",
      "Trained batch 80 batch loss 1.17673898 epoch total loss 1.17297626\n",
      "Trained batch 81 batch loss 1.25374842 epoch total loss 1.17397332\n",
      "Trained batch 82 batch loss 1.26824415 epoch total loss 1.17512298\n",
      "Trained batch 83 batch loss 1.23264635 epoch total loss 1.17581594\n",
      "Trained batch 84 batch loss 1.20353532 epoch total loss 1.17614603\n",
      "Trained batch 85 batch loss 1.01710951 epoch total loss 1.17427504\n",
      "Trained batch 86 batch loss 1.25116217 epoch total loss 1.17516899\n",
      "Trained batch 87 batch loss 1.16575265 epoch total loss 1.17506087\n",
      "Trained batch 88 batch loss 1.17851818 epoch total loss 1.17510021\n",
      "Trained batch 89 batch loss 1.10034549 epoch total loss 1.17426014\n",
      "Trained batch 90 batch loss 1.04924011 epoch total loss 1.17287111\n",
      "Trained batch 91 batch loss 1.06206679 epoch total loss 1.17165339\n",
      "Trained batch 92 batch loss 1.08261156 epoch total loss 1.17068553\n",
      "Trained batch 93 batch loss 1.22767258 epoch total loss 1.17129827\n",
      "Trained batch 94 batch loss 1.24939656 epoch total loss 1.17212915\n",
      "Trained batch 95 batch loss 1.27277184 epoch total loss 1.17318857\n",
      "Trained batch 96 batch loss 1.28278947 epoch total loss 1.17433023\n",
      "Trained batch 97 batch loss 1.23287201 epoch total loss 1.17493379\n",
      "Trained batch 98 batch loss 1.11071825 epoch total loss 1.1742785\n",
      "Trained batch 99 batch loss 1.14819407 epoch total loss 1.17401505\n",
      "Trained batch 100 batch loss 1.24505973 epoch total loss 1.17472541\n",
      "Trained batch 101 batch loss 1.20336366 epoch total loss 1.17500889\n",
      "Trained batch 102 batch loss 0.957636952 epoch total loss 1.17287779\n",
      "Trained batch 103 batch loss 0.945987403 epoch total loss 1.17067492\n",
      "Trained batch 104 batch loss 1.07369876 epoch total loss 1.16974247\n",
      "Trained batch 105 batch loss 1.15556633 epoch total loss 1.16960752\n",
      "Trained batch 106 batch loss 1.30370831 epoch total loss 1.17087257\n",
      "Trained batch 107 batch loss 1.37002146 epoch total loss 1.17273378\n",
      "Trained batch 108 batch loss 1.28644085 epoch total loss 1.17378664\n",
      "Trained batch 109 batch loss 1.29075015 epoch total loss 1.17485976\n",
      "Trained batch 110 batch loss 1.21396637 epoch total loss 1.17521524\n",
      "Trained batch 111 batch loss 1.15873206 epoch total loss 1.17506683\n",
      "Trained batch 112 batch loss 1.1326704 epoch total loss 1.17468834\n",
      "Trained batch 113 batch loss 1.14556241 epoch total loss 1.17443061\n",
      "Trained batch 114 batch loss 1.12470865 epoch total loss 1.17399454\n",
      "Trained batch 115 batch loss 1.14619613 epoch total loss 1.17375278\n",
      "Trained batch 116 batch loss 1.21765459 epoch total loss 1.17413116\n",
      "Trained batch 117 batch loss 1.11118972 epoch total loss 1.17359328\n",
      "Trained batch 118 batch loss 0.924348 epoch total loss 1.17148101\n",
      "Trained batch 119 batch loss 0.909756303 epoch total loss 1.1692816\n",
      "Trained batch 120 batch loss 1.20570588 epoch total loss 1.16958511\n",
      "Trained batch 121 batch loss 1.21066177 epoch total loss 1.16992462\n",
      "Trained batch 122 batch loss 1.28093815 epoch total loss 1.17083466\n",
      "Trained batch 123 batch loss 1.18919671 epoch total loss 1.17098391\n",
      "Trained batch 124 batch loss 1.12750816 epoch total loss 1.1706332\n",
      "Trained batch 125 batch loss 1.08011949 epoch total loss 1.16990924\n",
      "Trained batch 126 batch loss 1.06061411 epoch total loss 1.16904175\n",
      "Trained batch 127 batch loss 1.11672795 epoch total loss 1.16862977\n",
      "Trained batch 128 batch loss 1.15929067 epoch total loss 1.16855681\n",
      "Trained batch 129 batch loss 1.19670951 epoch total loss 1.16877508\n",
      "Trained batch 130 batch loss 1.26196611 epoch total loss 1.16949189\n",
      "Trained batch 131 batch loss 1.14101195 epoch total loss 1.16927445\n",
      "Trained batch 132 batch loss 1.25064969 epoch total loss 1.169891\n",
      "Trained batch 133 batch loss 1.15991235 epoch total loss 1.16981602\n",
      "Trained batch 134 batch loss 1.26439643 epoch total loss 1.17052174\n",
      "Trained batch 135 batch loss 1.314538 epoch total loss 1.17158854\n",
      "Trained batch 136 batch loss 1.24679136 epoch total loss 1.17214155\n",
      "Trained batch 137 batch loss 1.11714649 epoch total loss 1.17174017\n",
      "Trained batch 138 batch loss 1.23709416 epoch total loss 1.17221367\n",
      "Trained batch 139 batch loss 1.25702953 epoch total loss 1.17282391\n",
      "Trained batch 140 batch loss 1.18969917 epoch total loss 1.17294443\n",
      "Trained batch 141 batch loss 1.24479866 epoch total loss 1.17345405\n",
      "Trained batch 142 batch loss 1.22725189 epoch total loss 1.17383289\n",
      "Trained batch 143 batch loss 1.20804358 epoch total loss 1.17407203\n",
      "Trained batch 144 batch loss 1.19554234 epoch total loss 1.17422116\n",
      "Trained batch 145 batch loss 1.13925099 epoch total loss 1.17398\n",
      "Trained batch 146 batch loss 1.06005156 epoch total loss 1.17319965\n",
      "Trained batch 147 batch loss 1.04792678 epoch total loss 1.17234755\n",
      "Trained batch 148 batch loss 0.998088717 epoch total loss 1.17117012\n",
      "Trained batch 149 batch loss 1.1202209 epoch total loss 1.17082822\n",
      "Trained batch 150 batch loss 1.07799971 epoch total loss 1.17020941\n",
      "Trained batch 151 batch loss 1.02161229 epoch total loss 1.16922522\n",
      "Trained batch 152 batch loss 1.03057516 epoch total loss 1.16831303\n",
      "Trained batch 153 batch loss 1.07202673 epoch total loss 1.16768372\n",
      "Trained batch 154 batch loss 1.08411717 epoch total loss 1.16714108\n",
      "Trained batch 155 batch loss 1.07979608 epoch total loss 1.1665777\n",
      "Trained batch 156 batch loss 1.16183496 epoch total loss 1.16654718\n",
      "Trained batch 157 batch loss 1.23457026 epoch total loss 1.1669805\n",
      "Trained batch 158 batch loss 1.23396766 epoch total loss 1.16740441\n",
      "Trained batch 159 batch loss 1.22131157 epoch total loss 1.16774356\n",
      "Trained batch 160 batch loss 1.23328173 epoch total loss 1.16815305\n",
      "Trained batch 161 batch loss 1.22637892 epoch total loss 1.16851473\n",
      "Trained batch 162 batch loss 1.3371743 epoch total loss 1.1695559\n",
      "Trained batch 163 batch loss 1.27480769 epoch total loss 1.17020154\n",
      "Trained batch 164 batch loss 1.27211785 epoch total loss 1.1708231\n",
      "Trained batch 165 batch loss 1.1065706 epoch total loss 1.17043364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 166 batch loss 1.07963252 epoch total loss 1.16988671\n",
      "Trained batch 167 batch loss 1.21265435 epoch total loss 1.17014277\n",
      "Trained batch 168 batch loss 1.20979631 epoch total loss 1.1703788\n",
      "Trained batch 169 batch loss 1.27220345 epoch total loss 1.17098129\n",
      "Trained batch 170 batch loss 1.27035224 epoch total loss 1.17156589\n",
      "Trained batch 171 batch loss 1.2291379 epoch total loss 1.17190254\n",
      "Trained batch 172 batch loss 1.1511873 epoch total loss 1.17178214\n",
      "Trained batch 173 batch loss 1.22463655 epoch total loss 1.17208767\n",
      "Trained batch 174 batch loss 1.25114989 epoch total loss 1.17254198\n",
      "Trained batch 175 batch loss 1.27999711 epoch total loss 1.17315602\n",
      "Trained batch 176 batch loss 1.15441549 epoch total loss 1.17304957\n",
      "Trained batch 177 batch loss 1.04107678 epoch total loss 1.17230392\n",
      "Trained batch 178 batch loss 1.16899371 epoch total loss 1.17228532\n",
      "Trained batch 179 batch loss 1.10103416 epoch total loss 1.17188728\n",
      "Trained batch 180 batch loss 1.15910041 epoch total loss 1.17181623\n",
      "Trained batch 181 batch loss 1.09917068 epoch total loss 1.17141485\n",
      "Trained batch 182 batch loss 1.18465364 epoch total loss 1.17148757\n",
      "Trained batch 183 batch loss 1.1475234 epoch total loss 1.17135656\n",
      "Trained batch 184 batch loss 1.14993441 epoch total loss 1.17124021\n",
      "Trained batch 185 batch loss 1.22742832 epoch total loss 1.17154396\n",
      "Trained batch 186 batch loss 1.30482292 epoch total loss 1.17226052\n",
      "Trained batch 187 batch loss 1.37476695 epoch total loss 1.17334342\n",
      "Trained batch 188 batch loss 1.42249417 epoch total loss 1.17466879\n",
      "Trained batch 189 batch loss 1.42959583 epoch total loss 1.17601752\n",
      "Trained batch 190 batch loss 1.14481807 epoch total loss 1.17585337\n",
      "Trained batch 191 batch loss 1.18377364 epoch total loss 1.17589486\n",
      "Trained batch 192 batch loss 1.21348763 epoch total loss 1.1760906\n",
      "Trained batch 193 batch loss 1.37701023 epoch total loss 1.17713165\n",
      "Trained batch 194 batch loss 1.36807466 epoch total loss 1.17811596\n",
      "Trained batch 195 batch loss 1.22719955 epoch total loss 1.17836761\n",
      "Trained batch 196 batch loss 0.989682078 epoch total loss 1.177405\n",
      "Trained batch 197 batch loss 0.946919382 epoch total loss 1.17623496\n",
      "Trained batch 198 batch loss 1.04725254 epoch total loss 1.1755836\n",
      "Trained batch 199 batch loss 1.27051413 epoch total loss 1.17606056\n",
      "Trained batch 200 batch loss 1.39480937 epoch total loss 1.1771543\n",
      "Trained batch 201 batch loss 1.3868041 epoch total loss 1.17819738\n",
      "Trained batch 202 batch loss 1.27149689 epoch total loss 1.17865932\n",
      "Trained batch 203 batch loss 1.14624369 epoch total loss 1.17849958\n",
      "Trained batch 204 batch loss 1.27893341 epoch total loss 1.17899191\n",
      "Trained batch 205 batch loss 1.23725224 epoch total loss 1.17927611\n",
      "Trained batch 206 batch loss 1.17535341 epoch total loss 1.17925704\n",
      "Trained batch 207 batch loss 1.303105 epoch total loss 1.17985535\n",
      "Trained batch 208 batch loss 1.24185359 epoch total loss 1.18015337\n",
      "Trained batch 209 batch loss 1.25992823 epoch total loss 1.18053508\n",
      "Trained batch 210 batch loss 1.19350648 epoch total loss 1.18059695\n",
      "Trained batch 211 batch loss 1.18551111 epoch total loss 1.18062019\n",
      "Trained batch 212 batch loss 1.07701588 epoch total loss 1.18013155\n",
      "Trained batch 213 batch loss 1.27571988 epoch total loss 1.18058038\n",
      "Trained batch 214 batch loss 1.23157144 epoch total loss 1.18081856\n",
      "Trained batch 215 batch loss 1.07941556 epoch total loss 1.18034697\n",
      "Trained batch 216 batch loss 1.07898819 epoch total loss 1.17987776\n",
      "Trained batch 217 batch loss 1.00288022 epoch total loss 1.17906213\n",
      "Trained batch 218 batch loss 0.952874541 epoch total loss 1.17802453\n",
      "Trained batch 219 batch loss 1.00160193 epoch total loss 1.17721891\n",
      "Trained batch 220 batch loss 1.04064584 epoch total loss 1.17659819\n",
      "Trained batch 221 batch loss 1.1376431 epoch total loss 1.17642188\n",
      "Trained batch 222 batch loss 1.16121578 epoch total loss 1.17635334\n",
      "Trained batch 223 batch loss 1.07000113 epoch total loss 1.1758765\n",
      "Trained batch 224 batch loss 1.08538055 epoch total loss 1.1754725\n",
      "Trained batch 225 batch loss 1.14434099 epoch total loss 1.17533422\n",
      "Trained batch 226 batch loss 1.19667196 epoch total loss 1.17542863\n",
      "Trained batch 227 batch loss 1.23601007 epoch total loss 1.17569566\n",
      "Trained batch 228 batch loss 1.19260228 epoch total loss 1.17576969\n",
      "Trained batch 229 batch loss 0.955056965 epoch total loss 1.17480588\n",
      "Trained batch 230 batch loss 1.10699368 epoch total loss 1.17451108\n",
      "Trained batch 231 batch loss 1.15705931 epoch total loss 1.17443562\n",
      "Trained batch 232 batch loss 1.24472713 epoch total loss 1.17473853\n",
      "Trained batch 233 batch loss 1.28244352 epoch total loss 1.17520082\n",
      "Trained batch 234 batch loss 1.33270681 epoch total loss 1.17587388\n",
      "Trained batch 235 batch loss 1.22902 epoch total loss 1.1761\n",
      "Trained batch 236 batch loss 1.24091041 epoch total loss 1.17637467\n",
      "Trained batch 237 batch loss 1.04602039 epoch total loss 1.17582464\n",
      "Trained batch 238 batch loss 1.18916607 epoch total loss 1.17588079\n",
      "Trained batch 239 batch loss 1.20090914 epoch total loss 1.17598546\n",
      "Trained batch 240 batch loss 1.19789732 epoch total loss 1.17607677\n",
      "Trained batch 241 batch loss 1.09254932 epoch total loss 1.17573023\n",
      "Trained batch 242 batch loss 1.22312891 epoch total loss 1.17592597\n",
      "Trained batch 243 batch loss 1.20878053 epoch total loss 1.17606115\n",
      "Trained batch 244 batch loss 1.18008101 epoch total loss 1.17607772\n",
      "Trained batch 245 batch loss 1.09762526 epoch total loss 1.17575741\n",
      "Trained batch 246 batch loss 1.1835053 epoch total loss 1.175789\n",
      "Trained batch 247 batch loss 1.20747387 epoch total loss 1.17591727\n",
      "Trained batch 248 batch loss 1.28083467 epoch total loss 1.17634034\n",
      "Trained batch 249 batch loss 1.23336697 epoch total loss 1.17656934\n",
      "Trained batch 250 batch loss 1.06937468 epoch total loss 1.17614055\n",
      "Trained batch 251 batch loss 1.11159039 epoch total loss 1.17588341\n",
      "Trained batch 252 batch loss 1.20113885 epoch total loss 1.17598367\n",
      "Trained batch 253 batch loss 1.23094392 epoch total loss 1.17620087\n",
      "Trained batch 254 batch loss 1.16590703 epoch total loss 1.17616034\n",
      "Trained batch 255 batch loss 1.26516581 epoch total loss 1.17650938\n",
      "Trained batch 256 batch loss 1.20619547 epoch total loss 1.17662537\n",
      "Trained batch 257 batch loss 1.06779027 epoch total loss 1.17620182\n",
      "Trained batch 258 batch loss 1.03689766 epoch total loss 1.17566192\n",
      "Trained batch 259 batch loss 1.21590233 epoch total loss 1.17581725\n",
      "Trained batch 260 batch loss 1.11186373 epoch total loss 1.17557144\n",
      "Trained batch 261 batch loss 1.31830585 epoch total loss 1.17611825\n",
      "Trained batch 262 batch loss 1.28411984 epoch total loss 1.17653048\n",
      "Trained batch 263 batch loss 1.30771768 epoch total loss 1.17702925\n",
      "Trained batch 264 batch loss 1.22938073 epoch total loss 1.1772275\n",
      "Trained batch 265 batch loss 1.19527292 epoch total loss 1.17729557\n",
      "Trained batch 266 batch loss 1.25969315 epoch total loss 1.17760539\n",
      "Trained batch 267 batch loss 1.15484786 epoch total loss 1.17752016\n",
      "Trained batch 268 batch loss 1.13447309 epoch total loss 1.17735946\n",
      "Trained batch 269 batch loss 1.14569247 epoch total loss 1.1772418\n",
      "Trained batch 270 batch loss 1.31357372 epoch total loss 1.17774665\n",
      "Trained batch 271 batch loss 1.24907494 epoch total loss 1.17801\n",
      "Trained batch 272 batch loss 1.18208694 epoch total loss 1.17802501\n",
      "Trained batch 273 batch loss 1.24635673 epoch total loss 1.17827535\n",
      "Trained batch 274 batch loss 1.07506585 epoch total loss 1.17789865\n",
      "Trained batch 275 batch loss 1.05391216 epoch total loss 1.1774478\n",
      "Trained batch 276 batch loss 1.14878249 epoch total loss 1.17734396\n",
      "Trained batch 277 batch loss 1.17104411 epoch total loss 1.1773212\n",
      "Trained batch 278 batch loss 1.09436965 epoch total loss 1.17702281\n",
      "Trained batch 279 batch loss 1.12152088 epoch total loss 1.17682385\n",
      "Trained batch 280 batch loss 1.09004664 epoch total loss 1.17651403\n",
      "Trained batch 281 batch loss 1.12108624 epoch total loss 1.17631674\n",
      "Trained batch 282 batch loss 1.23204625 epoch total loss 1.17651439\n",
      "Trained batch 283 batch loss 1.19115937 epoch total loss 1.17656624\n",
      "Trained batch 284 batch loss 1.08557892 epoch total loss 1.17624581\n",
      "Trained batch 285 batch loss 0.964430094 epoch total loss 1.17550254\n",
      "Trained batch 286 batch loss 1.21799147 epoch total loss 1.17565107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 287 batch loss 1.20952797 epoch total loss 1.17576909\n",
      "Trained batch 288 batch loss 1.25956869 epoch total loss 1.1760602\n",
      "Trained batch 289 batch loss 1.14909256 epoch total loss 1.17596674\n",
      "Trained batch 290 batch loss 1.1761694 epoch total loss 1.17596745\n",
      "Trained batch 291 batch loss 1.17676735 epoch total loss 1.1759702\n",
      "Trained batch 292 batch loss 1.25365627 epoch total loss 1.17623627\n",
      "Trained batch 293 batch loss 1.20657516 epoch total loss 1.17633986\n",
      "Trained batch 294 batch loss 1.16038966 epoch total loss 1.17628562\n",
      "Trained batch 295 batch loss 1.11415458 epoch total loss 1.1760751\n",
      "Trained batch 296 batch loss 1.12555194 epoch total loss 1.17590439\n",
      "Trained batch 297 batch loss 1.03997517 epoch total loss 1.17544663\n",
      "Trained batch 298 batch loss 1.16898656 epoch total loss 1.17542493\n",
      "Trained batch 299 batch loss 1.15628409 epoch total loss 1.17536092\n",
      "Trained batch 300 batch loss 1.07651031 epoch total loss 1.17503142\n",
      "Trained batch 301 batch loss 1.1549921 epoch total loss 1.1749649\n",
      "Trained batch 302 batch loss 1.01942682 epoch total loss 1.17444992\n",
      "Trained batch 303 batch loss 1.09794974 epoch total loss 1.17419744\n",
      "Trained batch 304 batch loss 1.0518229 epoch total loss 1.17379487\n",
      "Trained batch 305 batch loss 1.25104129 epoch total loss 1.17404819\n",
      "Trained batch 306 batch loss 1.22415054 epoch total loss 1.17421186\n",
      "Trained batch 307 batch loss 1.18728518 epoch total loss 1.17425442\n",
      "Trained batch 308 batch loss 1.4002502 epoch total loss 1.17498815\n",
      "Trained batch 309 batch loss 1.38970637 epoch total loss 1.17568302\n",
      "Trained batch 310 batch loss 1.51304197 epoch total loss 1.17677128\n",
      "Trained batch 311 batch loss 1.38446593 epoch total loss 1.17743909\n",
      "Trained batch 312 batch loss 1.1434238 epoch total loss 1.17733014\n",
      "Trained batch 313 batch loss 1.1771394 epoch total loss 1.17732954\n",
      "Trained batch 314 batch loss 1.20150805 epoch total loss 1.17740655\n",
      "Trained batch 315 batch loss 1.11688077 epoch total loss 1.17721438\n",
      "Trained batch 316 batch loss 1.06206369 epoch total loss 1.17685008\n",
      "Trained batch 317 batch loss 1.05872464 epoch total loss 1.17647743\n",
      "Trained batch 318 batch loss 1.12581968 epoch total loss 1.17631805\n",
      "Trained batch 319 batch loss 1.08191109 epoch total loss 1.17602217\n",
      "Trained batch 320 batch loss 0.97772 epoch total loss 1.1754024\n",
      "Trained batch 321 batch loss 1.02577233 epoch total loss 1.17493641\n",
      "Trained batch 322 batch loss 1.22932184 epoch total loss 1.17510521\n",
      "Trained batch 323 batch loss 1.19155633 epoch total loss 1.17515612\n",
      "Trained batch 324 batch loss 1.07178938 epoch total loss 1.17483711\n",
      "Trained batch 325 batch loss 1.10240769 epoch total loss 1.17461431\n",
      "Trained batch 326 batch loss 1.1481365 epoch total loss 1.17453301\n",
      "Trained batch 327 batch loss 1.06328607 epoch total loss 1.17419279\n",
      "Trained batch 328 batch loss 1.27540588 epoch total loss 1.17450142\n",
      "Trained batch 329 batch loss 1.21358359 epoch total loss 1.17462015\n",
      "Trained batch 330 batch loss 1.49766541 epoch total loss 1.1755991\n",
      "Trained batch 331 batch loss 1.25778091 epoch total loss 1.17584741\n",
      "Trained batch 332 batch loss 1.38332057 epoch total loss 1.17647231\n",
      "Trained batch 333 batch loss 1.28666389 epoch total loss 1.17680323\n",
      "Trained batch 334 batch loss 1.19971216 epoch total loss 1.17687178\n",
      "Trained batch 335 batch loss 1.17045093 epoch total loss 1.17685258\n",
      "Trained batch 336 batch loss 1.29811692 epoch total loss 1.17721343\n",
      "Trained batch 337 batch loss 1.26783907 epoch total loss 1.17748249\n",
      "Trained batch 338 batch loss 1.28021383 epoch total loss 1.17778635\n",
      "Trained batch 339 batch loss 1.33462656 epoch total loss 1.178249\n",
      "Trained batch 340 batch loss 1.36705112 epoch total loss 1.1788044\n",
      "Trained batch 341 batch loss 1.41590452 epoch total loss 1.17949963\n",
      "Trained batch 342 batch loss 1.43394148 epoch total loss 1.18024361\n",
      "Trained batch 343 batch loss 1.3369168 epoch total loss 1.18070042\n",
      "Trained batch 344 batch loss 1.31527114 epoch total loss 1.18109155\n",
      "Trained batch 345 batch loss 1.33054459 epoch total loss 1.18152475\n",
      "Trained batch 346 batch loss 1.27818668 epoch total loss 1.18180418\n",
      "Trained batch 347 batch loss 1.37340474 epoch total loss 1.18235636\n",
      "Trained batch 348 batch loss 1.33483076 epoch total loss 1.18279445\n",
      "Trained batch 349 batch loss 1.26659679 epoch total loss 1.18303466\n",
      "Trained batch 350 batch loss 1.18968964 epoch total loss 1.18305373\n",
      "Trained batch 351 batch loss 1.26804638 epoch total loss 1.18329585\n",
      "Trained batch 352 batch loss 1.33529949 epoch total loss 1.18372762\n",
      "Trained batch 353 batch loss 1.1828804 epoch total loss 1.18372524\n",
      "Trained batch 354 batch loss 1.13799667 epoch total loss 1.18359613\n",
      "Trained batch 355 batch loss 1.22693288 epoch total loss 1.18371809\n",
      "Trained batch 356 batch loss 1.2455281 epoch total loss 1.18389177\n",
      "Trained batch 357 batch loss 1.13827157 epoch total loss 1.18376398\n",
      "Trained batch 358 batch loss 1.15651882 epoch total loss 1.18368781\n",
      "Trained batch 359 batch loss 1.1742171 epoch total loss 1.18366146\n",
      "Trained batch 360 batch loss 1.23814154 epoch total loss 1.18381286\n",
      "Trained batch 361 batch loss 1.41654646 epoch total loss 1.18445742\n",
      "Trained batch 362 batch loss 1.35279512 epoch total loss 1.18492246\n",
      "Trained batch 363 batch loss 1.29762292 epoch total loss 1.185233\n",
      "Trained batch 364 batch loss 1.26639068 epoch total loss 1.18545592\n",
      "Trained batch 365 batch loss 1.1734736 epoch total loss 1.18542302\n",
      "Trained batch 366 batch loss 0.988106728 epoch total loss 1.18488395\n",
      "Trained batch 367 batch loss 0.980495393 epoch total loss 1.18432701\n",
      "Trained batch 368 batch loss 0.960971713 epoch total loss 1.18372\n",
      "Trained batch 369 batch loss 1.1056447 epoch total loss 1.18350852\n",
      "Trained batch 370 batch loss 1.16824043 epoch total loss 1.18346727\n",
      "Trained batch 371 batch loss 1.09654427 epoch total loss 1.18323302\n",
      "Trained batch 372 batch loss 1.00079453 epoch total loss 1.1827426\n",
      "Trained batch 373 batch loss 0.92617619 epoch total loss 1.18205476\n",
      "Trained batch 374 batch loss 1.10382855 epoch total loss 1.18184555\n",
      "Trained batch 375 batch loss 1.25148177 epoch total loss 1.18203127\n",
      "Trained batch 376 batch loss 1.22311354 epoch total loss 1.18214047\n",
      "Trained batch 377 batch loss 1.27529013 epoch total loss 1.18238759\n",
      "Trained batch 378 batch loss 1.18595743 epoch total loss 1.18239701\n",
      "Trained batch 379 batch loss 1.06497264 epoch total loss 1.18208718\n",
      "Trained batch 380 batch loss 1.14323461 epoch total loss 1.18198502\n",
      "Trained batch 381 batch loss 1.23597586 epoch total loss 1.18212664\n",
      "Trained batch 382 batch loss 1.21268797 epoch total loss 1.18220663\n",
      "Trained batch 383 batch loss 1.19316912 epoch total loss 1.18223524\n",
      "Trained batch 384 batch loss 1.23026276 epoch total loss 1.18236029\n",
      "Trained batch 385 batch loss 1.2692554 epoch total loss 1.18258607\n",
      "Trained batch 386 batch loss 1.3240478 epoch total loss 1.18295252\n",
      "Trained batch 387 batch loss 1.17776752 epoch total loss 1.18293905\n",
      "Trained batch 388 batch loss 1.18101299 epoch total loss 1.18293405\n",
      "Trained batch 389 batch loss 1.18046749 epoch total loss 1.18292773\n",
      "Trained batch 390 batch loss 1.06036758 epoch total loss 1.18261349\n",
      "Trained batch 391 batch loss 1.24092734 epoch total loss 1.18276262\n",
      "Trained batch 392 batch loss 1.14202583 epoch total loss 1.18265879\n",
      "Trained batch 393 batch loss 1.18228555 epoch total loss 1.18265784\n",
      "Trained batch 394 batch loss 1.29293323 epoch total loss 1.18293774\n",
      "Trained batch 395 batch loss 1.2044909 epoch total loss 1.18299234\n",
      "Trained batch 396 batch loss 1.17878926 epoch total loss 1.18298173\n",
      "Trained batch 397 batch loss 1.17594182 epoch total loss 1.18296397\n",
      "Trained batch 398 batch loss 1.19550395 epoch total loss 1.18299544\n",
      "Trained batch 399 batch loss 1.10420501 epoch total loss 1.18279803\n",
      "Trained batch 400 batch loss 1.04643929 epoch total loss 1.18245709\n",
      "Trained batch 401 batch loss 1.23222244 epoch total loss 1.18258119\n",
      "Trained batch 402 batch loss 1.23652864 epoch total loss 1.18271542\n",
      "Trained batch 403 batch loss 1.22275281 epoch total loss 1.18281472\n",
      "Trained batch 404 batch loss 1.20203614 epoch total loss 1.18286228\n",
      "Trained batch 405 batch loss 1.18967366 epoch total loss 1.18287909\n",
      "Trained batch 406 batch loss 1.13001513 epoch total loss 1.18274891\n",
      "Trained batch 407 batch loss 1.18820751 epoch total loss 1.18276227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 408 batch loss 1.06731808 epoch total loss 1.18247938\n",
      "Trained batch 409 batch loss 1.19908452 epoch total loss 1.18251991\n",
      "Trained batch 410 batch loss 1.21542478 epoch total loss 1.18260026\n",
      "Trained batch 411 batch loss 1.08293343 epoch total loss 1.18235779\n",
      "Trained batch 412 batch loss 1.15113783 epoch total loss 1.18228197\n",
      "Trained batch 413 batch loss 1.30521226 epoch total loss 1.18257952\n",
      "Trained batch 414 batch loss 1.15036869 epoch total loss 1.18250179\n",
      "Trained batch 415 batch loss 1.2646805 epoch total loss 1.1826998\n",
      "Trained batch 416 batch loss 1.14480448 epoch total loss 1.18260872\n",
      "Trained batch 417 batch loss 1.18345809 epoch total loss 1.18261075\n",
      "Trained batch 418 batch loss 1.11693573 epoch total loss 1.18245363\n",
      "Trained batch 419 batch loss 1.17709172 epoch total loss 1.18244088\n",
      "Trained batch 420 batch loss 1.15557122 epoch total loss 1.18237686\n",
      "Trained batch 421 batch loss 1.3084656 epoch total loss 1.18267643\n",
      "Trained batch 422 batch loss 1.24473429 epoch total loss 1.18282342\n",
      "Trained batch 423 batch loss 1.34680521 epoch total loss 1.18321109\n",
      "Trained batch 424 batch loss 1.33480442 epoch total loss 1.1835686\n",
      "Trained batch 425 batch loss 1.273229 epoch total loss 1.1837796\n",
      "Trained batch 426 batch loss 1.26904547 epoch total loss 1.18397975\n",
      "Trained batch 427 batch loss 1.21322656 epoch total loss 1.18404818\n",
      "Trained batch 428 batch loss 1.12732983 epoch total loss 1.18391573\n",
      "Trained batch 429 batch loss 1.31564665 epoch total loss 1.1842227\n",
      "Trained batch 430 batch loss 1.23750472 epoch total loss 1.18434668\n",
      "Trained batch 431 batch loss 1.14743757 epoch total loss 1.18426108\n",
      "Trained batch 432 batch loss 1.07728791 epoch total loss 1.18401349\n",
      "Trained batch 433 batch loss 0.996470034 epoch total loss 1.1835804\n",
      "Trained batch 434 batch loss 1.20154071 epoch total loss 1.18362176\n",
      "Trained batch 435 batch loss 1.20122576 epoch total loss 1.18366218\n",
      "Trained batch 436 batch loss 1.22870243 epoch total loss 1.18376553\n",
      "Trained batch 437 batch loss 1.25488782 epoch total loss 1.18392825\n",
      "Trained batch 438 batch loss 1.23442841 epoch total loss 1.18404353\n",
      "Trained batch 439 batch loss 1.27240896 epoch total loss 1.18424487\n",
      "Trained batch 440 batch loss 1.24414229 epoch total loss 1.18438101\n",
      "Trained batch 441 batch loss 1.11061752 epoch total loss 1.18421364\n",
      "Trained batch 442 batch loss 1.15034103 epoch total loss 1.18413699\n",
      "Trained batch 443 batch loss 1.21677184 epoch total loss 1.18421066\n",
      "Trained batch 444 batch loss 1.31610942 epoch total loss 1.18450773\n",
      "Trained batch 445 batch loss 1.28597558 epoch total loss 1.18473577\n",
      "Trained batch 446 batch loss 1.21285629 epoch total loss 1.18479872\n",
      "Trained batch 447 batch loss 1.12644958 epoch total loss 1.18466818\n",
      "Trained batch 448 batch loss 1.2385211 epoch total loss 1.18478847\n",
      "Trained batch 449 batch loss 1.21483088 epoch total loss 1.18485534\n",
      "Trained batch 450 batch loss 1.26425195 epoch total loss 1.18503189\n",
      "Trained batch 451 batch loss 1.33194733 epoch total loss 1.18535769\n",
      "Trained batch 452 batch loss 1.34819424 epoch total loss 1.18571794\n",
      "Trained batch 453 batch loss 1.22342873 epoch total loss 1.18580127\n",
      "Trained batch 454 batch loss 1.08886671 epoch total loss 1.18558776\n",
      "Trained batch 455 batch loss 1.01281464 epoch total loss 1.18520808\n",
      "Trained batch 456 batch loss 1.11768591 epoch total loss 1.18505991\n",
      "Trained batch 457 batch loss 1.06916928 epoch total loss 1.18480635\n",
      "Trained batch 458 batch loss 1.14881372 epoch total loss 1.18472767\n",
      "Trained batch 459 batch loss 1.17982948 epoch total loss 1.18471694\n",
      "Trained batch 460 batch loss 1.26177776 epoch total loss 1.18488455\n",
      "Trained batch 461 batch loss 1.17166424 epoch total loss 1.18485594\n",
      "Trained batch 462 batch loss 1.2775104 epoch total loss 1.18505645\n",
      "Trained batch 463 batch loss 1.36264789 epoch total loss 1.18544006\n",
      "Trained batch 464 batch loss 1.39856839 epoch total loss 1.18589938\n",
      "Trained batch 465 batch loss 1.31247568 epoch total loss 1.18617165\n",
      "Trained batch 466 batch loss 1.22896671 epoch total loss 1.18626344\n",
      "Trained batch 467 batch loss 1.25360417 epoch total loss 1.18640769\n",
      "Trained batch 468 batch loss 1.16641533 epoch total loss 1.18636501\n",
      "Trained batch 469 batch loss 1.32372868 epoch total loss 1.18665791\n",
      "Trained batch 470 batch loss 1.25549459 epoch total loss 1.18680429\n",
      "Trained batch 471 batch loss 1.17697072 epoch total loss 1.18678343\n",
      "Trained batch 472 batch loss 1.10398042 epoch total loss 1.18660796\n",
      "Trained batch 473 batch loss 1.14086521 epoch total loss 1.18651128\n",
      "Trained batch 474 batch loss 1.10311282 epoch total loss 1.18633533\n",
      "Trained batch 475 batch loss 1.06299281 epoch total loss 1.18607569\n",
      "Trained batch 476 batch loss 1.18990529 epoch total loss 1.18608367\n",
      "Trained batch 477 batch loss 1.19303918 epoch total loss 1.18609822\n",
      "Trained batch 478 batch loss 1.18678832 epoch total loss 1.18609965\n",
      "Trained batch 479 batch loss 1.24607217 epoch total loss 1.18622494\n",
      "Trained batch 480 batch loss 1.1710875 epoch total loss 1.18619335\n",
      "Trained batch 481 batch loss 1.21179128 epoch total loss 1.18624651\n",
      "Trained batch 482 batch loss 1.24753571 epoch total loss 1.18637371\n",
      "Trained batch 483 batch loss 1.24860859 epoch total loss 1.18650258\n",
      "Trained batch 484 batch loss 1.21515357 epoch total loss 1.18656182\n",
      "Trained batch 485 batch loss 1.15379822 epoch total loss 1.18649423\n",
      "Trained batch 486 batch loss 1.06202567 epoch total loss 1.18623805\n",
      "Trained batch 487 batch loss 1.09823537 epoch total loss 1.18605733\n",
      "Trained batch 488 batch loss 1.30747747 epoch total loss 1.18630624\n",
      "Trained batch 489 batch loss 1.34104538 epoch total loss 1.18662262\n",
      "Trained batch 490 batch loss 1.21010375 epoch total loss 1.18667054\n",
      "Trained batch 491 batch loss 1.12664092 epoch total loss 1.18654835\n",
      "Trained batch 492 batch loss 1.25234306 epoch total loss 1.18668199\n",
      "Trained batch 493 batch loss 1.21673989 epoch total loss 1.1867429\n",
      "Trained batch 494 batch loss 1.13435936 epoch total loss 1.18663681\n",
      "Trained batch 495 batch loss 1.0334692 epoch total loss 1.18632734\n",
      "Trained batch 496 batch loss 1.20063591 epoch total loss 1.18635619\n",
      "Trained batch 497 batch loss 1.29421782 epoch total loss 1.18657315\n",
      "Trained batch 498 batch loss 1.16830766 epoch total loss 1.18653655\n",
      "Trained batch 499 batch loss 1.19938564 epoch total loss 1.1865623\n",
      "Trained batch 500 batch loss 1.24748039 epoch total loss 1.18668425\n",
      "Trained batch 501 batch loss 1.17137623 epoch total loss 1.18665361\n",
      "Trained batch 502 batch loss 0.921205163 epoch total loss 1.18612492\n",
      "Trained batch 503 batch loss 1.05392361 epoch total loss 1.18586195\n",
      "Trained batch 504 batch loss 1.02603745 epoch total loss 1.18554497\n",
      "Trained batch 505 batch loss 1.11493528 epoch total loss 1.18540514\n",
      "Trained batch 506 batch loss 1.11573625 epoch total loss 1.18526745\n",
      "Trained batch 507 batch loss 1.06700087 epoch total loss 1.18503416\n",
      "Trained batch 508 batch loss 1.11601269 epoch total loss 1.18489826\n",
      "Trained batch 509 batch loss 1.33115375 epoch total loss 1.18518567\n",
      "Trained batch 510 batch loss 1.29496896 epoch total loss 1.18540096\n",
      "Trained batch 511 batch loss 1.36461556 epoch total loss 1.18575168\n",
      "Trained batch 512 batch loss 1.15760612 epoch total loss 1.18569672\n",
      "Trained batch 513 batch loss 1.02252316 epoch total loss 1.18537867\n",
      "Trained batch 514 batch loss 1.14788067 epoch total loss 1.18530571\n",
      "Trained batch 515 batch loss 1.15053368 epoch total loss 1.18523812\n",
      "Trained batch 516 batch loss 1.30138874 epoch total loss 1.18546319\n",
      "Trained batch 517 batch loss 1.30277193 epoch total loss 1.18569016\n",
      "Trained batch 518 batch loss 1.32470477 epoch total loss 1.18595862\n",
      "Trained batch 519 batch loss 1.19787991 epoch total loss 1.18598151\n",
      "Trained batch 520 batch loss 1.05321336 epoch total loss 1.18572617\n",
      "Trained batch 521 batch loss 1.15090895 epoch total loss 1.18565929\n",
      "Trained batch 522 batch loss 1.24837172 epoch total loss 1.18577945\n",
      "Trained batch 523 batch loss 1.16095233 epoch total loss 1.18573201\n",
      "Trained batch 524 batch loss 1.20006347 epoch total loss 1.18575931\n",
      "Trained batch 525 batch loss 1.23397171 epoch total loss 1.1858511\n",
      "Trained batch 526 batch loss 1.23418641 epoch total loss 1.18594301\n",
      "Trained batch 527 batch loss 1.11689055 epoch total loss 1.185812\n",
      "Trained batch 528 batch loss 1.11873555 epoch total loss 1.18568492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 529 batch loss 1.0706389 epoch total loss 1.18546736\n",
      "Trained batch 530 batch loss 1.0775758 epoch total loss 1.18526375\n",
      "Trained batch 531 batch loss 1.10187209 epoch total loss 1.18510675\n",
      "Trained batch 532 batch loss 1.09005284 epoch total loss 1.18492806\n",
      "Trained batch 533 batch loss 1.21599376 epoch total loss 1.18498635\n",
      "Trained batch 534 batch loss 1.06146502 epoch total loss 1.18475497\n",
      "Trained batch 535 batch loss 1.12104475 epoch total loss 1.18463588\n",
      "Trained batch 536 batch loss 1.20966351 epoch total loss 1.18468261\n",
      "Trained batch 537 batch loss 1.05017006 epoch total loss 1.18443215\n",
      "Trained batch 538 batch loss 1.2370441 epoch total loss 1.1845299\n",
      "Trained batch 539 batch loss 1.17624521 epoch total loss 1.18451464\n",
      "Trained batch 540 batch loss 1.13786 epoch total loss 1.18442822\n",
      "Trained batch 541 batch loss 1.2568413 epoch total loss 1.18456209\n",
      "Trained batch 542 batch loss 1.1364994 epoch total loss 1.1844734\n",
      "Trained batch 543 batch loss 1.16850448 epoch total loss 1.18444395\n",
      "Trained batch 544 batch loss 1.18067682 epoch total loss 1.18443704\n",
      "Trained batch 545 batch loss 1.04607284 epoch total loss 1.18418312\n",
      "Trained batch 546 batch loss 1.07024276 epoch total loss 1.1839745\n",
      "Trained batch 547 batch loss 1.08092439 epoch total loss 1.18378615\n",
      "Trained batch 548 batch loss 0.993678629 epoch total loss 1.18343914\n",
      "Trained batch 549 batch loss 1.03649497 epoch total loss 1.18317151\n",
      "Trained batch 550 batch loss 1.06068778 epoch total loss 1.18294883\n",
      "Trained batch 551 batch loss 0.978017569 epoch total loss 1.18257689\n",
      "Trained batch 552 batch loss 1.08087873 epoch total loss 1.1823926\n",
      "Trained batch 553 batch loss 1.08877671 epoch total loss 1.18222344\n",
      "Trained batch 554 batch loss 1.07128167 epoch total loss 1.18202317\n",
      "Trained batch 555 batch loss 1.12303185 epoch total loss 1.18191683\n",
      "Trained batch 556 batch loss 1.36808133 epoch total loss 1.18225169\n",
      "Trained batch 557 batch loss 1.2712456 epoch total loss 1.18241155\n",
      "Trained batch 558 batch loss 1.10950541 epoch total loss 1.18228078\n",
      "Trained batch 559 batch loss 1.39201 epoch total loss 1.18265605\n",
      "Trained batch 560 batch loss 1.30346203 epoch total loss 1.18287182\n",
      "Trained batch 561 batch loss 1.26667428 epoch total loss 1.18302119\n",
      "Trained batch 562 batch loss 1.16081715 epoch total loss 1.18298161\n",
      "Trained batch 563 batch loss 1.08797145 epoch total loss 1.18281281\n",
      "Trained batch 564 batch loss 1.01945066 epoch total loss 1.18252325\n",
      "Trained batch 565 batch loss 0.9345631 epoch total loss 1.18208444\n",
      "Trained batch 566 batch loss 1.24174953 epoch total loss 1.18218982\n",
      "Trained batch 567 batch loss 1.11148989 epoch total loss 1.18206513\n",
      "Trained batch 568 batch loss 1.26959026 epoch total loss 1.18221927\n",
      "Trained batch 569 batch loss 1.25175583 epoch total loss 1.18234146\n",
      "Trained batch 570 batch loss 1.23758984 epoch total loss 1.18243849\n",
      "Trained batch 571 batch loss 1.21902883 epoch total loss 1.18250263\n",
      "Trained batch 572 batch loss 1.06102729 epoch total loss 1.1822902\n",
      "Trained batch 573 batch loss 1.11415887 epoch total loss 1.18217134\n",
      "Trained batch 574 batch loss 1.20434642 epoch total loss 1.18221\n",
      "Trained batch 575 batch loss 1.21995139 epoch total loss 1.18227565\n",
      "Trained batch 576 batch loss 1.26921272 epoch total loss 1.18242657\n",
      "Trained batch 577 batch loss 1.36381054 epoch total loss 1.18274093\n",
      "Trained batch 578 batch loss 1.23769569 epoch total loss 1.18283594\n",
      "Trained batch 579 batch loss 1.19906211 epoch total loss 1.18286395\n",
      "Trained batch 580 batch loss 1.27664697 epoch total loss 1.18302572\n",
      "Trained batch 581 batch loss 1.3255024 epoch total loss 1.18327093\n",
      "Trained batch 582 batch loss 1.35335124 epoch total loss 1.18356311\n",
      "Trained batch 583 batch loss 1.33533943 epoch total loss 1.18382347\n",
      "Trained batch 584 batch loss 1.38228381 epoch total loss 1.18416321\n",
      "Trained batch 585 batch loss 1.22168779 epoch total loss 1.18422735\n",
      "Trained batch 586 batch loss 1.28690648 epoch total loss 1.18440259\n",
      "Trained batch 587 batch loss 1.2743814 epoch total loss 1.18455589\n",
      "Trained batch 588 batch loss 1.3070327 epoch total loss 1.18476415\n",
      "Trained batch 589 batch loss 1.16671944 epoch total loss 1.18473351\n",
      "Trained batch 590 batch loss 1.11860108 epoch total loss 1.18462145\n",
      "Trained batch 591 batch loss 1.20329595 epoch total loss 1.18465304\n",
      "Trained batch 592 batch loss 1.12973142 epoch total loss 1.1845603\n",
      "Trained batch 593 batch loss 1.2303499 epoch total loss 1.18463755\n",
      "Trained batch 594 batch loss 1.2412678 epoch total loss 1.18473291\n",
      "Trained batch 595 batch loss 1.19566751 epoch total loss 1.18475127\n",
      "Trained batch 596 batch loss 1.19287491 epoch total loss 1.18476486\n",
      "Trained batch 597 batch loss 1.23917007 epoch total loss 1.18485606\n",
      "Trained batch 598 batch loss 1.14874232 epoch total loss 1.18479562\n",
      "Trained batch 599 batch loss 1.13092291 epoch total loss 1.18470573\n",
      "Trained batch 600 batch loss 1.24874413 epoch total loss 1.18481243\n",
      "Trained batch 601 batch loss 1.15288079 epoch total loss 1.18475926\n",
      "Trained batch 602 batch loss 1.28014803 epoch total loss 1.18491781\n",
      "Trained batch 603 batch loss 1.08592081 epoch total loss 1.18475366\n",
      "Trained batch 604 batch loss 1.1336987 epoch total loss 1.18466914\n",
      "Trained batch 605 batch loss 1.17821813 epoch total loss 1.18465853\n",
      "Trained batch 606 batch loss 1.13490045 epoch total loss 1.18457639\n",
      "Trained batch 607 batch loss 1.00862026 epoch total loss 1.18428648\n",
      "Trained batch 608 batch loss 1.02653182 epoch total loss 1.18402708\n",
      "Trained batch 609 batch loss 1.0115658 epoch total loss 1.18374383\n",
      "Trained batch 610 batch loss 1.2302264 epoch total loss 1.18382\n",
      "Trained batch 611 batch loss 1.28314149 epoch total loss 1.18398249\n",
      "Trained batch 612 batch loss 1.27395916 epoch total loss 1.1841296\n",
      "Trained batch 613 batch loss 1.23979306 epoch total loss 1.18422043\n",
      "Trained batch 614 batch loss 1.43074059 epoch total loss 1.18462193\n",
      "Trained batch 615 batch loss 1.22122276 epoch total loss 1.18468142\n",
      "Trained batch 616 batch loss 1.20501149 epoch total loss 1.18471444\n",
      "Trained batch 617 batch loss 1.28780198 epoch total loss 1.18488157\n",
      "Trained batch 618 batch loss 1.26139629 epoch total loss 1.18500531\n",
      "Trained batch 619 batch loss 1.28084695 epoch total loss 1.18516016\n",
      "Trained batch 620 batch loss 1.13069463 epoch total loss 1.1850723\n",
      "Trained batch 621 batch loss 1.12427294 epoch total loss 1.18497431\n",
      "Trained batch 622 batch loss 0.997486532 epoch total loss 1.18467295\n",
      "Trained batch 623 batch loss 1.06910086 epoch total loss 1.18448746\n",
      "Trained batch 624 batch loss 1.21747303 epoch total loss 1.18454027\n",
      "Trained batch 625 batch loss 1.13664651 epoch total loss 1.18446362\n",
      "Trained batch 626 batch loss 1.28401732 epoch total loss 1.18462265\n",
      "Trained batch 627 batch loss 1.32264018 epoch total loss 1.18484282\n",
      "Trained batch 628 batch loss 1.21555305 epoch total loss 1.1848917\n",
      "Trained batch 629 batch loss 1.17850351 epoch total loss 1.18488157\n",
      "Trained batch 630 batch loss 1.25550973 epoch total loss 1.18499362\n",
      "Trained batch 631 batch loss 1.21590066 epoch total loss 1.18504262\n",
      "Trained batch 632 batch loss 1.12356222 epoch total loss 1.18494534\n",
      "Trained batch 633 batch loss 1.25660336 epoch total loss 1.18505847\n",
      "Trained batch 634 batch loss 1.18910837 epoch total loss 1.18506479\n",
      "Trained batch 635 batch loss 1.10704827 epoch total loss 1.18494201\n",
      "Trained batch 636 batch loss 1.25789332 epoch total loss 1.18505669\n",
      "Trained batch 637 batch loss 1.33698595 epoch total loss 1.18529522\n",
      "Trained batch 638 batch loss 1.09173894 epoch total loss 1.18514848\n",
      "Trained batch 639 batch loss 1.1117183 epoch total loss 1.18503356\n",
      "Trained batch 640 batch loss 0.987667441 epoch total loss 1.18472517\n",
      "Trained batch 641 batch loss 1.08878613 epoch total loss 1.18457556\n",
      "Trained batch 642 batch loss 1.04799199 epoch total loss 1.18436277\n",
      "Trained batch 643 batch loss 0.952777863 epoch total loss 1.18400264\n",
      "Trained batch 644 batch loss 1.06621528 epoch total loss 1.18381965\n",
      "Trained batch 645 batch loss 1.04484236 epoch total loss 1.18360424\n",
      "Trained batch 646 batch loss 1.12058103 epoch total loss 1.18350673\n",
      "Trained batch 647 batch loss 1.05567861 epoch total loss 1.1833092\n",
      "Trained batch 648 batch loss 1.24111891 epoch total loss 1.18339825\n",
      "Trained batch 649 batch loss 1.29244137 epoch total loss 1.18356633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 650 batch loss 1.20869899 epoch total loss 1.18360496\n",
      "Trained batch 651 batch loss 1.24348009 epoch total loss 1.18369687\n",
      "Trained batch 652 batch loss 1.18853915 epoch total loss 1.18370426\n",
      "Trained batch 653 batch loss 1.24842453 epoch total loss 1.18380344\n",
      "Trained batch 654 batch loss 1.19053328 epoch total loss 1.18381369\n",
      "Trained batch 655 batch loss 1.24847949 epoch total loss 1.1839124\n",
      "Trained batch 656 batch loss 1.19506 epoch total loss 1.18392944\n",
      "Trained batch 657 batch loss 1.28484499 epoch total loss 1.1840831\n",
      "Trained batch 658 batch loss 1.31334722 epoch total loss 1.18427956\n",
      "Trained batch 659 batch loss 1.15515518 epoch total loss 1.18423533\n",
      "Trained batch 660 batch loss 1.22189867 epoch total loss 1.18429244\n",
      "Trained batch 661 batch loss 1.22455406 epoch total loss 1.18435335\n",
      "Trained batch 662 batch loss 1.23683798 epoch total loss 1.18443263\n",
      "Trained batch 663 batch loss 1.22762442 epoch total loss 1.18449771\n",
      "Trained batch 664 batch loss 1.22076011 epoch total loss 1.18455231\n",
      "Trained batch 665 batch loss 1.14607406 epoch total loss 1.18449438\n",
      "Trained batch 666 batch loss 1.25057685 epoch total loss 1.18459356\n",
      "Trained batch 667 batch loss 1.24913037 epoch total loss 1.18469036\n",
      "Trained batch 668 batch loss 1.31287694 epoch total loss 1.18488228\n",
      "Trained batch 669 batch loss 1.28109527 epoch total loss 1.18502605\n",
      "Trained batch 670 batch loss 1.30350614 epoch total loss 1.18520284\n",
      "Trained batch 671 batch loss 1.34655809 epoch total loss 1.1854434\n",
      "Trained batch 672 batch loss 1.14767694 epoch total loss 1.18538725\n",
      "Trained batch 673 batch loss 1.13791478 epoch total loss 1.18531668\n",
      "Trained batch 674 batch loss 1.14450145 epoch total loss 1.18525624\n",
      "Trained batch 675 batch loss 1.27936363 epoch total loss 1.1853956\n",
      "Trained batch 676 batch loss 1.07097471 epoch total loss 1.18522632\n",
      "Trained batch 677 batch loss 1.23591459 epoch total loss 1.18530118\n",
      "Trained batch 678 batch loss 1.19907951 epoch total loss 1.18532157\n",
      "Trained batch 679 batch loss 1.1852746 epoch total loss 1.18532157\n",
      "Trained batch 680 batch loss 1.27061927 epoch total loss 1.18544698\n",
      "Trained batch 681 batch loss 1.20446515 epoch total loss 1.18547487\n",
      "Trained batch 682 batch loss 1.04533923 epoch total loss 1.18526947\n",
      "Trained batch 683 batch loss 1.00517797 epoch total loss 1.18500578\n",
      "Trained batch 684 batch loss 1.15546453 epoch total loss 1.18496263\n",
      "Trained batch 685 batch loss 1.12171876 epoch total loss 1.18487024\n",
      "Trained batch 686 batch loss 1.13661027 epoch total loss 1.18479991\n",
      "Trained batch 687 batch loss 1.15443921 epoch total loss 1.18475568\n",
      "Trained batch 688 batch loss 1.12269199 epoch total loss 1.18466544\n",
      "Trained batch 689 batch loss 1.25627208 epoch total loss 1.18476939\n",
      "Trained batch 690 batch loss 1.3052187 epoch total loss 1.18494391\n",
      "Trained batch 691 batch loss 1.18055975 epoch total loss 1.1849376\n",
      "Trained batch 692 batch loss 1.33022428 epoch total loss 1.18514752\n",
      "Trained batch 693 batch loss 1.4531709 epoch total loss 1.18553424\n",
      "Trained batch 694 batch loss 1.3919363 epoch total loss 1.18583167\n",
      "Trained batch 695 batch loss 1.06660175 epoch total loss 1.18566012\n",
      "Trained batch 696 batch loss 1.18048537 epoch total loss 1.18565261\n",
      "Trained batch 697 batch loss 1.24090064 epoch total loss 1.18573189\n",
      "Trained batch 698 batch loss 1.2892493 epoch total loss 1.18588018\n",
      "Trained batch 699 batch loss 1.27509832 epoch total loss 1.18600786\n",
      "Trained batch 700 batch loss 1.19869912 epoch total loss 1.18602586\n",
      "Trained batch 701 batch loss 1.20159554 epoch total loss 1.18604815\n",
      "Trained batch 702 batch loss 1.35519195 epoch total loss 1.18628907\n",
      "Trained batch 703 batch loss 1.21991014 epoch total loss 1.18633687\n",
      "Trained batch 704 batch loss 1.24656129 epoch total loss 1.18642247\n",
      "Trained batch 705 batch loss 1.33716583 epoch total loss 1.18663621\n",
      "Trained batch 706 batch loss 1.2219677 epoch total loss 1.18668628\n",
      "Trained batch 707 batch loss 1.26385224 epoch total loss 1.18679547\n",
      "Trained batch 708 batch loss 1.23174 epoch total loss 1.18685901\n",
      "Trained batch 709 batch loss 1.19592416 epoch total loss 1.18687177\n",
      "Trained batch 710 batch loss 1.18033469 epoch total loss 1.18686259\n",
      "Trained batch 711 batch loss 1.25482321 epoch total loss 1.18695819\n",
      "Trained batch 712 batch loss 1.22235096 epoch total loss 1.1870079\n",
      "Trained batch 713 batch loss 1.16891408 epoch total loss 1.18698239\n",
      "Trained batch 714 batch loss 1.16962516 epoch total loss 1.18695807\n",
      "Trained batch 715 batch loss 1.06435251 epoch total loss 1.18678665\n",
      "Trained batch 716 batch loss 1.26113415 epoch total loss 1.18689036\n",
      "Trained batch 717 batch loss 1.22752786 epoch total loss 1.18694711\n",
      "Trained batch 718 batch loss 1.16707051 epoch total loss 1.18691945\n",
      "Trained batch 719 batch loss 1.23474801 epoch total loss 1.18698597\n",
      "Trained batch 720 batch loss 1.23204422 epoch total loss 1.18704855\n",
      "Trained batch 721 batch loss 1.27222681 epoch total loss 1.18716669\n",
      "Trained batch 722 batch loss 1.24058688 epoch total loss 1.1872406\n",
      "Trained batch 723 batch loss 1.25734794 epoch total loss 1.18733752\n",
      "Trained batch 724 batch loss 1.21266925 epoch total loss 1.18737257\n",
      "Trained batch 725 batch loss 1.10447228 epoch total loss 1.18725824\n",
      "Trained batch 726 batch loss 1.18114448 epoch total loss 1.18724978\n",
      "Trained batch 727 batch loss 1.18027687 epoch total loss 1.18724024\n",
      "Trained batch 728 batch loss 1.1859535 epoch total loss 1.18723845\n",
      "Trained batch 729 batch loss 1.09778059 epoch total loss 1.18711579\n",
      "Trained batch 730 batch loss 1.09861434 epoch total loss 1.18699455\n",
      "Trained batch 731 batch loss 0.996731758 epoch total loss 1.18673432\n",
      "Trained batch 732 batch loss 1.09462214 epoch total loss 1.18660843\n",
      "Trained batch 733 batch loss 1.06524289 epoch total loss 1.18644285\n",
      "Trained batch 734 batch loss 1.30411077 epoch total loss 1.18660319\n",
      "Trained batch 735 batch loss 1.14405179 epoch total loss 1.18654525\n",
      "Trained batch 736 batch loss 1.2653321 epoch total loss 1.1866523\n",
      "Trained batch 737 batch loss 1.12727261 epoch total loss 1.18657172\n",
      "Trained batch 738 batch loss 1.26636803 epoch total loss 1.18667984\n",
      "Trained batch 739 batch loss 1.28570127 epoch total loss 1.18681383\n",
      "Trained batch 740 batch loss 1.30027771 epoch total loss 1.18696713\n",
      "Trained batch 741 batch loss 1.27674091 epoch total loss 1.18708837\n",
      "Trained batch 742 batch loss 1.25694549 epoch total loss 1.18718243\n",
      "Trained batch 743 batch loss 1.22122872 epoch total loss 1.18722832\n",
      "Trained batch 744 batch loss 1.19412029 epoch total loss 1.18723762\n",
      "Trained batch 745 batch loss 1.30331385 epoch total loss 1.18739331\n",
      "Trained batch 746 batch loss 1.34734035 epoch total loss 1.18760777\n",
      "Trained batch 747 batch loss 1.21866131 epoch total loss 1.18764937\n",
      "Trained batch 748 batch loss 1.13429642 epoch total loss 1.18757796\n",
      "Trained batch 749 batch loss 1.34041286 epoch total loss 1.18778205\n",
      "Trained batch 750 batch loss 1.30834031 epoch total loss 1.18794274\n",
      "Trained batch 751 batch loss 1.18604016 epoch total loss 1.18794024\n",
      "Trained batch 752 batch loss 1.26284683 epoch total loss 1.18803978\n",
      "Trained batch 753 batch loss 1.13614905 epoch total loss 1.18797088\n",
      "Trained batch 754 batch loss 1.01783729 epoch total loss 1.18774533\n",
      "Trained batch 755 batch loss 1.18988132 epoch total loss 1.18774807\n",
      "Trained batch 756 batch loss 1.14117312 epoch total loss 1.18768644\n",
      "Trained batch 757 batch loss 1.21018279 epoch total loss 1.18771625\n",
      "Trained batch 758 batch loss 1.21636224 epoch total loss 1.18775403\n",
      "Trained batch 759 batch loss 1.30752563 epoch total loss 1.18791187\n",
      "Trained batch 760 batch loss 1.25911427 epoch total loss 1.18800557\n",
      "Trained batch 761 batch loss 1.19087756 epoch total loss 1.18800926\n",
      "Trained batch 762 batch loss 1.18015933 epoch total loss 1.18799901\n",
      "Trained batch 763 batch loss 1.11830866 epoch total loss 1.1879077\n",
      "Trained batch 764 batch loss 1.14232266 epoch total loss 1.18784797\n",
      "Trained batch 765 batch loss 1.19325042 epoch total loss 1.18785501\n",
      "Trained batch 766 batch loss 1.13799226 epoch total loss 1.18778992\n",
      "Trained batch 767 batch loss 1.26518774 epoch total loss 1.18789089\n",
      "Trained batch 768 batch loss 1.16492975 epoch total loss 1.18786097\n",
      "Trained batch 769 batch loss 1.16237557 epoch total loss 1.18782783\n",
      "Trained batch 770 batch loss 1.17815053 epoch total loss 1.18781519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 771 batch loss 1.20784867 epoch total loss 1.18784118\n",
      "Trained batch 772 batch loss 1.24865174 epoch total loss 1.18792\n",
      "Trained batch 773 batch loss 1.21983087 epoch total loss 1.18796122\n",
      "Trained batch 774 batch loss 1.20415246 epoch total loss 1.1879822\n",
      "Trained batch 775 batch loss 1.13070369 epoch total loss 1.18790829\n",
      "Trained batch 776 batch loss 1.117854 epoch total loss 1.18781805\n",
      "Trained batch 777 batch loss 1.06714952 epoch total loss 1.18766272\n",
      "Trained batch 778 batch loss 1.17589176 epoch total loss 1.18764758\n",
      "Trained batch 779 batch loss 1.26373565 epoch total loss 1.18774521\n",
      "Trained batch 780 batch loss 1.1522361 epoch total loss 1.18769968\n",
      "Trained batch 781 batch loss 1.32011545 epoch total loss 1.18786931\n",
      "Trained batch 782 batch loss 1.32438183 epoch total loss 1.18804383\n",
      "Trained batch 783 batch loss 1.24477196 epoch total loss 1.18811631\n",
      "Trained batch 784 batch loss 1.00493991 epoch total loss 1.18788266\n",
      "Trained batch 785 batch loss 1.08019197 epoch total loss 1.18774545\n",
      "Trained batch 786 batch loss 1.2563175 epoch total loss 1.18783271\n",
      "Trained batch 787 batch loss 1.28479028 epoch total loss 1.18795598\n",
      "Trained batch 788 batch loss 1.30422401 epoch total loss 1.18810344\n",
      "Trained batch 789 batch loss 1.21473765 epoch total loss 1.18813717\n",
      "Trained batch 790 batch loss 1.18414903 epoch total loss 1.18813217\n",
      "Trained batch 791 batch loss 1.16579914 epoch total loss 1.18810391\n",
      "Trained batch 792 batch loss 1.17406762 epoch total loss 1.18808615\n",
      "Trained batch 793 batch loss 1.13287103 epoch total loss 1.18801653\n",
      "Trained batch 794 batch loss 1.24647939 epoch total loss 1.18809009\n",
      "Trained batch 795 batch loss 1.17085302 epoch total loss 1.18806839\n",
      "Trained batch 796 batch loss 1.16300058 epoch total loss 1.18803692\n",
      "Trained batch 797 batch loss 1.22653353 epoch total loss 1.18808532\n",
      "Trained batch 798 batch loss 1.18096519 epoch total loss 1.18807638\n",
      "Trained batch 799 batch loss 1.15742373 epoch total loss 1.18803799\n",
      "Trained batch 800 batch loss 1.23725426 epoch total loss 1.1880995\n",
      "Trained batch 801 batch loss 1.27831435 epoch total loss 1.18821216\n",
      "Trained batch 802 batch loss 1.14265239 epoch total loss 1.18815529\n",
      "Trained batch 803 batch loss 1.28970027 epoch total loss 1.18828177\n",
      "Trained batch 804 batch loss 1.14019799 epoch total loss 1.18822193\n",
      "Trained batch 805 batch loss 1.14097452 epoch total loss 1.18816328\n",
      "Trained batch 806 batch loss 1.1026355 epoch total loss 1.18805718\n",
      "Trained batch 807 batch loss 1.22460949 epoch total loss 1.18810248\n",
      "Trained batch 808 batch loss 1.29806173 epoch total loss 1.1882385\n",
      "Trained batch 809 batch loss 1.47149825 epoch total loss 1.18858862\n",
      "Trained batch 810 batch loss 1.24655926 epoch total loss 1.18866026\n",
      "Trained batch 811 batch loss 1.21544063 epoch total loss 1.18869328\n",
      "Trained batch 812 batch loss 1.18974614 epoch total loss 1.1886946\n",
      "Trained batch 813 batch loss 1.44455302 epoch total loss 1.18900931\n",
      "Trained batch 814 batch loss 1.20308793 epoch total loss 1.18902659\n",
      "Trained batch 815 batch loss 1.17845726 epoch total loss 1.18901372\n",
      "Trained batch 816 batch loss 1.23721421 epoch total loss 1.18907273\n",
      "Trained batch 817 batch loss 1.15109599 epoch total loss 1.18902636\n",
      "Trained batch 818 batch loss 1.155761 epoch total loss 1.18898571\n",
      "Trained batch 819 batch loss 1.27685857 epoch total loss 1.18909299\n",
      "Trained batch 820 batch loss 1.17243719 epoch total loss 1.18907261\n",
      "Trained batch 821 batch loss 1.18448317 epoch total loss 1.18906701\n",
      "Trained batch 822 batch loss 1.10437894 epoch total loss 1.18896401\n",
      "Trained batch 823 batch loss 1.15297973 epoch total loss 1.18892026\n",
      "Trained batch 824 batch loss 1.16839349 epoch total loss 1.18889534\n",
      "Trained batch 825 batch loss 1.17969954 epoch total loss 1.18888426\n",
      "Trained batch 826 batch loss 1.18680668 epoch total loss 1.18888175\n",
      "Trained batch 827 batch loss 1.18291819 epoch total loss 1.18887448\n",
      "Trained batch 828 batch loss 1.03195655 epoch total loss 1.18868506\n",
      "Trained batch 829 batch loss 0.967120886 epoch total loss 1.18841779\n",
      "Trained batch 830 batch loss 1.18532586 epoch total loss 1.18841398\n",
      "Trained batch 831 batch loss 1.43183243 epoch total loss 1.18870687\n",
      "Trained batch 832 batch loss 1.31509721 epoch total loss 1.18885887\n",
      "Trained batch 833 batch loss 1.08260345 epoch total loss 1.18873131\n",
      "Trained batch 834 batch loss 1.24981797 epoch total loss 1.18880451\n",
      "Trained batch 835 batch loss 1.11036694 epoch total loss 1.18871057\n",
      "Trained batch 836 batch loss 1.12587905 epoch total loss 1.18863535\n",
      "Trained batch 837 batch loss 1.05096507 epoch total loss 1.18847084\n",
      "Trained batch 838 batch loss 1.19410348 epoch total loss 1.18847764\n",
      "Trained batch 839 batch loss 1.22732425 epoch total loss 1.18852389\n",
      "Trained batch 840 batch loss 1.23593867 epoch total loss 1.18858027\n",
      "Trained batch 841 batch loss 1.21427631 epoch total loss 1.18861091\n",
      "Trained batch 842 batch loss 1.21807313 epoch total loss 1.18864584\n",
      "Trained batch 843 batch loss 1.16923428 epoch total loss 1.18862283\n",
      "Trained batch 844 batch loss 1.14564395 epoch total loss 1.18857193\n",
      "Trained batch 845 batch loss 1.12072802 epoch total loss 1.1884917\n",
      "Trained batch 846 batch loss 1.12846184 epoch total loss 1.18842077\n",
      "Trained batch 847 batch loss 1.27569246 epoch total loss 1.18852377\n",
      "Trained batch 848 batch loss 1.24237263 epoch total loss 1.18858731\n",
      "Trained batch 849 batch loss 1.0865047 epoch total loss 1.18846703\n",
      "Trained batch 850 batch loss 1.04298711 epoch total loss 1.18829584\n",
      "Trained batch 851 batch loss 1.04004133 epoch total loss 1.18812156\n",
      "Trained batch 852 batch loss 1.13595939 epoch total loss 1.1880604\n",
      "Trained batch 853 batch loss 0.934832692 epoch total loss 1.18776357\n",
      "Trained batch 854 batch loss 1.0743438 epoch total loss 1.18763077\n",
      "Trained batch 855 batch loss 1.15565026 epoch total loss 1.18759334\n",
      "Trained batch 856 batch loss 1.11090636 epoch total loss 1.1875037\n",
      "Trained batch 857 batch loss 1.05959654 epoch total loss 1.18735445\n",
      "Trained batch 858 batch loss 1.08815777 epoch total loss 1.18723881\n",
      "Trained batch 859 batch loss 1.184304 epoch total loss 1.18723536\n",
      "Trained batch 860 batch loss 1.1506083 epoch total loss 1.1871928\n",
      "Trained batch 861 batch loss 1.22334373 epoch total loss 1.18723476\n",
      "Trained batch 862 batch loss 1.29832077 epoch total loss 1.18736374\n",
      "Trained batch 863 batch loss 1.23717928 epoch total loss 1.18742132\n",
      "Trained batch 864 batch loss 1.19181323 epoch total loss 1.18742645\n",
      "Trained batch 865 batch loss 1.19710481 epoch total loss 1.18743765\n",
      "Trained batch 866 batch loss 1.18383753 epoch total loss 1.18743348\n",
      "Trained batch 867 batch loss 1.04018641 epoch total loss 1.18726361\n",
      "Trained batch 868 batch loss 1.14047992 epoch total loss 1.18720973\n",
      "Trained batch 869 batch loss 1.28103077 epoch total loss 1.18731761\n",
      "Trained batch 870 batch loss 1.42783666 epoch total loss 1.18759418\n",
      "Trained batch 871 batch loss 1.30305409 epoch total loss 1.18772674\n",
      "Trained batch 872 batch loss 1.17591977 epoch total loss 1.18771315\n",
      "Trained batch 873 batch loss 1.22652578 epoch total loss 1.18775773\n",
      "Trained batch 874 batch loss 1.2139858 epoch total loss 1.18778777\n",
      "Trained batch 875 batch loss 1.16895032 epoch total loss 1.18776619\n",
      "Trained batch 876 batch loss 1.04231107 epoch total loss 1.18760014\n",
      "Trained batch 877 batch loss 1.26140118 epoch total loss 1.1876843\n",
      "Trained batch 878 batch loss 1.14863062 epoch total loss 1.18763983\n",
      "Trained batch 879 batch loss 1.10392451 epoch total loss 1.18754458\n",
      "Trained batch 880 batch loss 1.06293607 epoch total loss 1.18740308\n",
      "Trained batch 881 batch loss 1.01104105 epoch total loss 1.18720281\n",
      "Trained batch 882 batch loss 0.924583077 epoch total loss 1.18690503\n",
      "Trained batch 883 batch loss 1.12104428 epoch total loss 1.18683052\n",
      "Trained batch 884 batch loss 1.50527442 epoch total loss 1.18719065\n",
      "Trained batch 885 batch loss 1.44081926 epoch total loss 1.18747723\n",
      "Trained batch 886 batch loss 1.36840367 epoch total loss 1.18768144\n",
      "Trained batch 887 batch loss 1.2800281 epoch total loss 1.18778551\n",
      "Trained batch 888 batch loss 1.37634623 epoch total loss 1.18799794\n",
      "Trained batch 889 batch loss 1.430493 epoch total loss 1.18827069\n",
      "Trained batch 890 batch loss 1.45967436 epoch total loss 1.18857574\n",
      "Trained batch 891 batch loss 1.35119808 epoch total loss 1.18875825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 892 batch loss 1.35008919 epoch total loss 1.18893909\n",
      "Trained batch 893 batch loss 1.32928681 epoch total loss 1.18909633\n",
      "Trained batch 894 batch loss 1.26346457 epoch total loss 1.18917954\n",
      "Trained batch 895 batch loss 1.24970984 epoch total loss 1.18924713\n",
      "Trained batch 896 batch loss 1.23213434 epoch total loss 1.18929505\n",
      "Trained batch 897 batch loss 1.19513285 epoch total loss 1.18930161\n",
      "Trained batch 898 batch loss 1.20883751 epoch total loss 1.18932343\n",
      "Trained batch 899 batch loss 1.22078192 epoch total loss 1.18935847\n",
      "Trained batch 900 batch loss 1.22486854 epoch total loss 1.18939793\n",
      "Trained batch 901 batch loss 1.21520424 epoch total loss 1.18942654\n",
      "Trained batch 902 batch loss 1.28186369 epoch total loss 1.18952906\n",
      "Trained batch 903 batch loss 1.22111583 epoch total loss 1.18956399\n",
      "Trained batch 904 batch loss 1.3763957 epoch total loss 1.18977058\n",
      "Trained batch 905 batch loss 1.13360023 epoch total loss 1.18970847\n",
      "Trained batch 906 batch loss 1.02456295 epoch total loss 1.1895262\n",
      "Trained batch 907 batch loss 1.10170078 epoch total loss 1.18942928\n",
      "Trained batch 908 batch loss 1.16236341 epoch total loss 1.18939948\n",
      "Trained batch 909 batch loss 1.27155519 epoch total loss 1.18949\n",
      "Trained batch 910 batch loss 1.27280152 epoch total loss 1.18958151\n",
      "Trained batch 911 batch loss 1.13191867 epoch total loss 1.18951821\n",
      "Trained batch 912 batch loss 1.12880051 epoch total loss 1.18945169\n",
      "Trained batch 913 batch loss 1.05454302 epoch total loss 1.18930387\n",
      "Trained batch 914 batch loss 1.05637646 epoch total loss 1.18915856\n",
      "Trained batch 915 batch loss 1.17510951 epoch total loss 1.18914306\n",
      "Trained batch 916 batch loss 1.15615892 epoch total loss 1.18910706\n",
      "Trained batch 917 batch loss 1.26146138 epoch total loss 1.18918598\n",
      "Trained batch 918 batch loss 1.2232641 epoch total loss 1.18922305\n",
      "Trained batch 919 batch loss 1.28080201 epoch total loss 1.18932271\n",
      "Trained batch 920 batch loss 1.29301035 epoch total loss 1.18943536\n",
      "Trained batch 921 batch loss 1.1882962 epoch total loss 1.18943417\n",
      "Trained batch 922 batch loss 1.08586073 epoch total loss 1.18932176\n",
      "Trained batch 923 batch loss 1.02617908 epoch total loss 1.18914497\n",
      "Trained batch 924 batch loss 1.15804935 epoch total loss 1.18911135\n",
      "Trained batch 925 batch loss 1.19294167 epoch total loss 1.18911552\n",
      "Trained batch 926 batch loss 1.33792973 epoch total loss 1.18927622\n",
      "Trained batch 927 batch loss 1.24375498 epoch total loss 1.18933499\n",
      "Trained batch 928 batch loss 1.2366817 epoch total loss 1.18938601\n",
      "Trained batch 929 batch loss 1.11562216 epoch total loss 1.18930662\n",
      "Trained batch 930 batch loss 1.16325378 epoch total loss 1.1892786\n",
      "Trained batch 931 batch loss 1.1894381 epoch total loss 1.18927872\n",
      "Trained batch 932 batch loss 1.02317262 epoch total loss 1.1891005\n",
      "Trained batch 933 batch loss 1.13171864 epoch total loss 1.18903899\n",
      "Trained batch 934 batch loss 1.23041987 epoch total loss 1.18908334\n",
      "Trained batch 935 batch loss 1.27294934 epoch total loss 1.1891731\n",
      "Trained batch 936 batch loss 1.10057271 epoch total loss 1.18907845\n",
      "Trained batch 937 batch loss 1.02788639 epoch total loss 1.18890631\n",
      "Trained batch 938 batch loss 1.13910925 epoch total loss 1.18885326\n",
      "Trained batch 939 batch loss 1.17335725 epoch total loss 1.18883681\n",
      "Trained batch 940 batch loss 1.13727283 epoch total loss 1.18878198\n",
      "Trained batch 941 batch loss 1.27704406 epoch total loss 1.18887579\n",
      "Trained batch 942 batch loss 1.18552959 epoch total loss 1.18887234\n",
      "Trained batch 943 batch loss 1.2677325 epoch total loss 1.1889559\n",
      "Trained batch 944 batch loss 1.24073935 epoch total loss 1.18901074\n",
      "Trained batch 945 batch loss 1.25192344 epoch total loss 1.18907738\n",
      "Trained batch 946 batch loss 1.33920908 epoch total loss 1.18923604\n",
      "Trained batch 947 batch loss 1.36207509 epoch total loss 1.18941855\n",
      "Trained batch 948 batch loss 1.10140884 epoch total loss 1.18932581\n",
      "Trained batch 949 batch loss 1.11865759 epoch total loss 1.1892513\n",
      "Trained batch 950 batch loss 1.16907573 epoch total loss 1.18923008\n",
      "Trained batch 951 batch loss 1.13126171 epoch total loss 1.18916905\n",
      "Trained batch 952 batch loss 1.02684188 epoch total loss 1.18899858\n",
      "Trained batch 953 batch loss 1.11556423 epoch total loss 1.18892157\n",
      "Trained batch 954 batch loss 1.11934876 epoch total loss 1.18884861\n",
      "Trained batch 955 batch loss 1.13746727 epoch total loss 1.18879485\n",
      "Trained batch 956 batch loss 1.30495548 epoch total loss 1.18891633\n",
      "Trained batch 957 batch loss 1.27053225 epoch total loss 1.18900156\n",
      "Trained batch 958 batch loss 1.10747623 epoch total loss 1.18891644\n",
      "Trained batch 959 batch loss 1.39083755 epoch total loss 1.18912697\n",
      "Trained batch 960 batch loss 1.18613684 epoch total loss 1.18912387\n",
      "Trained batch 961 batch loss 1.1450851 epoch total loss 1.18907809\n",
      "Trained batch 962 batch loss 1.26660347 epoch total loss 1.18915868\n",
      "Trained batch 963 batch loss 1.36004329 epoch total loss 1.18933618\n",
      "Trained batch 964 batch loss 1.33523536 epoch total loss 1.18948746\n",
      "Trained batch 965 batch loss 1.18096304 epoch total loss 1.18947852\n",
      "Trained batch 966 batch loss 1.28429079 epoch total loss 1.18957675\n",
      "Trained batch 967 batch loss 1.28600371 epoch total loss 1.1896764\n",
      "Trained batch 968 batch loss 1.21575868 epoch total loss 1.18970335\n",
      "Trained batch 969 batch loss 1.22561777 epoch total loss 1.1897403\n",
      "Trained batch 970 batch loss 1.21973491 epoch total loss 1.18977129\n",
      "Trained batch 971 batch loss 1.13571978 epoch total loss 1.18971562\n",
      "Trained batch 972 batch loss 1.20396411 epoch total loss 1.18973029\n",
      "Trained batch 973 batch loss 1.18216133 epoch total loss 1.18972254\n",
      "Trained batch 974 batch loss 1.36172044 epoch total loss 1.18989909\n",
      "Trained batch 975 batch loss 1.25192451 epoch total loss 1.18996274\n",
      "Trained batch 976 batch loss 0.974285901 epoch total loss 1.18974161\n",
      "Trained batch 977 batch loss 1.20930684 epoch total loss 1.18976176\n",
      "Trained batch 978 batch loss 1.38364983 epoch total loss 1.18996\n",
      "Trained batch 979 batch loss 1.42325377 epoch total loss 1.1901983\n",
      "Trained batch 980 batch loss 1.39110684 epoch total loss 1.19040322\n",
      "Trained batch 981 batch loss 1.24018788 epoch total loss 1.19045413\n",
      "Trained batch 982 batch loss 1.3062067 epoch total loss 1.1905719\n",
      "Trained batch 983 batch loss 1.20333219 epoch total loss 1.1905849\n",
      "Trained batch 984 batch loss 1.2225374 epoch total loss 1.19061744\n",
      "Trained batch 985 batch loss 1.23174989 epoch total loss 1.19065905\n",
      "Trained batch 986 batch loss 1.2171855 epoch total loss 1.19068599\n",
      "Trained batch 987 batch loss 1.25145769 epoch total loss 1.1907475\n",
      "Trained batch 988 batch loss 1.14575255 epoch total loss 1.19070196\n",
      "Trained batch 989 batch loss 1.23110867 epoch total loss 1.19074285\n",
      "Trained batch 990 batch loss 1.16371083 epoch total loss 1.19071555\n",
      "Trained batch 991 batch loss 1.10993111 epoch total loss 1.19063401\n",
      "Trained batch 992 batch loss 1.02231276 epoch total loss 1.19046438\n",
      "Trained batch 993 batch loss 0.996806145 epoch total loss 1.19026935\n",
      "Trained batch 994 batch loss 1.18371272 epoch total loss 1.19026279\n",
      "Trained batch 995 batch loss 1.19292521 epoch total loss 1.19026542\n",
      "Trained batch 996 batch loss 1.35726213 epoch total loss 1.19043314\n",
      "Trained batch 997 batch loss 1.32489979 epoch total loss 1.19056809\n",
      "Trained batch 998 batch loss 1.31802797 epoch total loss 1.19069576\n",
      "Trained batch 999 batch loss 1.28295958 epoch total loss 1.19078803\n",
      "Trained batch 1000 batch loss 1.2315985 epoch total loss 1.1908288\n",
      "Trained batch 1001 batch loss 1.2449367 epoch total loss 1.19088292\n",
      "Trained batch 1002 batch loss 1.23391056 epoch total loss 1.19092584\n",
      "Trained batch 1003 batch loss 1.42343688 epoch total loss 1.1911577\n",
      "Trained batch 1004 batch loss 1.17994392 epoch total loss 1.19114649\n",
      "Trained batch 1005 batch loss 1.21711063 epoch total loss 1.19117248\n",
      "Trained batch 1006 batch loss 1.38209748 epoch total loss 1.19136226\n",
      "Trained batch 1007 batch loss 1.27559912 epoch total loss 1.19144595\n",
      "Trained batch 1008 batch loss 1.19701195 epoch total loss 1.19145143\n",
      "Trained batch 1009 batch loss 1.22119808 epoch total loss 1.19148088\n",
      "Trained batch 1010 batch loss 1.10639143 epoch total loss 1.19139671\n",
      "Trained batch 1011 batch loss 1.18549466 epoch total loss 1.19139087\n",
      "Trained batch 1012 batch loss 1.22673452 epoch total loss 1.1914258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1013 batch loss 1.1519556 epoch total loss 1.19138682\n",
      "Trained batch 1014 batch loss 1.21082556 epoch total loss 1.19140601\n",
      "Trained batch 1015 batch loss 1.10851 epoch total loss 1.19132435\n",
      "Trained batch 1016 batch loss 1.1868546 epoch total loss 1.19132\n",
      "Trained batch 1017 batch loss 1.14261341 epoch total loss 1.19127202\n",
      "Trained batch 1018 batch loss 1.15808916 epoch total loss 1.19123948\n",
      "Trained batch 1019 batch loss 1.25816393 epoch total loss 1.19130516\n",
      "Trained batch 1020 batch loss 1.15254974 epoch total loss 1.19126713\n",
      "Trained batch 1021 batch loss 1.15214682 epoch total loss 1.19122887\n",
      "Trained batch 1022 batch loss 1.11678684 epoch total loss 1.19115603\n",
      "Trained batch 1023 batch loss 1.21013749 epoch total loss 1.19117451\n",
      "Trained batch 1024 batch loss 1.14703977 epoch total loss 1.19113147\n",
      "Trained batch 1025 batch loss 1.1494112 epoch total loss 1.19109082\n",
      "Trained batch 1026 batch loss 1.08024514 epoch total loss 1.1909827\n",
      "Trained batch 1027 batch loss 1.07763159 epoch total loss 1.19087231\n",
      "Trained batch 1028 batch loss 1.07252 epoch total loss 1.19075716\n",
      "Trained batch 1029 batch loss 1.03748095 epoch total loss 1.19060826\n",
      "Trained batch 1030 batch loss 1.07351494 epoch total loss 1.19049454\n",
      "Trained batch 1031 batch loss 1.0866847 epoch total loss 1.19039381\n",
      "Trained batch 1032 batch loss 1.03280485 epoch total loss 1.1902411\n",
      "Trained batch 1033 batch loss 1.29324341 epoch total loss 1.19034088\n",
      "Trained batch 1034 batch loss 1.23157823 epoch total loss 1.19038069\n",
      "Trained batch 1035 batch loss 1.07985711 epoch total loss 1.19027388\n",
      "Trained batch 1036 batch loss 1.23545361 epoch total loss 1.19031751\n",
      "Trained batch 1037 batch loss 1.24236369 epoch total loss 1.1903677\n",
      "Trained batch 1038 batch loss 1.0345552 epoch total loss 1.19021749\n",
      "Trained batch 1039 batch loss 1.1459949 epoch total loss 1.19017494\n",
      "Trained batch 1040 batch loss 1.2464416 epoch total loss 1.19022906\n",
      "Trained batch 1041 batch loss 1.12143815 epoch total loss 1.19016302\n",
      "Trained batch 1042 batch loss 1.13103163 epoch total loss 1.19010627\n",
      "Trained batch 1043 batch loss 1.01327217 epoch total loss 1.18993676\n",
      "Trained batch 1044 batch loss 1.10477424 epoch total loss 1.1898551\n",
      "Trained batch 1045 batch loss 1.21633315 epoch total loss 1.18988037\n",
      "Trained batch 1046 batch loss 1.15806818 epoch total loss 1.18985\n",
      "Trained batch 1047 batch loss 1.18231738 epoch total loss 1.18984294\n",
      "Trained batch 1048 batch loss 1.23199868 epoch total loss 1.18988311\n",
      "Trained batch 1049 batch loss 1.03045535 epoch total loss 1.18973112\n",
      "Trained batch 1050 batch loss 1.1932373 epoch total loss 1.18973446\n",
      "Trained batch 1051 batch loss 1.12811458 epoch total loss 1.18967593\n",
      "Trained batch 1052 batch loss 0.989495158 epoch total loss 1.18948567\n",
      "Trained batch 1053 batch loss 1.23545909 epoch total loss 1.1895293\n",
      "Trained batch 1054 batch loss 1.26283431 epoch total loss 1.1895988\n",
      "Trained batch 1055 batch loss 1.22127151 epoch total loss 1.18962884\n",
      "Trained batch 1056 batch loss 1.27355886 epoch total loss 1.18970835\n",
      "Trained batch 1057 batch loss 1.07766533 epoch total loss 1.18960238\n",
      "Trained batch 1058 batch loss 1.14395499 epoch total loss 1.1895591\n",
      "Trained batch 1059 batch loss 1.138816 epoch total loss 1.18951118\n",
      "Trained batch 1060 batch loss 1.17048979 epoch total loss 1.1894933\n",
      "Trained batch 1061 batch loss 1.15772653 epoch total loss 1.18946338\n",
      "Trained batch 1062 batch loss 1.18166852 epoch total loss 1.18945599\n",
      "Trained batch 1063 batch loss 1.14370942 epoch total loss 1.18941295\n",
      "Trained batch 1064 batch loss 1.18924642 epoch total loss 1.18941271\n",
      "Trained batch 1065 batch loss 1.2463758 epoch total loss 1.18946624\n",
      "Trained batch 1066 batch loss 1.31683803 epoch total loss 1.18958569\n",
      "Trained batch 1067 batch loss 1.45041537 epoch total loss 1.18983018\n",
      "Trained batch 1068 batch loss 1.36232305 epoch total loss 1.18999171\n",
      "Trained batch 1069 batch loss 1.32741642 epoch total loss 1.19012022\n",
      "Trained batch 1070 batch loss 1.19300008 epoch total loss 1.19012296\n",
      "Trained batch 1071 batch loss 1.20793891 epoch total loss 1.19013953\n",
      "Trained batch 1072 batch loss 1.02969909 epoch total loss 1.18998981\n",
      "Trained batch 1073 batch loss 1.16174829 epoch total loss 1.18996346\n",
      "Trained batch 1074 batch loss 1.18207347 epoch total loss 1.18995619\n",
      "Trained batch 1075 batch loss 1.26249051 epoch total loss 1.19002366\n",
      "Trained batch 1076 batch loss 1.17949533 epoch total loss 1.19001377\n",
      "Trained batch 1077 batch loss 1.11055851 epoch total loss 1.18994009\n",
      "Trained batch 1078 batch loss 1.05648112 epoch total loss 1.18981624\n",
      "Trained batch 1079 batch loss 1.0723393 epoch total loss 1.1897074\n",
      "Trained batch 1080 batch loss 1.00298464 epoch total loss 1.18953454\n",
      "Trained batch 1081 batch loss 1.17960858 epoch total loss 1.18952525\n",
      "Trained batch 1082 batch loss 1.07625484 epoch total loss 1.18942058\n",
      "Trained batch 1083 batch loss 1.15604722 epoch total loss 1.18938982\n",
      "Trained batch 1084 batch loss 1.16358 epoch total loss 1.18936598\n",
      "Trained batch 1085 batch loss 1.17464721 epoch total loss 1.18935239\n",
      "Trained batch 1086 batch loss 1.17879748 epoch total loss 1.18934274\n",
      "Trained batch 1087 batch loss 1.23362482 epoch total loss 1.18938351\n",
      "Trained batch 1088 batch loss 1.32099533 epoch total loss 1.1895045\n",
      "Trained batch 1089 batch loss 1.16445482 epoch total loss 1.1894815\n",
      "Trained batch 1090 batch loss 1.22849536 epoch total loss 1.18951726\n",
      "Trained batch 1091 batch loss 1.10668933 epoch total loss 1.18944132\n",
      "Trained batch 1092 batch loss 1.05731452 epoch total loss 1.18932045\n",
      "Trained batch 1093 batch loss 1.12985575 epoch total loss 1.18926609\n",
      "Trained batch 1094 batch loss 1.07374 epoch total loss 1.18916047\n",
      "Trained batch 1095 batch loss 1.21424484 epoch total loss 1.18918335\n",
      "Trained batch 1096 batch loss 1.14450955 epoch total loss 1.18914258\n",
      "Trained batch 1097 batch loss 1.08995473 epoch total loss 1.18905222\n",
      "Trained batch 1098 batch loss 1.05322075 epoch total loss 1.18892848\n",
      "Trained batch 1099 batch loss 1.18801856 epoch total loss 1.18892765\n",
      "Trained batch 1100 batch loss 1.31621766 epoch total loss 1.18904328\n",
      "Trained batch 1101 batch loss 1.21793079 epoch total loss 1.18906951\n",
      "Trained batch 1102 batch loss 1.11532795 epoch total loss 1.18900263\n",
      "Trained batch 1103 batch loss 1.06417596 epoch total loss 1.1888895\n",
      "Trained batch 1104 batch loss 0.898759782 epoch total loss 1.18862677\n",
      "Trained batch 1105 batch loss 0.842070162 epoch total loss 1.18831301\n",
      "Trained batch 1106 batch loss 0.869895816 epoch total loss 1.18802512\n",
      "Trained batch 1107 batch loss 1.06408668 epoch total loss 1.18791318\n",
      "Trained batch 1108 batch loss 1.27766919 epoch total loss 1.18799424\n",
      "Trained batch 1109 batch loss 1.31828892 epoch total loss 1.18811166\n",
      "Trained batch 1110 batch loss 1.09085751 epoch total loss 1.18802404\n",
      "Trained batch 1111 batch loss 0.994013727 epoch total loss 1.1878494\n",
      "Trained batch 1112 batch loss 1.07162797 epoch total loss 1.18774486\n",
      "Trained batch 1113 batch loss 1.07287526 epoch total loss 1.18764174\n",
      "Trained batch 1114 batch loss 1.15952587 epoch total loss 1.18761647\n",
      "Trained batch 1115 batch loss 1.29499018 epoch total loss 1.18771279\n",
      "Trained batch 1116 batch loss 1.26621318 epoch total loss 1.18778324\n",
      "Trained batch 1117 batch loss 1.14778972 epoch total loss 1.18774748\n",
      "Trained batch 1118 batch loss 1.16878009 epoch total loss 1.18773055\n",
      "Trained batch 1119 batch loss 1.11044848 epoch total loss 1.18766141\n",
      "Trained batch 1120 batch loss 1.21680021 epoch total loss 1.18768752\n",
      "Trained batch 1121 batch loss 1.2051096 epoch total loss 1.18770301\n",
      "Trained batch 1122 batch loss 1.23065829 epoch total loss 1.18774128\n",
      "Trained batch 1123 batch loss 1.2404753 epoch total loss 1.18778825\n",
      "Trained batch 1124 batch loss 1.11601603 epoch total loss 1.18772435\n",
      "Trained batch 1125 batch loss 1.18314421 epoch total loss 1.1877203\n",
      "Trained batch 1126 batch loss 1.02302933 epoch total loss 1.18757403\n",
      "Trained batch 1127 batch loss 1.18403339 epoch total loss 1.18757093\n",
      "Trained batch 1128 batch loss 1.33766639 epoch total loss 1.18770397\n",
      "Trained batch 1129 batch loss 1.17920649 epoch total loss 1.18769646\n",
      "Trained batch 1130 batch loss 1.11086643 epoch total loss 1.18762839\n",
      "Trained batch 1131 batch loss 1.29154515 epoch total loss 1.1877203\n",
      "Trained batch 1132 batch loss 1.18854809 epoch total loss 1.18772101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1133 batch loss 1.09150052 epoch total loss 1.18763614\n",
      "Trained batch 1134 batch loss 1.15614343 epoch total loss 1.18760836\n",
      "Trained batch 1135 batch loss 1.18784022 epoch total loss 1.1876086\n",
      "Trained batch 1136 batch loss 1.10403585 epoch total loss 1.18753505\n",
      "Trained batch 1137 batch loss 0.941816 epoch total loss 1.18731892\n",
      "Trained batch 1138 batch loss 1.0001595 epoch total loss 1.18715441\n",
      "Trained batch 1139 batch loss 1.09554672 epoch total loss 1.18707395\n",
      "Trained batch 1140 batch loss 1.1150074 epoch total loss 1.18701077\n",
      "Trained batch 1141 batch loss 1.05697727 epoch total loss 1.1868968\n",
      "Trained batch 1142 batch loss 1.11537647 epoch total loss 1.18683422\n",
      "Trained batch 1143 batch loss 1.08954144 epoch total loss 1.1867491\n",
      "Trained batch 1144 batch loss 1.04765189 epoch total loss 1.18662751\n",
      "Trained batch 1145 batch loss 1.29012549 epoch total loss 1.18671787\n",
      "Trained batch 1146 batch loss 1.28221536 epoch total loss 1.1868012\n",
      "Trained batch 1147 batch loss 1.31337547 epoch total loss 1.18691158\n",
      "Trained batch 1148 batch loss 1.26840901 epoch total loss 1.18698263\n",
      "Trained batch 1149 batch loss 1.16628647 epoch total loss 1.18696451\n",
      "Trained batch 1150 batch loss 1.20129251 epoch total loss 1.18697703\n",
      "Trained batch 1151 batch loss 1.08903527 epoch total loss 1.18689191\n",
      "Trained batch 1152 batch loss 1.24945521 epoch total loss 1.18694627\n",
      "Trained batch 1153 batch loss 1.22984612 epoch total loss 1.18698347\n",
      "Trained batch 1154 batch loss 1.11634243 epoch total loss 1.18692219\n",
      "Trained batch 1155 batch loss 1.2120682 epoch total loss 1.18694401\n",
      "Trained batch 1156 batch loss 1.13221788 epoch total loss 1.18689656\n",
      "Trained batch 1157 batch loss 1.20677316 epoch total loss 1.18691385\n",
      "Trained batch 1158 batch loss 1.2529242 epoch total loss 1.18697083\n",
      "Trained batch 1159 batch loss 1.21950936 epoch total loss 1.18699884\n",
      "Trained batch 1160 batch loss 1.1696918 epoch total loss 1.18698394\n",
      "Trained batch 1161 batch loss 1.27949607 epoch total loss 1.18706369\n",
      "Trained batch 1162 batch loss 1.10058033 epoch total loss 1.18698919\n",
      "Trained batch 1163 batch loss 1.12381685 epoch total loss 1.18693483\n",
      "Trained batch 1164 batch loss 1.23743021 epoch total loss 1.18697822\n",
      "Trained batch 1165 batch loss 1.18944013 epoch total loss 1.18698037\n",
      "Trained batch 1166 batch loss 1.31641364 epoch total loss 1.18709135\n",
      "Trained batch 1167 batch loss 1.32779276 epoch total loss 1.18721187\n",
      "Trained batch 1168 batch loss 1.31315207 epoch total loss 1.18731976\n",
      "Trained batch 1169 batch loss 1.25581264 epoch total loss 1.18737841\n",
      "Trained batch 1170 batch loss 1.21925449 epoch total loss 1.18740559\n",
      "Trained batch 1171 batch loss 1.13503551 epoch total loss 1.18736088\n",
      "Trained batch 1172 batch loss 1.13301289 epoch total loss 1.18731451\n",
      "Trained batch 1173 batch loss 1.21276474 epoch total loss 1.18733621\n",
      "Trained batch 1174 batch loss 1.28241372 epoch total loss 1.18741727\n",
      "Trained batch 1175 batch loss 1.1416986 epoch total loss 1.18737829\n",
      "Trained batch 1176 batch loss 1.46184921 epoch total loss 1.1876117\n",
      "Trained batch 1177 batch loss 1.41231298 epoch total loss 1.18780267\n",
      "Trained batch 1178 batch loss 1.23270285 epoch total loss 1.1878407\n",
      "Trained batch 1179 batch loss 1.20133865 epoch total loss 1.18785214\n",
      "Trained batch 1180 batch loss 1.08169413 epoch total loss 1.18776214\n",
      "Trained batch 1181 batch loss 0.951339424 epoch total loss 1.18756187\n",
      "Trained batch 1182 batch loss 1.07146156 epoch total loss 1.18746364\n",
      "Trained batch 1183 batch loss 1.11596191 epoch total loss 1.1874032\n",
      "Trained batch 1184 batch loss 0.921823919 epoch total loss 1.18717897\n",
      "Trained batch 1185 batch loss 0.868146479 epoch total loss 1.18690979\n",
      "Trained batch 1186 batch loss 0.872200131 epoch total loss 1.18664443\n",
      "Trained batch 1187 batch loss 1.05771875 epoch total loss 1.18653584\n",
      "Trained batch 1188 batch loss 1.10092723 epoch total loss 1.18646371\n",
      "Trained batch 1189 batch loss 1.18513787 epoch total loss 1.18646264\n",
      "Trained batch 1190 batch loss 1.26548672 epoch total loss 1.18652904\n",
      "Trained batch 1191 batch loss 1.20313978 epoch total loss 1.18654299\n",
      "Trained batch 1192 batch loss 1.21423531 epoch total loss 1.18656623\n",
      "Trained batch 1193 batch loss 1.38045156 epoch total loss 1.18672884\n",
      "Trained batch 1194 batch loss 1.41539645 epoch total loss 1.18692029\n",
      "Trained batch 1195 batch loss 1.4608376 epoch total loss 1.18714952\n",
      "Trained batch 1196 batch loss 1.06538785 epoch total loss 1.18704772\n",
      "Trained batch 1197 batch loss 1.13356328 epoch total loss 1.18700302\n",
      "Trained batch 1198 batch loss 1.07288885 epoch total loss 1.18690777\n",
      "Trained batch 1199 batch loss 1.29170454 epoch total loss 1.18699527\n",
      "Trained batch 1200 batch loss 1.21909118 epoch total loss 1.18702197\n",
      "Trained batch 1201 batch loss 1.19050455 epoch total loss 1.18702495\n",
      "Trained batch 1202 batch loss 1.32601857 epoch total loss 1.18714058\n",
      "Trained batch 1203 batch loss 1.32283854 epoch total loss 1.18725348\n",
      "Trained batch 1204 batch loss 1.40527225 epoch total loss 1.18743455\n",
      "Trained batch 1205 batch loss 1.32085562 epoch total loss 1.18754518\n",
      "Trained batch 1206 batch loss 1.16853607 epoch total loss 1.18752944\n",
      "Trained batch 1207 batch loss 1.20659506 epoch total loss 1.18754518\n",
      "Trained batch 1208 batch loss 1.13847113 epoch total loss 1.18750453\n",
      "Trained batch 1209 batch loss 1.30475187 epoch total loss 1.18760157\n",
      "Trained batch 1210 batch loss 1.1344732 epoch total loss 1.1875577\n",
      "Trained batch 1211 batch loss 1.16293943 epoch total loss 1.18753743\n",
      "Trained batch 1212 batch loss 1.14574742 epoch total loss 1.18750298\n",
      "Trained batch 1213 batch loss 1.17658961 epoch total loss 1.18749392\n",
      "Trained batch 1214 batch loss 1.13087916 epoch total loss 1.18744731\n",
      "Trained batch 1215 batch loss 1.40749717 epoch total loss 1.18762839\n",
      "Trained batch 1216 batch loss 1.27548337 epoch total loss 1.18770063\n",
      "Trained batch 1217 batch loss 1.27137017 epoch total loss 1.18776941\n",
      "Trained batch 1218 batch loss 1.15645516 epoch total loss 1.18774378\n",
      "Trained batch 1219 batch loss 1.1252315 epoch total loss 1.18769252\n",
      "Trained batch 1220 batch loss 1.16991401 epoch total loss 1.18767786\n",
      "Trained batch 1221 batch loss 1.20552647 epoch total loss 1.18769252\n",
      "Trained batch 1222 batch loss 1.13796198 epoch total loss 1.18765187\n",
      "Trained batch 1223 batch loss 0.947075784 epoch total loss 1.18745506\n",
      "Trained batch 1224 batch loss 1.03450692 epoch total loss 1.18733013\n",
      "Trained batch 1225 batch loss 1.08975673 epoch total loss 1.18725049\n",
      "Trained batch 1226 batch loss 1.08286607 epoch total loss 1.18716538\n",
      "Trained batch 1227 batch loss 1.07817411 epoch total loss 1.18707645\n",
      "Trained batch 1228 batch loss 1.04292345 epoch total loss 1.18695915\n",
      "Trained batch 1229 batch loss 1.17547762 epoch total loss 1.18694985\n",
      "Trained batch 1230 batch loss 1.27397025 epoch total loss 1.18702054\n",
      "Trained batch 1231 batch loss 1.10660958 epoch total loss 1.18695521\n",
      "Trained batch 1232 batch loss 1.08218384 epoch total loss 1.1868701\n",
      "Trained batch 1233 batch loss 1.18681514 epoch total loss 1.1868701\n",
      "Trained batch 1234 batch loss 1.27101135 epoch total loss 1.18693817\n",
      "Trained batch 1235 batch loss 1.24960756 epoch total loss 1.18698895\n",
      "Trained batch 1236 batch loss 1.2058146 epoch total loss 1.18700421\n",
      "Trained batch 1237 batch loss 1.10784197 epoch total loss 1.18694019\n",
      "Trained batch 1238 batch loss 1.07132614 epoch total loss 1.18684673\n",
      "Trained batch 1239 batch loss 1.15548432 epoch total loss 1.18682146\n",
      "Trained batch 1240 batch loss 1.09656286 epoch total loss 1.18674862\n",
      "Trained batch 1241 batch loss 1.02724981 epoch total loss 1.18662012\n",
      "Trained batch 1242 batch loss 1.18662155 epoch total loss 1.18662012\n",
      "Trained batch 1243 batch loss 1.06123078 epoch total loss 1.18651927\n",
      "Trained batch 1244 batch loss 1.13813722 epoch total loss 1.1864804\n",
      "Trained batch 1245 batch loss 1.12681437 epoch total loss 1.1864326\n",
      "Trained batch 1246 batch loss 1.14197838 epoch total loss 1.18639684\n",
      "Trained batch 1247 batch loss 1.08503389 epoch total loss 1.18631566\n",
      "Trained batch 1248 batch loss 1.05835223 epoch total loss 1.18621314\n",
      "Trained batch 1249 batch loss 1.02379847 epoch total loss 1.18608308\n",
      "Trained batch 1250 batch loss 1.17051649 epoch total loss 1.18607056\n",
      "Trained batch 1251 batch loss 1.20343387 epoch total loss 1.18608451\n",
      "Trained batch 1252 batch loss 1.18288827 epoch total loss 1.18608201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1253 batch loss 1.20503211 epoch total loss 1.18609715\n",
      "Trained batch 1254 batch loss 1.26945722 epoch total loss 1.18616354\n",
      "Trained batch 1255 batch loss 1.20343626 epoch total loss 1.18617737\n",
      "Trained batch 1256 batch loss 1.24920881 epoch total loss 1.18622756\n",
      "Trained batch 1257 batch loss 1.23010039 epoch total loss 1.18626249\n",
      "Trained batch 1258 batch loss 1.13879275 epoch total loss 1.18622482\n",
      "Trained batch 1259 batch loss 1.0789696 epoch total loss 1.18613958\n",
      "Trained batch 1260 batch loss 1.1062094 epoch total loss 1.18607616\n",
      "Trained batch 1261 batch loss 1.03603745 epoch total loss 1.18595707\n",
      "Trained batch 1262 batch loss 1.16931379 epoch total loss 1.18594396\n",
      "Trained batch 1263 batch loss 1.13442945 epoch total loss 1.18590307\n",
      "Trained batch 1264 batch loss 1.17597795 epoch total loss 1.18589532\n",
      "Trained batch 1265 batch loss 1.34428453 epoch total loss 1.18602049\n",
      "Trained batch 1266 batch loss 1.27446866 epoch total loss 1.18609035\n",
      "Trained batch 1267 batch loss 1.19089627 epoch total loss 1.18609416\n",
      "Trained batch 1268 batch loss 1.23282802 epoch total loss 1.186131\n",
      "Trained batch 1269 batch loss 1.14083779 epoch total loss 1.18609524\n",
      "Trained batch 1270 batch loss 1.19637513 epoch total loss 1.18610334\n",
      "Trained batch 1271 batch loss 1.20845973 epoch total loss 1.18612099\n",
      "Trained batch 1272 batch loss 1.24052191 epoch total loss 1.18616378\n",
      "Trained batch 1273 batch loss 1.25338173 epoch total loss 1.18621659\n",
      "Trained batch 1274 batch loss 1.18108058 epoch total loss 1.18621254\n",
      "Trained batch 1275 batch loss 1.31600904 epoch total loss 1.18631434\n",
      "Trained batch 1276 batch loss 1.31008625 epoch total loss 1.18641126\n",
      "Trained batch 1277 batch loss 1.29890227 epoch total loss 1.18649948\n",
      "Trained batch 1278 batch loss 1.24608457 epoch total loss 1.18654609\n",
      "Trained batch 1279 batch loss 1.1224575 epoch total loss 1.1864959\n",
      "Trained batch 1280 batch loss 1.15702081 epoch total loss 1.18647289\n",
      "Trained batch 1281 batch loss 1.14960206 epoch total loss 1.18644416\n",
      "Trained batch 1282 batch loss 1.18409014 epoch total loss 1.18644226\n",
      "Trained batch 1283 batch loss 1.05233288 epoch total loss 1.18633783\n",
      "Trained batch 1284 batch loss 0.99331516 epoch total loss 1.18618751\n",
      "Trained batch 1285 batch loss 0.982361495 epoch total loss 1.18602884\n",
      "Trained batch 1286 batch loss 1.15734148 epoch total loss 1.18600655\n",
      "Trained batch 1287 batch loss 1.24105191 epoch total loss 1.18604934\n",
      "Trained batch 1288 batch loss 1.31122112 epoch total loss 1.18614662\n",
      "Trained batch 1289 batch loss 1.34235227 epoch total loss 1.18626785\n",
      "Trained batch 1290 batch loss 1.36244094 epoch total loss 1.18640435\n",
      "Trained batch 1291 batch loss 1.24399877 epoch total loss 1.18644905\n",
      "Trained batch 1292 batch loss 1.22468376 epoch total loss 1.18647861\n",
      "Trained batch 1293 batch loss 1.15616739 epoch total loss 1.18645513\n",
      "Trained batch 1294 batch loss 1.12030649 epoch total loss 1.18640411\n",
      "Trained batch 1295 batch loss 1.09364414 epoch total loss 1.18633246\n",
      "Trained batch 1296 batch loss 1.24826074 epoch total loss 1.18638027\n",
      "Trained batch 1297 batch loss 1.11146736 epoch total loss 1.18632245\n",
      "Trained batch 1298 batch loss 1.17867732 epoch total loss 1.18631661\n",
      "Trained batch 1299 batch loss 1.21061575 epoch total loss 1.18633533\n",
      "Trained batch 1300 batch loss 1.20284009 epoch total loss 1.18634808\n",
      "Trained batch 1301 batch loss 1.14729309 epoch total loss 1.18631804\n",
      "Trained batch 1302 batch loss 1.17144239 epoch total loss 1.1863066\n",
      "Trained batch 1303 batch loss 1.2171191 epoch total loss 1.18633032\n",
      "Trained batch 1304 batch loss 1.26913571 epoch total loss 1.18639374\n",
      "Trained batch 1305 batch loss 1.10464501 epoch total loss 1.18633115\n",
      "Trained batch 1306 batch loss 1.10477567 epoch total loss 1.18626869\n",
      "Trained batch 1307 batch loss 1.15719223 epoch total loss 1.1862464\n",
      "Trained batch 1308 batch loss 1.00197887 epoch total loss 1.18610549\n",
      "Trained batch 1309 batch loss 1.07818794 epoch total loss 1.18602312\n",
      "Trained batch 1310 batch loss 1.2299819 epoch total loss 1.18605673\n",
      "Trained batch 1311 batch loss 1.10783195 epoch total loss 1.18599701\n",
      "Trained batch 1312 batch loss 1.18765545 epoch total loss 1.1859982\n",
      "Trained batch 1313 batch loss 1.16796708 epoch total loss 1.18598449\n",
      "Trained batch 1314 batch loss 1.07814908 epoch total loss 1.18590236\n",
      "Trained batch 1315 batch loss 1.14694023 epoch total loss 1.18587279\n",
      "Trained batch 1316 batch loss 1.15841758 epoch total loss 1.18585193\n",
      "Trained batch 1317 batch loss 1.15264177 epoch total loss 1.18582666\n",
      "Trained batch 1318 batch loss 1.0375123 epoch total loss 1.18571413\n",
      "Trained batch 1319 batch loss 1.11033762 epoch total loss 1.18565702\n",
      "Trained batch 1320 batch loss 1.22460186 epoch total loss 1.18568647\n",
      "Trained batch 1321 batch loss 1.12427962 epoch total loss 1.18564\n",
      "Trained batch 1322 batch loss 1.14847791 epoch total loss 1.18561184\n",
      "Trained batch 1323 batch loss 1.1423943 epoch total loss 1.18557918\n",
      "Trained batch 1324 batch loss 1.08802617 epoch total loss 1.18550551\n",
      "Trained batch 1325 batch loss 1.12101984 epoch total loss 1.18545675\n",
      "Trained batch 1326 batch loss 1.25746799 epoch total loss 1.18551111\n",
      "Trained batch 1327 batch loss 1.23041058 epoch total loss 1.18554497\n",
      "Trained batch 1328 batch loss 1.1441474 epoch total loss 1.18551373\n",
      "Trained batch 1329 batch loss 1.19177985 epoch total loss 1.1855185\n",
      "Trained batch 1330 batch loss 1.14183927 epoch total loss 1.1854856\n",
      "Trained batch 1331 batch loss 1.14911795 epoch total loss 1.18545842\n",
      "Trained batch 1332 batch loss 1.21576452 epoch total loss 1.18548119\n",
      "Trained batch 1333 batch loss 1.13108921 epoch total loss 1.18544042\n",
      "Trained batch 1334 batch loss 1.07264805 epoch total loss 1.18535578\n",
      "Trained batch 1335 batch loss 1.08161187 epoch total loss 1.18527818\n",
      "Trained batch 1336 batch loss 1.18125486 epoch total loss 1.1852752\n",
      "Trained batch 1337 batch loss 1.05820763 epoch total loss 1.18518007\n",
      "Trained batch 1338 batch loss 1.25250912 epoch total loss 1.18523049\n",
      "Trained batch 1339 batch loss 1.21606874 epoch total loss 1.1852535\n",
      "Trained batch 1340 batch loss 0.977376878 epoch total loss 1.18509841\n",
      "Trained batch 1341 batch loss 1.04722273 epoch total loss 1.18499565\n",
      "Trained batch 1342 batch loss 0.909964919 epoch total loss 1.18479061\n",
      "Trained batch 1343 batch loss 0.945310116 epoch total loss 1.18461227\n",
      "Trained batch 1344 batch loss 1.02272677 epoch total loss 1.18449187\n",
      "Trained batch 1345 batch loss 1.12669981 epoch total loss 1.18444884\n",
      "Trained batch 1346 batch loss 1.22341657 epoch total loss 1.18447781\n",
      "Trained batch 1347 batch loss 1.14521658 epoch total loss 1.18444872\n",
      "Trained batch 1348 batch loss 1.20619476 epoch total loss 1.18446481\n",
      "Trained batch 1349 batch loss 1.10063457 epoch total loss 1.18440259\n",
      "Trained batch 1350 batch loss 1.18162179 epoch total loss 1.18440056\n",
      "Trained batch 1351 batch loss 1.17961049 epoch total loss 1.18439698\n",
      "Trained batch 1352 batch loss 1.22176623 epoch total loss 1.18442464\n",
      "Trained batch 1353 batch loss 1.13540924 epoch total loss 1.1843884\n",
      "Trained batch 1354 batch loss 1.18126011 epoch total loss 1.18438613\n",
      "Trained batch 1355 batch loss 1.17678702 epoch total loss 1.18438053\n",
      "Trained batch 1356 batch loss 1.26837516 epoch total loss 1.18444252\n",
      "Trained batch 1357 batch loss 1.23432577 epoch total loss 1.18447924\n",
      "Trained batch 1358 batch loss 1.04240489 epoch total loss 1.18437457\n",
      "Trained batch 1359 batch loss 1.20650899 epoch total loss 1.1843909\n",
      "Trained batch 1360 batch loss 1.19236112 epoch total loss 1.18439686\n",
      "Trained batch 1361 batch loss 1.15804923 epoch total loss 1.18437743\n",
      "Trained batch 1362 batch loss 1.19022226 epoch total loss 1.18438172\n",
      "Trained batch 1363 batch loss 1.26209986 epoch total loss 1.18443871\n",
      "Trained batch 1364 batch loss 1.04841304 epoch total loss 1.18433905\n",
      "Trained batch 1365 batch loss 1.20985246 epoch total loss 1.18435776\n",
      "Trained batch 1366 batch loss 1.15638971 epoch total loss 1.18433726\n",
      "Trained batch 1367 batch loss 1.26071417 epoch total loss 1.18439317\n",
      "Trained batch 1368 batch loss 1.27031636 epoch total loss 1.18445587\n",
      "Trained batch 1369 batch loss 1.20114636 epoch total loss 1.18446815\n",
      "Trained batch 1370 batch loss 1.19018972 epoch total loss 1.18447232\n",
      "Trained batch 1371 batch loss 1.33400393 epoch total loss 1.1845814\n",
      "Trained batch 1372 batch loss 1.32853055 epoch total loss 1.1846863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1373 batch loss 1.28729904 epoch total loss 1.18476105\n",
      "Trained batch 1374 batch loss 1.20260298 epoch total loss 1.18477404\n",
      "Trained batch 1375 batch loss 1.16421044 epoch total loss 1.18475902\n",
      "Trained batch 1376 batch loss 1.18103623 epoch total loss 1.1847564\n",
      "Trained batch 1377 batch loss 1.25976193 epoch total loss 1.18481088\n",
      "Trained batch 1378 batch loss 1.23303521 epoch total loss 1.18484581\n",
      "Trained batch 1379 batch loss 1.32010043 epoch total loss 1.18494391\n",
      "Trained batch 1380 batch loss 1.4211607 epoch total loss 1.18511498\n",
      "Trained batch 1381 batch loss 1.26409245 epoch total loss 1.1851722\n",
      "Trained batch 1382 batch loss 1.29573059 epoch total loss 1.18525219\n",
      "Trained batch 1383 batch loss 1.24252617 epoch total loss 1.18529367\n",
      "Trained batch 1384 batch loss 1.25232136 epoch total loss 1.18534207\n",
      "Trained batch 1385 batch loss 1.25407743 epoch total loss 1.18539166\n",
      "Trained batch 1386 batch loss 1.23883367 epoch total loss 1.18543029\n",
      "Trained batch 1387 batch loss 1.11527419 epoch total loss 1.18537962\n",
      "Trained batch 1388 batch loss 1.06759369 epoch total loss 1.18529487\n",
      "Epoch 5 train loss 1.1852948665618896\n",
      "Validated batch 1 batch loss 1.12428498\n",
      "Validated batch 2 batch loss 1.18541193\n",
      "Validated batch 3 batch loss 1.27354813\n",
      "Validated batch 4 batch loss 1.1774497\n",
      "Validated batch 5 batch loss 1.29033232\n",
      "Validated batch 6 batch loss 1.35421693\n",
      "Validated batch 7 batch loss 1.08477306\n",
      "Validated batch 8 batch loss 1.20905757\n",
      "Validated batch 9 batch loss 1.23128653\n",
      "Validated batch 10 batch loss 1.28305197\n",
      "Validated batch 11 batch loss 1.21158671\n",
      "Validated batch 12 batch loss 1.0448966\n",
      "Validated batch 13 batch loss 1.11199832\n",
      "Validated batch 14 batch loss 1.1639502\n",
      "Validated batch 15 batch loss 1.13623762\n",
      "Validated batch 16 batch loss 1.21938491\n",
      "Validated batch 17 batch loss 1.17541218\n",
      "Validated batch 18 batch loss 1.09632146\n",
      "Validated batch 19 batch loss 1.22902226\n",
      "Validated batch 20 batch loss 1.28654051\n",
      "Validated batch 21 batch loss 1.30218577\n",
      "Validated batch 22 batch loss 1.20111382\n",
      "Validated batch 23 batch loss 1.10746098\n",
      "Validated batch 24 batch loss 1.15338516\n",
      "Validated batch 25 batch loss 1.14981842\n",
      "Validated batch 26 batch loss 1.21718431\n",
      "Validated batch 27 batch loss 1.13724709\n",
      "Validated batch 28 batch loss 1.09272206\n",
      "Validated batch 29 batch loss 1.2667402\n",
      "Validated batch 30 batch loss 1.15513945\n",
      "Validated batch 31 batch loss 1.00813818\n",
      "Validated batch 32 batch loss 1.0800277\n",
      "Validated batch 33 batch loss 1.15956068\n",
      "Validated batch 34 batch loss 1.1095643\n",
      "Validated batch 35 batch loss 1.15548038\n",
      "Validated batch 36 batch loss 1.18468225\n",
      "Validated batch 37 batch loss 1.16707838\n",
      "Validated batch 38 batch loss 1.28201091\n",
      "Validated batch 39 batch loss 1.22428095\n",
      "Validated batch 40 batch loss 1.16138697\n",
      "Validated batch 41 batch loss 1.25813925\n",
      "Validated batch 42 batch loss 0.981139481\n",
      "Validated batch 43 batch loss 1.12691307\n",
      "Validated batch 44 batch loss 1.08184218\n",
      "Validated batch 45 batch loss 1.18726695\n",
      "Validated batch 46 batch loss 1.3464421\n",
      "Validated batch 47 batch loss 1.17389059\n",
      "Validated batch 48 batch loss 1.19219339\n",
      "Validated batch 49 batch loss 1.2299186\n",
      "Validated batch 50 batch loss 1.0860635\n",
      "Validated batch 51 batch loss 1.21578526\n",
      "Validated batch 52 batch loss 1.25417709\n",
      "Validated batch 53 batch loss 1.26042545\n",
      "Validated batch 54 batch loss 1.28496432\n",
      "Validated batch 55 batch loss 1.28926635\n",
      "Validated batch 56 batch loss 1.18618655\n",
      "Validated batch 57 batch loss 1.26606011\n",
      "Validated batch 58 batch loss 1.35961068\n",
      "Validated batch 59 batch loss 1.32830417\n",
      "Validated batch 60 batch loss 1.28828096\n",
      "Validated batch 61 batch loss 1.27557075\n",
      "Validated batch 62 batch loss 1.29661667\n",
      "Validated batch 63 batch loss 1.31751561\n",
      "Validated batch 64 batch loss 1.10469353\n",
      "Validated batch 65 batch loss 1.20839202\n",
      "Validated batch 66 batch loss 1.29741621\n",
      "Validated batch 67 batch loss 1.23426628\n",
      "Validated batch 68 batch loss 1.19869924\n",
      "Validated batch 69 batch loss 1.19531012\n",
      "Validated batch 70 batch loss 1.20860147\n",
      "Validated batch 71 batch loss 1.15319014\n",
      "Validated batch 72 batch loss 1.09711409\n",
      "Validated batch 73 batch loss 1.18297613\n",
      "Validated batch 74 batch loss 1.21074772\n",
      "Validated batch 75 batch loss 1.21017122\n",
      "Validated batch 76 batch loss 1.23918986\n",
      "Validated batch 77 batch loss 1.18355799\n",
      "Validated batch 78 batch loss 1.17871881\n",
      "Validated batch 79 batch loss 1.30392182\n",
      "Validated batch 80 batch loss 1.09002376\n",
      "Validated batch 81 batch loss 1.05182683\n",
      "Validated batch 82 batch loss 1.24888563\n",
      "Validated batch 83 batch loss 1.27795959\n",
      "Validated batch 84 batch loss 1.37316716\n",
      "Validated batch 85 batch loss 1.39035618\n",
      "Validated batch 86 batch loss 1.14874482\n",
      "Validated batch 87 batch loss 1.38317347\n",
      "Validated batch 88 batch loss 1.15922427\n",
      "Validated batch 89 batch loss 1.25761628\n",
      "Validated batch 90 batch loss 1.22518921\n",
      "Validated batch 91 batch loss 0.955106497\n",
      "Validated batch 92 batch loss 1.15191686\n",
      "Validated batch 93 batch loss 1.31216097\n",
      "Validated batch 94 batch loss 1.22613692\n",
      "Validated batch 95 batch loss 1.15295172\n",
      "Validated batch 96 batch loss 1.17289352\n",
      "Validated batch 97 batch loss 1.1131655\n",
      "Validated batch 98 batch loss 1.21196747\n",
      "Validated batch 99 batch loss 1.21965933\n",
      "Validated batch 100 batch loss 1.16342485\n",
      "Validated batch 101 batch loss 1.18789363\n",
      "Validated batch 102 batch loss 1.25539637\n",
      "Validated batch 103 batch loss 1.2416538\n",
      "Validated batch 104 batch loss 1.30043948\n",
      "Validated batch 105 batch loss 1.22360241\n",
      "Validated batch 106 batch loss 1.13977301\n",
      "Validated batch 107 batch loss 1.14922059\n",
      "Validated batch 108 batch loss 1.25731266\n",
      "Validated batch 109 batch loss 1.16266453\n",
      "Validated batch 110 batch loss 1.21823478\n",
      "Validated batch 111 batch loss 1.21521151\n",
      "Validated batch 112 batch loss 1.3538934\n",
      "Validated batch 113 batch loss 1.32103419\n",
      "Validated batch 114 batch loss 1.22664189\n",
      "Validated batch 115 batch loss 1.12095892\n",
      "Validated batch 116 batch loss 1.15115845\n",
      "Validated batch 117 batch loss 1.25275469\n",
      "Validated batch 118 batch loss 1.17807734\n",
      "Validated batch 119 batch loss 1.20291269\n",
      "Validated batch 120 batch loss 1.28661537\n",
      "Validated batch 121 batch loss 1.35932863\n",
      "Validated batch 122 batch loss 1.17524147\n",
      "Validated batch 123 batch loss 1.2419045\n",
      "Validated batch 124 batch loss 1.12276864\n",
      "Validated batch 125 batch loss 1.25150239\n",
      "Validated batch 126 batch loss 1.21699977\n",
      "Validated batch 127 batch loss 1.13801658\n",
      "Validated batch 128 batch loss 1.27660751\n",
      "Validated batch 129 batch loss 1.25127435\n",
      "Validated batch 130 batch loss 1.30883265\n",
      "Validated batch 131 batch loss 1.24205339\n",
      "Validated batch 132 batch loss 1.27777362\n",
      "Validated batch 133 batch loss 1.15900254\n",
      "Validated batch 134 batch loss 1.181391\n",
      "Validated batch 135 batch loss 1.21498942\n",
      "Validated batch 136 batch loss 1.16137469\n",
      "Validated batch 137 batch loss 1.23089552\n",
      "Validated batch 138 batch loss 1.21747875\n",
      "Validated batch 139 batch loss 1.2340703\n",
      "Validated batch 140 batch loss 1.24794888\n",
      "Validated batch 141 batch loss 1.20107532\n",
      "Validated batch 142 batch loss 1.07871342\n",
      "Validated batch 143 batch loss 1.19372118\n",
      "Validated batch 144 batch loss 1.24321711\n",
      "Validated batch 145 batch loss 1.25053358\n",
      "Validated batch 146 batch loss 1.32020283\n",
      "Validated batch 147 batch loss 1.23738897\n",
      "Validated batch 148 batch loss 1.14791822\n",
      "Validated batch 149 batch loss 1.19677532\n",
      "Validated batch 150 batch loss 1.19952822\n",
      "Validated batch 151 batch loss 1.20303845\n",
      "Validated batch 152 batch loss 1.29142952\n",
      "Validated batch 153 batch loss 1.32900643\n",
      "Validated batch 154 batch loss 1.1927104\n",
      "Validated batch 155 batch loss 1.29607058\n",
      "Validated batch 156 batch loss 1.18008208\n",
      "Validated batch 157 batch loss 1.17427\n",
      "Validated batch 158 batch loss 1.16338837\n",
      "Validated batch 159 batch loss 1.09561634\n",
      "Validated batch 160 batch loss 1.25793743\n",
      "Validated batch 161 batch loss 1.11483979\n",
      "Validated batch 162 batch loss 1.15868282\n",
      "Validated batch 163 batch loss 1.11872637\n",
      "Validated batch 164 batch loss 1.18642151\n",
      "Validated batch 165 batch loss 1.16508412\n",
      "Validated batch 166 batch loss 1.11850572\n",
      "Validated batch 167 batch loss 1.25431132\n",
      "Validated batch 168 batch loss 1.15950775\n",
      "Validated batch 169 batch loss 1.21247864\n",
      "Validated batch 170 batch loss 1.29360962\n",
      "Validated batch 171 batch loss 1.09589458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 172 batch loss 1.26087976\n",
      "Validated batch 173 batch loss 1.17621684\n",
      "Validated batch 174 batch loss 1.09480035\n",
      "Validated batch 175 batch loss 1.20415223\n",
      "Validated batch 176 batch loss 1.21378374\n",
      "Validated batch 177 batch loss 1.14600158\n",
      "Validated batch 178 batch loss 1.32445\n",
      "Validated batch 179 batch loss 1.22759461\n",
      "Validated batch 180 batch loss 1.28913426\n",
      "Validated batch 181 batch loss 1.12378991\n",
      "Validated batch 182 batch loss 1.27075624\n",
      "Validated batch 183 batch loss 1.18350279\n",
      "Validated batch 184 batch loss 1.11986744\n",
      "Validated batch 185 batch loss 1.26072693\n",
      "Epoch 5 val loss 1.2052125930786133\n",
      "Model /aiffel/aiffel/mpii/models/model_SHN-epoch-5-loss-1.2052.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "# StackedHourglassNetwork 학습\n",
    "best_model_SHN_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, 'model_SHN', is_stackedhourglassnetwork=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17cc712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    /tmp/ipykernel_47/715066145.py:78 train_step  *\n",
      "        loss = self.compute_loss(labels, outputs)\n",
      "    /tmp/ipykernel_47/715066145.py:69 compute_loss  *\n",
      "        loss += tf.math.reduce_mean(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n",
      "        raise e\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n",
      "        return func(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n",
      "        return gen_math_ops.add_v2(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n",
      "        raise TypeError(\n",
      "\n",
      "    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 695, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "TypeError: in user code:\n",
      "\n",
      "    /tmp/ipykernel_47/715066145.py:78 train_step  *\n",
      "        loss = self.compute_loss(labels, outputs)\n",
      "    /tmp/ipykernel_47/715066145.py:69 compute_loss  *\n",
      "        loss += tf.math.reduce_mean(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n",
      "        raise e\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n",
      "        return func(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n",
      "        return target(*args, **kwargs)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n",
      "        return gen_math_ops.add_v2(x, y, name=name)\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n",
      "        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n",
      "        raise TypeError(\n",
      "\n",
      "    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_47/715066145.py:100 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_47/715066145.py:78 train_step  *\n        loss = self.compute_loss(labels, outputs)\n    /tmp/ipykernel_47/715066145.py:69 compute_loss  *\n        loss += tf.math.reduce_mean(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n        raise TypeError(\n\n    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/2365056313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Simplebaseline 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_model_SBL_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tfrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_tfrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_SBL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_stackedhourglassnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_47/3422176411.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, model_file_name, is_stackedhourglassnetwork)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dist_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mtrain_tfrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFRECORD_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47/715066145.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_dist_dataset, val_dist_dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 epoch, self.current_learning_rate))\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             train_total_loss, num_train_batches = distributed_train_epoch(\n\u001b[0m\u001b[1;32m    136\u001b[0m                 train_dist_dataset)\n\u001b[1;32m    137\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /tmp/ipykernel_47/715066145.py:100 distributed_train_epoch  *\n        per_replica_loss = self.strategy.run(\n    /tmp/ipykernel_47/715066145.py:78 train_step  *\n        loss = self.compute_loss(labels, outputs)\n    /tmp/ipykernel_47/715066145.py:69 compute_loss  *\n        loss += tf.math.reduce_mean(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\n        raise TypeError(\n\n    TypeError: Input 'y' of 'AddV2' Op has type float32 that does not match type int32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "# Simplebaseline 학습\n",
    "best_model_SBL_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords, 'model_SBL', is_stackedhourglassnetwork=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a97e8",
   "metadata": {},
   "source": [
    "# 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf4eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15\n",
    "\n",
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]\n",
    "\n",
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()\n",
    "\n",
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints\n",
    "\n",
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp\n",
    "\n",
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()\n",
    "\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a871fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackedHourglass 모델 결과\n",
    "WEIGHTS_PATH = os.path.join(PROJECT_PATH, 'models', 'model-v0.0.1-epoch-2-loss-1.3072.h5')\n",
    "\n",
    "model_SHN = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "model_SHN.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "image_SHN, keypoints_SHN = predict(model_SHN, test_image)\n",
    "draw_keypoints_on_image(image_SHN, keypoints_SHN)\n",
    "draw_skeleton_on_image(image_SHN, keypoints_SHN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplebaseline 모델 결과\n",
    "model_SBL = Simplebaseline(IMAGE_SHAPE)\n",
    "model_SBL.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "image_SBL, keypoints_SBL = predict(model_SBL, test_image)\n",
    "draw_keypoints_on_image(image_SBL, keypoints_SBL)\n",
    "draw_skeleton_on_image(image_SBL, keypoints_SBL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51762c01",
   "metadata": {},
   "source": [
    "## 회고\n",
    "\n",
    "죄송합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b78085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cd396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d8ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a2b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
