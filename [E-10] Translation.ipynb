{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "962809c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "\n",
    "lines = lines[['eng', 'fra']][:33000] # 3.3만개 샘플 사용\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)\n",
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc0e98",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781dcc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점과 단어 분리, 소문자로 변환, 띄어쓰기 단위로 토큰화 수행.\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def preprocess_sentence_decoder(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' ' + sentence + ' '\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "lines.eng = lines.eng.apply(lambda x : preprocess_sentence(x))\n",
    "lines.fra = lines.fra.apply(lambda x : preprocess_sentence_decoder(x))\n",
    "\n",
    "# 토큰화\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(lines.eng)\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)\n",
    "fra_tokenizer = Tokenizer()\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bbf653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4671\n",
      "프랑스어 단어장의 크기 : 7453\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17fb9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Teacher Forcing\n",
    "sos_token = ''\n",
    "eos_token = ''\n",
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]\n",
    "\n",
    "\n",
    "# padding\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "\n",
    "\n",
    "# train / Validation split\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967d4d98",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7445af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    1195776     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1907968     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 256)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 256)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7453)   1915421     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,069,789\n",
      "Trainable params: 6,069,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "938/938 [==============================] - 46s 19ms/step - loss: 0.9360 - acc: 0.8672 - val_loss: 0.5063 - val_acc: 0.9343\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.4224 - acc: 0.9466 - val_loss: 0.3616 - val_acc: 0.9561\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.3276 - acc: 0.9605 - val_loss: 0.3093 - val_acc: 0.9643\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2838 - acc: 0.9671 - val_loss: 0.2753 - val_acc: 0.9690\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2512 - acc: 0.9716 - val_loss: 0.2575 - val_acc: 0.9724\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2348 - acc: 0.9743 - val_loss: 0.2466 - val_acc: 0.9746\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2212 - acc: 0.9764 - val_loss: 0.2347 - val_acc: 0.9764\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.2073 - acc: 0.9782 - val_loss: 0.2242 - val_acc: 0.9776\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1951 - acc: 0.9796 - val_loss: 0.2152 - val_acc: 0.9789\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1841 - acc: 0.9809 - val_loss: 0.2058 - val_acc: 0.9798\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1745 - acc: 0.9818 - val_loss: 0.1991 - val_acc: 0.9805\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1657 - acc: 0.9827 - val_loss: 0.1906 - val_acc: 0.9811\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1580 - acc: 0.9834 - val_loss: 0.1837 - val_acc: 0.9818\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1508 - acc: 0.9841 - val_loss: 0.1778 - val_acc: 0.9823\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1443 - acc: 0.9845 - val_loss: 0.1716 - val_acc: 0.9827\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1382 - acc: 0.9851 - val_loss: 0.1649 - val_acc: 0.9830\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1322 - acc: 0.9855 - val_loss: 0.1596 - val_acc: 0.9835\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1271 - acc: 0.9860 - val_loss: 0.1554 - val_acc: 0.9837\n",
      "Epoch 19/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1221 - acc: 0.9864 - val_loss: 0.1517 - val_acc: 0.9842\n",
      "Epoch 20/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1185 - acc: 0.9868 - val_loss: 0.1488 - val_acc: 0.9845\n",
      "Epoch 21/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1156 - acc: 0.9871 - val_loss: 0.1459 - val_acc: 0.9849\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1128 - acc: 0.9875 - val_loss: 0.1427 - val_acc: 0.9852\n",
      "Epoch 23/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1110 - acc: 0.9877 - val_loss: 0.1409 - val_acc: 0.9855\n",
      "Epoch 24/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1096 - acc: 0.9879 - val_loss: 0.1392 - val_acc: 0.9856\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1076 - acc: 0.9882 - val_loss: 0.1376 - val_acc: 0.9857\n",
      "Epoch 26/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1053 - acc: 0.9884 - val_loss: 0.1358 - val_acc: 0.9858\n",
      "Epoch 27/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1029 - acc: 0.9886 - val_loss: 0.1337 - val_acc: 0.9861\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.1008 - acc: 0.9888 - val_loss: 0.1319 - val_acc: 0.9862\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0984 - acc: 0.9890 - val_loss: 0.1299 - val_acc: 0.9863\n",
      "Epoch 30/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0963 - acc: 0.9892 - val_loss: 0.1282 - val_acc: 0.9864\n",
      "Epoch 31/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0942 - acc: 0.9893 - val_loss: 0.1264 - val_acc: 0.9864\n",
      "Epoch 32/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0925 - acc: 0.9894 - val_loss: 0.1249 - val_acc: 0.9865\n",
      "Epoch 33/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0909 - acc: 0.9894 - val_loss: 0.1235 - val_acc: 0.9865\n",
      "Epoch 34/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0896 - acc: 0.9895 - val_loss: 0.1223 - val_acc: 0.9865\n",
      "Epoch 35/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0883 - acc: 0.9896 - val_loss: 0.1212 - val_acc: 0.9866\n",
      "Epoch 36/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0874 - acc: 0.9896 - val_loss: 0.1201 - val_acc: 0.9866\n",
      "Epoch 37/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0865 - acc: 0.9897 - val_loss: 0.1197 - val_acc: 0.9866\n",
      "Epoch 38/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0859 - acc: 0.9896 - val_loss: 0.1193 - val_acc: 0.9866\n",
      "Epoch 39/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0853 - acc: 0.9897 - val_loss: 0.1186 - val_acc: 0.9866\n",
      "Epoch 40/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0849 - acc: 0.9897 - val_loss: 0.1183 - val_acc: 0.9867\n",
      "Epoch 41/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0844 - acc: 0.9897 - val_loss: 0.1175 - val_acc: 0.9867\n",
      "Epoch 42/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0842 - acc: 0.9897 - val_loss: 0.1176 - val_acc: 0.9867\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0839 - acc: 0.9897 - val_loss: 0.1171 - val_acc: 0.9867\n",
      "Epoch 44/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0835 - acc: 0.9897 - val_loss: 0.1168 - val_acc: 0.9867\n",
      "Epoch 45/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.0834 - acc: 0.9897 - val_loss: 0.1168 - val_acc: 0.9867\n",
      "Epoch 46/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0833 - acc: 0.9897 - val_loss: 0.1166 - val_acc: 0.9867\n",
      "Epoch 47/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0830 - acc: 0.9897 - val_loss: 0.1163 - val_acc: 0.9867\n",
      "Epoch 48/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0828 - acc: 0.9898 - val_loss: 0.1166 - val_acc: 0.9867\n",
      "Epoch 49/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0829 - acc: 0.9898 - val_loss: 0.1166 - val_acc: 0.9867\n",
      "Epoch 50/50\n",
      "938/938 [==============================] - 15s 16ms/step - loss: 0.0829 - acc: 0.9898 - val_loss: 0.1162 - val_acc: 0.9867\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "# encoder embedding\n",
    "enc_emb = Embedding(eng_vocab_size, 256, input_length=max_eng_seq_len)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "# decoder embedding\n",
    "dec_emb = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(dec_masking, initial_state = encoder_states)\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer=\"rmsprop\", loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model_history = model.fit([encoder_input_train, decoder_input_train],decoder_target_train,\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 32, epochs = 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385e94d",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94764fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LSTM_history_dict = model_history.history\n",
    "loss = LSTM_history_dict['loss']\n",
    "val_loss = LSTM_history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be8104",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "309ebf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 256)         1195776   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 525312    \n",
      "=================================================================\n",
      "Total params: 1,721,088\n",
      "Trainable params: 1,721,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    1907968     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_2[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7453)   1915421     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,348,701\n",
      "Trainable params: 4,348,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "-----------------------------------\n",
      "입력 문장: he came out on top . \n",
      "정답 문장: il finit en t te . \n",
      "번역기가 번역한 문장:  mangent mangent atio\n",
      "-----------------------------------\n",
      "입력 문장: stop laughing . \n",
      "정답 문장: arr tez de rire ! \n",
      "번역기가 번역한 문장:  mangent mangent mangen\n",
      "-----------------------------------\n",
      "입력 문장: i have your number . \n",
      "정답 문장: j ai ton num ro . \n",
      "번역기가 번역한 문장:  tu fort ative jumeau\n",
      "-----------------------------------\n",
      "입력 문장: tom needs time . \n",
      "정답 문장: tom a besoin de temps . \n",
      "번역기가 번역한 문장:  j les tres plaisanteri\n",
      "-----------------------------------\n",
      "입력 문장: i did it like this . \n",
      "정답 문장: je l ai fait comme ceci . \n",
      "번역기가 번역한 문장:  je sale nous text\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()\n",
    "\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb2 = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()\n",
    "\n",
    "\n",
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # 에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + idx2eng[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['']) and i!=fra2idx['']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp\n",
    "\n",
    "\n",
    "for seq_index in [1,100,301,777,2222]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', seq2src(encoder_input_test[seq_index]))\n",
    "    print('정답 문장:', seq2tar(decoder_input_test[seq_index]))\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfddc5fa",
   "metadata": {},
   "source": [
    "### 회고\n",
    "번역기가 완성되었지만, 무슨 일인지 번역기가 번역한 문장은 형편없었다. 마지막 프랑스어를 번역한 문장은 프랑스어로 '나는 우리 텍스트를 더럽힌다'라는 뜻이라고 한다. 그럼에도 불구하고 accuracy는 꽤 높은 수치로 나왔다. accuracy가 높다 라는 건 어느정도 번역의 수준이 잘되었다는 뜻같은데, 왜 이정도로 형편없는 번역이 나왔는지 알고싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf7d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b873084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d1884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd288a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336314ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fa721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3342c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4fe4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bb324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03345a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0550d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
